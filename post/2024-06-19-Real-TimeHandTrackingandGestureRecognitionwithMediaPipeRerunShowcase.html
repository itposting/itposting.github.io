<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스 | itposting" data-gatsby-head="true"/><meta property="og:title" content="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase" data-gatsby-head="true"/><meta name="twitter:title" content="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 18:31" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">12<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>미디어파이프의 손 추척 및 제스처 인식을 Rerun과 함께 시각화하는 방법</h2>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*pE_4QrsVPV7vMrxB6YS1cQ.gif" alt="image"></p>
<p>이 게시물에서는 미디어파이프 파이썬과 Rerun SDK를 사용하여 손 추척 및 제스처 인식의 예제를 소개하고 있습니다.</p>
<p>더 깊이 파고들고 이해를 넓히고 싶다면, 미디어파이프 파이썬 및 Rerun SDK를 설치하여 손을 추적하고 다양한 제스처를 인식하고 데이터를 시각화하는 방법을 안내해 드리겠습니다.</p>
<div class="content-ad"></div>
<h2>그러므로, 다음을 배울 것입니다:</h2>
<ul>
<li>MediaPipe Python 및 Rerun 설치하는 방법</li>
<li>MediaPipe 제스처 인식을 사용한 손 추적 및 제스처 인식 방법</li>
<li>손 추적 및 제스처 인식 결과를 Rerun Viewer에서 시각화하는 방법</li>
</ul>
<p>예제를 시도하기를 열망한다면, 아래 제공된 코드를 사용해보세요:</p>
<pre><code class="hljs language-js"># rerun <span class="hljs-title class_">GitHub</span> 저장소를 로컬 머신에 클론합니다.
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/rerun-io/rerun</span>

# rerun 저장소 디렉토리로 이동합니다.
cd rerun

# 필요한 <span class="hljs-title class_">Python</span> 패키지를 requirements 파일에 명시된대로 설치합니다.
pip install -r examples/python/gesture_detection/requirements.<span class="hljs-property">txt</span>

# 예제를 위한 주요 <span class="hljs-title class_">Python</span> 스크립트를 실행합니다.
python examples/python/gesture_detection/main.<span class="hljs-property">py</span>

# 특정 이미지에 대한 주요 <span class="hljs-title class_">Python</span> 스크립트를 실행합니다.
python examples/python/gesture_detection/main.<span class="hljs-property">py</span> --image path/to/your/image.<span class="hljs-property">jpg</span>

# 특정 비디오에 대한 주요 <span class="hljs-title class_">Python</span> 스크립트를 실행합니다.
python examples/python/gesture_detection/main.<span class="hljs-property">py</span> --video path/to/your/video.<span class="hljs-property">mp4</span>

# 카메라 스트림으로 주요 <span class="hljs-title class_">Python</span> 스크립트를 실행합니다.
python examples/python/gesture_detection/main.<span class="hljs-property">py</span> --camera
</code></pre>
<div class="content-ad"></div>
<h1>손 추적 및 제스처 인식 기술</h1>
<p>계속하기 전에, 우리가 가능하게 한 기술에 대해 인정해주어야 합니다. 손 추적 및 제스처 인식 기술은 기기가 손 움직임과 제스처를 명령이나 입력으로 해석할 수 있도록 하는 것을 목표로 합니다. 이 기술의 핵심은 미리 훈련된 기계 학습 모델이 시각 입력을 분석하고 손의 랜드마크와 제스처를 식별합니다. 이러한 기술의 실제 응용은 다양하며, 손 움직임과 제스처를 사용하여 스마트 기기를 제어하는 데 사용될 수 있습니다. 인간-컴퓨터 상호 작용, 로봇 공학, 게임 및 증강 현실은 이 기술의 잠재적인 응용 분야 중 가장 유망하게 보입니다.</p>
<p>그러나 이러한 기술을 사용하는 방법에 대해 항상 주의해야 합니다. 민감하고 중요한 시스템에서 사용시 손 제스처를 잘못 해석할 수 있고, 잘못된 양성 또는 음성의 가능성이 작지 않습니다. 이를 활용함으로써 발생하는 윤리적 및 법적 문제가 사용자들이 특히 공공장소에서 자신의 제스처가 기록되는 것을 원치 않을 수 있습니다. 현실 세계 시나리오에서 이 기술을 도입하기로 결정했다면, 윤리적 및 법적 고려 사항을 고려하는 것이 중요합니다.</p>
<h1>요구 사항 및 설정</h1>
<div class="content-ad"></div>
<p>먼저, 필요한 라이브러리를 설치해야 합니다. 이는 OpenCV, MediaPipe 및 Rerun과 같은 라이브러리를 포함합니다. MediaPipe Python은 컴퓨터 비전 및 머신러닝을 위한 온디바이스 ML 솔루션을 통합하려는 개발자들에게 유용한 도구이며, Rerun은 시간이 지남에 따라 변화하는 다중 모달 데이터를 시각화하기 위한 SDK입니다.</p>
<pre><code class="hljs language-js"># 요구 사항 파일에서 지정된 필수 <span class="hljs-title class_">Python</span> 패키지 설치
pip install -r examples/python/gesture_detection/requirements.<span class="hljs-property">txt</span>
</code></pre>
<p>그런 다음, 여기서 미리 정의된 모델을 다운로드해야 합니다: HandGestureClassifier</p>
<h1>MediaPipe를 사용한 손 추적과 제스처 인식</h1>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png">
<p>이제 샘플 이미지에 제스처 인식을 위해 MediaPipe 사전 훈련 모델을 사용해봅시다. 아래 코드는 MediaPipe 제스처 인식 솔루션의 초기화 및 구성을 설정하는 기초를 제공합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> mediapipe.<span class="hljs-property">tasks</span>.<span class="hljs-property">python</span> <span class="hljs-keyword">import</span> vision
<span class="hljs-keyword">from</span> mediapipe.<span class="hljs-property">tasks</span> <span class="hljs-keyword">import</span> python

<span class="hljs-keyword">class</span> <span class="hljs-title class_">GestureDetectorLogger</span>:

    def <span class="hljs-title function_">__init__</span>(self, <span class="hljs-attr">video_mode</span>: bool = <span class="hljs-title class_">False</span>):
        self.<span class="hljs-property">_video_mode</span> = video_mode

        base_options = python.<span class="hljs-title class_">BaseOptions</span>(
            model_asset_path=<span class="hljs-string">'gesture_recognizer.task'</span>
        )
        options = vision.<span class="hljs-title class_">GestureRecognizerOptions</span>(
            base_options=base_options,
            running_mode=mp.<span class="hljs-property">tasks</span>.<span class="hljs-property">vision</span>.<span class="hljs-property">RunningMode</span>.<span class="hljs-property">VIDEO</span> <span class="hljs-keyword">if</span> self.<span class="hljs-property">_video_mode</span> <span class="hljs-keyword">else</span> mp.<span class="hljs-property">tasks</span>.<span class="hljs-property">vision</span>.<span class="hljs-property">RunningMode</span>.<span class="hljs-property">IMAGE</span>
        )
        self.<span class="hljs-property">recognizer</span> = vision.<span class="hljs-property">GestureRecognizer</span>.<span class="hljs-title function_">create_from_options</span>(options)


    def <span class="hljs-title function_">detect</span>(self, <span class="hljs-attr">image</span>: npt.<span class="hljs-property">NDArray</span>[np.<span class="hljs-property">uint8</span>]) -> <span class="hljs-title class_">None</span>:
        image = mp.<span class="hljs-title class_">Image</span>(image_format=mp.<span class="hljs-property">ImageFormat</span>.<span class="hljs-property">SRGB</span>, data=image)
  
        # 제스처 검출 모델로부터 결과 가져오기
        recognition_result = self.<span class="hljs-property">recognizer</span>.<span class="hljs-title function_">recognize</span>(image)
  
        <span class="hljs-keyword">for</span> i, gesture <span class="hljs-keyword">in</span> <span class="hljs-title function_">enumerate</span>(recognition_result.<span class="hljs-property">gestures</span>):
            # 인식된 제스처 중 상위 제스처 가져오기
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"최상위 제스처 결과: "</span>, gesture[<span class="hljs-number">0</span>].<span class="hljs-property">category_name</span>)
  
        <span class="hljs-keyword">if</span> recognition_result.<span class="hljs-property">hand_landmarks</span>:
            # <span class="hljs-title class_">MediaPipe</span>에서 손 랜드마크 가져오기
            hand_landmarks = recognition_result.<span class="hljs-property">hand_landmarks</span>
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"손 랜드마크: "</span> + <span class="hljs-title function_">str</span>(hand_landmarks))
  
            # <span class="hljs-title class_">MediaPipe</span>에서 손 연결 정보 가져오기
            mp_hands_connections = mp.<span class="hljs-property">solutions</span>.<span class="hljs-property">hands</span>.<span class="hljs-property">HAND_CONNECTIONS</span>
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"손 연결 정보: "</span> + <span class="hljs-title function_">str</span>(mp_hands_connections))
</code></pre>
<p>GestureDetectorLogger 클래스 내의 detect 함수는 이미지를 인자로 받아 모델 결과를 출력하며, 인식된 최상위 제스처와 감지된 손 랜드마크를 강조합니다. 모델에 대한 추가 정보는 해당 모델 카드를 참조하세요.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_1.png" alt="이미지"></p>
<p>아래 코드를 사용하여 직접 시도해볼 수 있어요:</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">run_from_sample_image</span>(path)-> <span class="hljs-title class_">None</span>:
    image = cv2.<span class="hljs-title function_">imread</span>(<span class="hljs-title function_">str</span>(path))
    show_image = cv2.<span class="hljs-title function_">cvtColor</span>(image, cv2.<span class="hljs-property">COLOR_BGR2RGB</span>)
    logger = <span class="hljs-title class_">GestureDetectorLogger</span>(video_mode=<span class="hljs-title class_">False</span>)
    logger.<span class="hljs-title function_">detect_and_log</span>(show_image)

# 샘플 이미지로 제스처 인식 실행하기
<span class="hljs-title function_">run_from_sample_image</span>(<span class="hljs-variable constant_">SAMPLE_IMAGE_PATH</span>)
</code></pre>
<h1>재실행을 사용하여 확인, 디버그 및 데모하기</h1>
<div class="content-ad"></div>
<p>이 단계는 솔루션의 신뢰성과 효과성을 보장하는 데 도움이 됩니다. 모델을 준비한 상태로 결과를 시각화하여 정확성을 확인하고 잠재적인 문제를 해결하며 능력을 시연할 수 있습니다. 결과를 시각화해서 Rerun SDK를 사용하면 간단하고 빠르게 가능합니다.</p>
<h2>Rerun을 어떻게 사용할까요?</h2>
<p><img src="/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_2.png" alt="이미지"></p>
<ul>
<li>Rerun SDK를 사용하여 코드에서 로깅하여 다중 데이터를 스트림으로 전송</li>
<li>현지 또는 원격으로 라이브 또는 녹화된 스트림을 시각화하고 상호 작용</li>
<li>레이아웃을 대화식으로 구축하고 시각화를 사용자 정의</li>
<li>필요할 때 Rerun을 확장</li>
</ul>
<div class="content-ad"></div>
<p>코드 작성에 앞서, Rerun Viewer를 설치하기 위해 해당 페이지를 방문하는 것이 좋습니다. 그런 다음, Rerun SDK에 대해 읽어보는 것을 권해드립니다. 파이썬 빠른 시작 가이드와 파이썬에서 데이터 기록하기를 읽어보세요. 이러한 초기 단계는 원활한 설정을 보장하고 다가오는 코드 실행에 도움이 될 것입니다.</p>
<h2>비디오 또는 실시간 실행</h2>
<p>비디오 스트리밍에는 OpenCV가 사용됩니다. 특정 비디오의 파일 경로를 선택하거나 0 또는 1의 인수를 제공하여 자체 카메라에 액세스할 수 있습니다 (기본 카메라를 사용하려면 0을 사용하고, 맥에서는 1을 사용할 수 있습니다).</p>
<p>타임라인의 소개를 강조하는 것이 중요합니다. Rerun 타임라인의 기능은 데이터를 하나 이상의 타임라인과 연관시킬 수 있게 합니다. 결과적으로, 비디오의 각 프레임은 해당 타임스탬프와 연관되어 있습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">def <span class="hljs-title function_">run_from_video_capture</span>(<span class="hljs-attr">vid</span>: int | str, <span class="hljs-attr">max_frame_count</span>: int | <span class="hljs-title class_">None</span>) -> <span class="hljs-title class_">None</span>:
    <span class="hljs-string">""</span><span class="hljs-string">"
    비디오 스트림에서 탐지기를 실행합니다.

    매개변수
    ----------
    vid:
        탐지기가 실행될 비디오 스트림입니다. 기본 카메라에는 0/1을 사용하거나 비디오 파일의 경로를 지정하세요.
    max_frame_count:
        처리할 최대 프레임 수입니다. None이면 모든 프레임을 처리합니다.
    "</span><span class="hljs-string">""</span>
    cap = cv2.<span class="hljs-title class_">VideoCapture</span>(vid)
    fps = cap.<span class="hljs-title function_">get</span>(cv2.<span class="hljs-property">CAP_PROP_FPS</span>)

    detector = <span class="hljs-title class_">GestureDetectorLogger</span>(video_mode=<span class="hljs-title class_">True</span>)

    <span class="hljs-attr">try</span>:
        <span class="hljs-attr">it</span>: <span class="hljs-title class_">Iterable</span>[int] = itertools.<span class="hljs-title function_">count</span>() <span class="hljs-keyword">if</span> max_frame_count is <span class="hljs-title class_">None</span> <span class="hljs-keyword">else</span> <span class="hljs-title function_">range</span>(max_frame_count)

        <span class="hljs-keyword">for</span> frame_idx <span class="hljs-keyword">in</span> tqdm.<span class="hljs-title function_">tqdm</span>(it, desc=<span class="hljs-string">"프레임 처리 중"</span>):
            ret, frame = cap.<span class="hljs-title function_">read</span>()
            <span class="hljs-keyword">if</span> not <span class="hljs-attr">ret</span>:
                <span class="hljs-keyword">break</span>

            <span class="hljs-keyword">if</span> np.<span class="hljs-title function_">all</span>(frame == <span class="hljs-number">0</span>):
                <span class="hljs-keyword">continue</span>

            frame_time_nano = <span class="hljs-title function_">int</span>(cap.<span class="hljs-title function_">get</span>(cv2.<span class="hljs-property">CAP_PROP_POS_MSEC</span>) * <span class="hljs-number">1e6</span>)
            <span class="hljs-keyword">if</span> frame_time_nano == <span class="hljs-number">0</span>:
                frame_time_nano = <span class="hljs-title function_">int</span>(frame_idx * <span class="hljs-number">1000</span> / fps * <span class="hljs-number">1e6</span>)

            frame = cv2.<span class="hljs-title function_">cvtColor</span>(frame, cv2.<span class="hljs-property">COLOR_BGR2RGB</span>)

            rr.<span class="hljs-title function_">set_time_sequence</span>(<span class="hljs-string">"프레임 번호"</span>, frame_idx)
            rr.<span class="hljs-title function_">set_time_nanos</span>(<span class="hljs-string">"프레임 시간"</span>, frame_time_nano)
            detector.<span class="hljs-title function_">detect_and_log</span>(frame, frame_time_nano)
            rr.<span class="hljs-title function_">log</span>(
                <span class="hljs-string">"미디어/비디오"</span>,
                rr.<span class="hljs-title class_">Image</span>(frame)
            )

    except <span class="hljs-title class_">KeyboardInterrupt</span>:
        pass

    cap.<span class="hljs-title function_">release</span>()
    cv2.<span class="hljs-title function_">destroyAllWindows</span>()
</code></pre>
<h2>시각화를 위한 데이터 로깅</h2>
<img src="https://miro.medium.com/v2/resize:fit:1400/1*c1Us-7PoWSP0rgVdlMQUhA.gif">
<p>Rerun Viewer에서 데이터를 시각화하려면 Rerun SDK를 사용하여 데이터를 로깅하는 것이 중요합니다. 이전에 언급된 가이드는 이 프로세스에 대한 통찰을 제공합니다. 이 문맥에서는 정규화된 값으로 손 랜드마크 포인트를 추출한 다음, 이미지의 너비와 높이를 사용하여 이미지 좌표로 변환합니다. 이러한 좌표는 2D 포인트로 Rerun SDK에 로깅됩니다. 추가로, 랜드마크 간의 연결을 식별하고 2D 라인스트립으로 로깅합니다.</p>
<div class="content-ad"></div>
<p>제스처 인식을 위해 결과는 콘솔에 출력됩니다. 그러나 소스 코드 안에서는 TextDocument 및 이모지를 사용하여 이러한 결과를 시청자에게 제시하는 방법을 탐구할 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GestureDetectorLogger</span>:

    def <span class="hljs-title function_">detect_and_log</span>(self, <span class="hljs-attr">image</span>: npt.<span class="hljs-property">NDArray</span>[np.<span class="hljs-property">uint8</span>], <span class="hljs-attr">frame_time_nano</span>: int | <span class="hljs-title class_">None</span>) -> <span class="hljs-title class_">None</span>:
        # 이미지에서 제스처 인식
        height, width, _ = image.<span class="hljs-property">shape</span>
        image = mp.<span class="hljs-title class_">Image</span>(image_format=mp.<span class="hljs-property">ImageFormat</span>.<span class="hljs-property">SRGB</span>, data=image)

        recognition_result = (
            self.<span class="hljs-property">recognizer</span>.<span class="hljs-title function_">recognize_for_video</span>(image, <span class="hljs-title function_">int</span>(frame_time_nano / <span class="hljs-number">1e6</span>))
            <span class="hljs-keyword">if</span> self.<span class="hljs-property">_video_mode</span>
            <span class="hljs-keyword">else</span> self.<span class="hljs-property">recognizer</span>.<span class="hljs-title function_">recognize</span>(image)
        )

        # 값 지우기
        <span class="hljs-keyword">for</span> log_key <span class="hljs-keyword">in</span> [<span class="hljs-string">"Media/Points"</span>, <span class="hljs-string">"Media/Connections"</span>]:
            rr.<span class="hljs-title function_">log</span>(log_key, rr.<span class="hljs-title class_">Clear</span>(recursive=<span class="hljs-title class_">True</span>))

        <span class="hljs-keyword">for</span> i, gesture <span class="hljs-keyword">in</span> <span class="hljs-title function_">enumerate</span>(recognition_result.<span class="hljs-property">gestures</span>):
            # 인식된 제스처를 기록
            gesture_category = gesture[<span class="hljs-number">0</span>].<span class="hljs-property">category_name</span> <span class="hljs-keyword">if</span> recognition_result.<span class="hljs-property">gestures</span> <span class="hljs-keyword">else</span> <span class="hljs-string">"None"</span>
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"제스처 카테고리:"</span>, gesture_category)

        <span class="hljs-keyword">if</span> recognition_result.<span class="hljs-property">hand_landmarks</span>:
            hand_landmarks = recognition_result.<span class="hljs-property">hand_landmarks</span>

            # 정규화된 좌표를 이미지 좌표로 변환
            points = self.<span class="hljs-title function_">convert_landmarks_to_image_coordinates</span>(hand_landmarks, width, height)

            # 이미지 및 <span class="hljs-title class_">Hand</span> <span class="hljs-title class_">Entity</span>에 점 기록
            rr.<span class="hljs-title function_">log</span>(
               <span class="hljs-string">"Media/Points"</span>,
                rr.<span class="hljs-title class_">Points2D</span>(points, radii=<span class="hljs-number">10</span>, colors=[<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>])
            )

            # <span class="hljs-title class_">MediaPipe</span>에서 손 연결 가져오기
            mp_hands_connections = mp.<span class="hljs-property">solutions</span>.<span class="hljs-property">hands</span>.<span class="hljs-property">HAND_CONNECTIONS</span>
            points1 = [points[connection[<span class="hljs-number">0</span>]] <span class="hljs-keyword">for</span> connection <span class="hljs-keyword">in</span> mp_hands_connections]
            points2 = [points[connection[<span class="hljs-number">1</span>]] <span class="hljs-keyword">for</span> connection <span class="hljs-keyword">in</span> mp_hands_connections]

            # 이미지와 <span class="hljs-title class_">Hand</span> <span class="hljs-title class_">Entity</span>에 연결 기록
            rr.<span class="hljs-title function_">log</span>(
               <span class="hljs-string">"Media/Connections"</span>,
                rr.<span class="hljs-title class_">LineStrips2D</span>(
                   np.<span class="hljs-title function_">stack</span>((points1, points2), axis=<span class="hljs-number">1</span>),
                   colors=[<span class="hljs-number">255</span>, <span class="hljs-number">165</span>, <span class="hljs-number">0</span>]
                )
             )

    def <span class="hljs-title function_">convert_landmarks_to_image_coordinates</span>(hand_landmarks, width, height):
        <span class="hljs-keyword">return</span> [(<span class="hljs-title function_">int</span>(lm.<span class="hljs-property">x</span> * width), <span class="hljs-title function_">int</span>(lm.<span class="hljs-property">y</span> * height)) <span class="hljs-keyword">for</span> hand_landmark <span class="hljs-keyword">in</span> hand_landmarks <span class="hljs-keyword">for</span> lm <span class="hljs-keyword">in</span> hand_landmark]
</code></pre>
<h2>3D Points</h2>
<p>마지막으로, 손 랜드마크를 3D 포인트로 표시하는 방법을 살펴봅니다. 먼저 init 함수에서 Annotation Context의 키포인트를 사용하여 포인트 사이의 연결을 정의한 다음 3D 포인트로 기록합니다.</p>
<div class="content-ad"></div>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*rNILX857c8TfScr6t7KKgQ.gif" alt="image"></p>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GestureDetectorLogger</span>:

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, video_mode: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):
        <span class="hljs-comment"># ... existing code ...</span>
        rr.log(
            <span class="hljs-string">"/"</span>,
            rr.AnnotationContext(
                rr.ClassDescription(
                    info=rr.AnnotationInfo(<span class="hljs-built_in">id</span>=<span class="hljs-number">0</span>, label=<span class="hljs-string">"Hand3D"</span>),
                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS
                )
            ),
            timeless=<span class="hljs-literal">True</span>
        )
        rr.log(<span class="hljs-string">"Hand3D"</span>, rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=<span class="hljs-literal">True</span>)


    <span class="hljs-keyword">def</span> <span class="hljs-title function_">detect_and_log</span>(<span class="hljs-params">self, image: npt.NDArray[np.uint8], frame_time_nano: <span class="hljs-built_in">int</span> | <span class="hljs-literal">None</span></span>) -> <span class="hljs-literal">None</span>:
        <span class="hljs-comment"># ... existing code ...</span>

        <span class="hljs-keyword">if</span> recognition_result.hand_landmarks:
            hand_landmarks = recognition_result.hand_landmarks

            landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)
            <span class="hljs-keyword">if</span> landmark_positions_3d <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                rr.log(
                    <span class="hljs-string">"Hand3D/Points"</span>,
                    rr.Points3D(landmark_positions_3d, radii=<span class="hljs-number">20</span>, class_ids=<span class="hljs-number">0</span>, keypoint_ids=[i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(landmark_positions_3d))])
                )

        <span class="hljs-comment"># ... existing code ...</span>
</code></pre>
<p>준비 완료! 마법이 시작됩니다:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># For image</span>
run_from_sample_image(IMAGE_PATH)

<span class="hljs-comment"># For saved video</span>
run_from_video_capture(VIDEO_PATH)

<span class="hljs-comment"># For Real-Time</span>
run_from_video_capture(<span class="hljs-number">0</span>) <span class="hljs-comment"># mac may need 1</span>
</code></pre>
<div class="content-ad"></div>
<p>이 예제의 전체 소스 코드는 GitHub에서 확인할 수 있습니다. 탐색하고 변경하며 구현의 내부 작업을 이해하는 데 자유롭게 사용하세요.</p>
<h1>손 추적 및 제스처 인식을 넘어서</h1>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*zc2gezkjPuJMjuToD4gBOw.gif" alt="이미지"></p>
<p>마침내, 다양한 애플리케이션 범위에서 다양한 종류의 다중 모달 데이터를 시각화하는 데 관심이 있다면, Rerun Examples를 살펴보고 탐구할 것을 권장합니다. 이러한 예제는 잠재적인 현실 세계 사례를 강조하고 그러한 시각화 기술의 실용적인 응용에 대한 소중한 통찰력을 제공합니다.</p>
<div class="content-ad"></div>
<p>만약 이 글이 유익하고 통찰력이 있었다면, 더 많은 내용을 기대해주세요! 나는 로봇공학과 컴퓨터 비전 시각화 게시물에 대해 깊이 있는 내용을 정기적으로 공유하고 있습니다. 놓치고 싶지 않은 미래 업데이트와 흥미로운 프로젝트를 위해 팔로우해주세요!</p>
<p>또한, LinkedIn에서 저를 찾을 수 있습니다.</p>
<p>비슷한 글:</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스","description":"","date":"2024-06-19 18:31","slug":"2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase","content":"\n\n## 미디어파이프의 손 추척 및 제스처 인식을 Rerun과 함께 시각화하는 방법\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*pE_4QrsVPV7vMrxB6YS1cQ.gif)\n\n이 게시물에서는 미디어파이프 파이썬과 Rerun SDK를 사용하여 손 추척 및 제스처 인식의 예제를 소개하고 있습니다.\n\n더 깊이 파고들고 이해를 넓히고 싶다면, 미디어파이프 파이썬 및 Rerun SDK를 설치하여 손을 추적하고 다양한 제스처를 인식하고 데이터를 시각화하는 방법을 안내해 드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 그러므로, 다음을 배울 것입니다:\n\n- MediaPipe Python 및 Rerun 설치하는 방법\n- MediaPipe 제스처 인식을 사용한 손 추적 및 제스처 인식 방법\n- 손 추적 및 제스처 인식 결과를 Rerun Viewer에서 시각화하는 방법\n\n예제를 시도하기를 열망한다면, 아래 제공된 코드를 사용해보세요:\n\n```js\n# rerun GitHub 저장소를 로컬 머신에 클론합니다.\ngit clone https://github.com/rerun-io/rerun\n\n# rerun 저장소 디렉토리로 이동합니다.\ncd rerun\n\n# 필요한 Python 패키지를 requirements 파일에 명시된대로 설치합니다.\npip install -r examples/python/gesture_detection/requirements.txt\n\n# 예제를 위한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py\n\n# 특정 이미지에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --image path/to/your/image.jpg\n\n# 특정 비디오에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --video path/to/your/video.mp4\n\n# 카메라 스트림으로 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --camera\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 손 추적 및 제스처 인식 기술\n\n계속하기 전에, 우리가 가능하게 한 기술에 대해 인정해주어야 합니다. 손 추적 및 제스처 인식 기술은 기기가 손 움직임과 제스처를 명령이나 입력으로 해석할 수 있도록 하는 것을 목표로 합니다. 이 기술의 핵심은 미리 훈련된 기계 학습 모델이 시각 입력을 분석하고 손의 랜드마크와 제스처를 식별합니다. 이러한 기술의 실제 응용은 다양하며, 손 움직임과 제스처를 사용하여 스마트 기기를 제어하는 데 사용될 수 있습니다. 인간-컴퓨터 상호 작용, 로봇 공학, 게임 및 증강 현실은 이 기술의 잠재적인 응용 분야 중 가장 유망하게 보입니다.\n\n그러나 이러한 기술을 사용하는 방법에 대해 항상 주의해야 합니다. 민감하고 중요한 시스템에서 사용시 손 제스처를 잘못 해석할 수 있고, 잘못된 양성 또는 음성의 가능성이 작지 않습니다. 이를 활용함으로써 발생하는 윤리적 및 법적 문제가 사용자들이 특히 공공장소에서 자신의 제스처가 기록되는 것을 원치 않을 수 있습니다. 현실 세계 시나리오에서 이 기술을 도입하기로 결정했다면, 윤리적 및 법적 고려 사항을 고려하는 것이 중요합니다.\n\n# 요구 사항 및 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 필요한 라이브러리를 설치해야 합니다. 이는 OpenCV, MediaPipe 및 Rerun과 같은 라이브러리를 포함합니다. MediaPipe Python은 컴퓨터 비전 및 머신러닝을 위한 온디바이스 ML 솔루션을 통합하려는 개발자들에게 유용한 도구이며, Rerun은 시간이 지남에 따라 변화하는 다중 모달 데이터를 시각화하기 위한 SDK입니다.\n\n```js\n# 요구 사항 파일에서 지정된 필수 Python 패키지 설치\npip install -r examples/python/gesture_detection/requirements.txt\n```\n\n그런 다음, 여기서 미리 정의된 모델을 다운로드해야 합니다: HandGestureClassifier\n\n# MediaPipe를 사용한 손 추적과 제스처 인식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png\" /\u003e\n\n이제 샘플 이미지에 제스처 인식을 위해 MediaPipe 사전 훈련 모델을 사용해봅시다. 아래 코드는 MediaPipe 제스처 인식 솔루션의 초기화 및 구성을 설정하는 기초를 제공합니다.\n\n```js\nfrom mediapipe.tasks.python import vision\nfrom mediapipe.tasks import python\n\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        self._video_mode = video_mode\n\n        base_options = python.BaseOptions(\n            model_asset_path='gesture_recognizer.task'\n        )\n        options = vision.GestureRecognizerOptions(\n            base_options=base_options,\n            running_mode=mp.tasks.vision.RunningMode.VIDEO if self._video_mode else mp.tasks.vision.RunningMode.IMAGE\n        )\n        self.recognizer = vision.GestureRecognizer.create_from_options(options)\n\n\n    def detect(self, image: npt.NDArray[np.uint8]) -\u003e None:\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n  \n        # 제스처 검출 모델로부터 결과 가져오기\n        recognition_result = self.recognizer.recognize(image)\n  \n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처 중 상위 제스처 가져오기\n            print(\"최상위 제스처 결과: \", gesture[0].category_name)\n  \n        if recognition_result.hand_landmarks:\n            # MediaPipe에서 손 랜드마크 가져오기\n            hand_landmarks = recognition_result.hand_landmarks\n            print(\"손 랜드마크: \" + str(hand_landmarks))\n  \n            # MediaPipe에서 손 연결 정보 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            print(\"손 연결 정보: \" + str(mp_hands_connections))\n```\n\nGestureDetectorLogger 클래스 내의 detect 함수는 이미지를 인자로 받아 모델 결과를 출력하며, 인식된 최상위 제스처와 감지된 손 랜드마크를 강조합니다. 모델에 대한 추가 정보는 해당 모델 카드를 참조하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_1.png)\n\n아래 코드를 사용하여 직접 시도해볼 수 있어요:\n\n```js\ndef run_from_sample_image(path)-\u003e None:\n    image = cv2.imread(str(path))\n    show_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    logger = GestureDetectorLogger(video_mode=False)\n    logger.detect_and_log(show_image)\n\n# 샘플 이미지로 제스처 인식 실행하기\nrun_from_sample_image(SAMPLE_IMAGE_PATH)\n```\n\n# 재실행을 사용하여 확인, 디버그 및 데모하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 단계는 솔루션의 신뢰성과 효과성을 보장하는 데 도움이 됩니다. 모델을 준비한 상태로 결과를 시각화하여 정확성을 확인하고 잠재적인 문제를 해결하며 능력을 시연할 수 있습니다. 결과를 시각화해서 Rerun SDK를 사용하면 간단하고 빠르게 가능합니다.\n\n## Rerun을 어떻게 사용할까요?\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_2.png)\n\n- Rerun SDK를 사용하여 코드에서 로깅하여 다중 데이터를 스트림으로 전송\n- 현지 또는 원격으로 라이브 또는 녹화된 스트림을 시각화하고 상호 작용\n- 레이아웃을 대화식으로 구축하고 시각화를 사용자 정의\n- 필요할 때 Rerun을 확장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 작성에 앞서, Rerun Viewer를 설치하기 위해 해당 페이지를 방문하는 것이 좋습니다. 그런 다음, Rerun SDK에 대해 읽어보는 것을 권해드립니다. 파이썬 빠른 시작 가이드와 파이썬에서 데이터 기록하기를 읽어보세요. 이러한 초기 단계는 원활한 설정을 보장하고 다가오는 코드 실행에 도움이 될 것입니다.\n\n## 비디오 또는 실시간 실행\n\n비디오 스트리밍에는 OpenCV가 사용됩니다. 특정 비디오의 파일 경로를 선택하거나 0 또는 1의 인수를 제공하여 자체 카메라에 액세스할 수 있습니다 (기본 카메라를 사용하려면 0을 사용하고, 맥에서는 1을 사용할 수 있습니다).\n\n타임라인의 소개를 강조하는 것이 중요합니다. Rerun 타임라인의 기능은 데이터를 하나 이상의 타임라인과 연관시킬 수 있게 합니다. 결과적으로, 비디오의 각 프레임은 해당 타임스탬프와 연관되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef run_from_video_capture(vid: int | str, max_frame_count: int | None) -\u003e None:\n    \"\"\"\n    비디오 스트림에서 탐지기를 실행합니다.\n\n    매개변수\n    ----------\n    vid:\n        탐지기가 실행될 비디오 스트림입니다. 기본 카메라에는 0/1을 사용하거나 비디오 파일의 경로를 지정하세요.\n    max_frame_count:\n        처리할 최대 프레임 수입니다. None이면 모든 프레임을 처리합니다.\n    \"\"\"\n    cap = cv2.VideoCapture(vid)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    detector = GestureDetectorLogger(video_mode=True)\n\n    try:\n        it: Iterable[int] = itertools.count() if max_frame_count is None else range(max_frame_count)\n\n        for frame_idx in tqdm.tqdm(it, desc=\"프레임 처리 중\"):\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            if np.all(frame == 0):\n                continue\n\n            frame_time_nano = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1e6)\n            if frame_time_nano == 0:\n                frame_time_nano = int(frame_idx * 1000 / fps * 1e6)\n\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            rr.set_time_sequence(\"프레임 번호\", frame_idx)\n            rr.set_time_nanos(\"프레임 시간\", frame_time_nano)\n            detector.detect_and_log(frame, frame_time_nano)\n            rr.log(\n                \"미디어/비디오\",\n                rr.Image(frame)\n            )\n\n    except KeyboardInterrupt:\n        pass\n\n    cap.release()\n    cv2.destroyAllWindows()\n```\n\n## 시각화를 위한 데이터 로깅\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*c1Us-7PoWSP0rgVdlMQUhA.gif\" /\u003e\n\nRerun Viewer에서 데이터를 시각화하려면 Rerun SDK를 사용하여 데이터를 로깅하는 것이 중요합니다. 이전에 언급된 가이드는 이 프로세스에 대한 통찰을 제공합니다. 이 문맥에서는 정규화된 값으로 손 랜드마크 포인트를 추출한 다음, 이미지의 너비와 높이를 사용하여 이미지 좌표로 변환합니다. 이러한 좌표는 2D 포인트로 Rerun SDK에 로깅됩니다. 추가로, 랜드마크 간의 연결을 식별하고 2D 라인스트립으로 로깅합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제스처 인식을 위해 결과는 콘솔에 출력됩니다. 그러나 소스 코드 안에서는 TextDocument 및 이모지를 사용하여 이러한 결과를 시청자에게 제시하는 방법을 탐구할 수 있습니다.\n\n```js\nclass GestureDetectorLogger:\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -\u003e None:\n        # 이미지에서 제스처 인식\n        height, width, _ = image.shape\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n\n        recognition_result = (\n            self.recognizer.recognize_for_video(image, int(frame_time_nano / 1e6))\n            if self._video_mode\n            else self.recognizer.recognize(image)\n        )\n\n        # 값 지우기\n        for log_key in [\"Media/Points\", \"Media/Connections\"]:\n            rr.log(log_key, rr.Clear(recursive=True))\n\n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처를 기록\n            gesture_category = gesture[0].category_name if recognition_result.gestures else \"None\"\n            print(\"제스처 카테고리:\", gesture_category)\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            # 정규화된 좌표를 이미지 좌표로 변환\n            points = self.convert_landmarks_to_image_coordinates(hand_landmarks, width, height)\n\n            # 이미지 및 Hand Entity에 점 기록\n            rr.log(\n               \"Media/Points\",\n                rr.Points2D(points, radii=10, colors=[255, 0, 0])\n            )\n\n            # MediaPipe에서 손 연결 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            points1 = [points[connection[0]] for connection in mp_hands_connections]\n            points2 = [points[connection[1]] for connection in mp_hands_connections]\n\n            # 이미지와 Hand Entity에 연결 기록\n            rr.log(\n               \"Media/Connections\",\n                rr.LineStrips2D(\n                   np.stack((points1, points2), axis=1),\n                   colors=[255, 165, 0]\n                )\n             )\n\n    def convert_landmarks_to_image_coordinates(hand_landmarks, width, height):\n        return [(int(lm.x * width), int(lm.y * height)) for hand_landmark in hand_landmarks for lm in hand_landmark]\n```\n\n## 3D Points\n\n마지막으로, 손 랜드마크를 3D 포인트로 표시하는 방법을 살펴봅니다. 먼저 init 함수에서 Annotation Context의 키포인트를 사용하여 포인트 사이의 연결을 정의한 다음 3D 포인트로 기록합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*rNILX857c8TfScr6t7KKgQ.gif)\n\n```python\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        # ... existing code ...\n        rr.log(\n            \"/\",\n            rr.AnnotationContext(\n                rr.ClassDescription(\n                    info=rr.AnnotationInfo(id=0, label=\"Hand3D\"),\n                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS\n                )\n            ),\n            timeless=True\n        )\n        rr.log(\"Hand3D\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -\u003e None:\n        # ... existing code ...\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)\n            if landmark_positions_3d is not None:\n                rr.log(\n                    \"Hand3D/Points\",\n                    rr.Points3D(landmark_positions_3d, radii=20, class_ids=0, keypoint_ids=[i for i in range(len(landmark_positions_3d))])\n                )\n\n        # ... existing code ...\n```\n\n준비 완료! 마법이 시작됩니다:\n\n```python\n# For image\nrun_from_sample_image(IMAGE_PATH)\n\n# For saved video\nrun_from_video_capture(VIDEO_PATH)\n\n# For Real-Time\nrun_from_video_capture(0) # mac may need 1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예제의 전체 소스 코드는 GitHub에서 확인할 수 있습니다. 탐색하고 변경하며 구현의 내부 작업을 이해하는 데 자유롭게 사용하세요.\n\n# 손 추적 및 제스처 인식을 넘어서\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*zc2gezkjPuJMjuToD4gBOw.gif)\n\n마침내, 다양한 애플리케이션 범위에서 다양한 종류의 다중 모달 데이터를 시각화하는 데 관심이 있다면, Rerun Examples를 살펴보고 탐구할 것을 권장합니다. 이러한 예제는 잠재적인 현실 세계 사례를 강조하고 그러한 시각화 기술의 실용적인 응용에 대한 소중한 통찰력을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 글이 유익하고 통찰력이 있었다면, 더 많은 내용을 기대해주세요! 나는 로봇공학과 컴퓨터 비전 시각화 게시물에 대해 깊이 있는 내용을 정기적으로 공유하고 있습니다. 놓치고 싶지 않은 미래 업데이트와 흥미로운 프로젝트를 위해 팔로우해주세요!\n\n또한, LinkedIn에서 저를 찾을 수 있습니다.\n\n비슷한 글:","ogImage":{"url":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png"},"coverImage":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png","tag":["Tech"],"readingTime":12},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e미디어파이프의 손 추척 및 제스처 인식을 Rerun과 함께 시각화하는 방법\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*pE_4QrsVPV7vMrxB6YS1cQ.gif\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 게시물에서는 미디어파이프 파이썬과 Rerun SDK를 사용하여 손 추척 및 제스처 인식의 예제를 소개하고 있습니다.\u003c/p\u003e\n\u003cp\u003e더 깊이 파고들고 이해를 넓히고 싶다면, 미디어파이프 파이썬 및 Rerun SDK를 설치하여 손을 추적하고 다양한 제스처를 인식하고 데이터를 시각화하는 방법을 안내해 드리겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e그러므로, 다음을 배울 것입니다:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eMediaPipe Python 및 Rerun 설치하는 방법\u003c/li\u003e\n\u003cli\u003eMediaPipe 제스처 인식을 사용한 손 추적 및 제스처 인식 방법\u003c/li\u003e\n\u003cli\u003e손 추적 및 제스처 인식 결과를 Rerun Viewer에서 시각화하는 방법\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e예제를 시도하기를 열망한다면, 아래 제공된 코드를 사용해보세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# rerun \u003cspan class=\"hljs-title class_\"\u003eGitHub\u003c/span\u003e 저장소를 로컬 머신에 클론합니다.\ngit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/rerun-io/rerun\u003c/span\u003e\n\n# rerun 저장소 디렉토리로 이동합니다.\ncd rerun\n\n# 필요한 \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 패키지를 requirements 파일에 명시된대로 설치합니다.\npip install -r examples/python/gesture_detection/requirements.\u003cspan class=\"hljs-property\"\u003etxt\u003c/span\u003e\n\n# 예제를 위한 주요 \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n# 특정 이미지에 대한 주요 \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --image path/to/your/image.\u003cspan class=\"hljs-property\"\u003ejpg\u003c/span\u003e\n\n# 특정 비디오에 대한 주요 \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --video path/to/your/video.\u003cspan class=\"hljs-property\"\u003emp4\u003c/span\u003e\n\n# 카메라 스트림으로 주요 \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --camera\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e손 추적 및 제스처 인식 기술\u003c/h1\u003e\n\u003cp\u003e계속하기 전에, 우리가 가능하게 한 기술에 대해 인정해주어야 합니다. 손 추적 및 제스처 인식 기술은 기기가 손 움직임과 제스처를 명령이나 입력으로 해석할 수 있도록 하는 것을 목표로 합니다. 이 기술의 핵심은 미리 훈련된 기계 학습 모델이 시각 입력을 분석하고 손의 랜드마크와 제스처를 식별합니다. 이러한 기술의 실제 응용은 다양하며, 손 움직임과 제스처를 사용하여 스마트 기기를 제어하는 데 사용될 수 있습니다. 인간-컴퓨터 상호 작용, 로봇 공학, 게임 및 증강 현실은 이 기술의 잠재적인 응용 분야 중 가장 유망하게 보입니다.\u003c/p\u003e\n\u003cp\u003e그러나 이러한 기술을 사용하는 방법에 대해 항상 주의해야 합니다. 민감하고 중요한 시스템에서 사용시 손 제스처를 잘못 해석할 수 있고, 잘못된 양성 또는 음성의 가능성이 작지 않습니다. 이를 활용함으로써 발생하는 윤리적 및 법적 문제가 사용자들이 특히 공공장소에서 자신의 제스처가 기록되는 것을 원치 않을 수 있습니다. 현실 세계 시나리오에서 이 기술을 도입하기로 결정했다면, 윤리적 및 법적 고려 사항을 고려하는 것이 중요합니다.\u003c/p\u003e\n\u003ch1\u003e요구 사항 및 설정\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e먼저, 필요한 라이브러리를 설치해야 합니다. 이는 OpenCV, MediaPipe 및 Rerun과 같은 라이브러리를 포함합니다. MediaPipe Python은 컴퓨터 비전 및 머신러닝을 위한 온디바이스 ML 솔루션을 통합하려는 개발자들에게 유용한 도구이며, Rerun은 시간이 지남에 따라 변화하는 다중 모달 데이터를 시각화하기 위한 SDK입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 요구 사항 파일에서 지정된 필수 \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 패키지 설치\npip install -r examples/python/gesture_detection/requirements.\u003cspan class=\"hljs-property\"\u003etxt\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그런 다음, 여기서 미리 정의된 모델을 다운로드해야 합니다: HandGestureClassifier\u003c/p\u003e\n\u003ch1\u003eMediaPipe를 사용한 손 추적과 제스처 인식\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png\"\u003e\n\u003cp\u003e이제 샘플 이미지에 제스처 인식을 위해 MediaPipe 사전 훈련 모델을 사용해봅시다. 아래 코드는 MediaPipe 제스처 인식 솔루션의 초기화 및 구성을 설정하는 기초를 제공합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e mediapipe.\u003cspan class=\"hljs-property\"\u003etasks\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epython\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e vision\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e mediapipe.\u003cspan class=\"hljs-property\"\u003etasks\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e python\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGestureDetectorLogger\u003c/span\u003e:\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, \u003cspan class=\"hljs-attr\"\u003evideo_mode\u003c/span\u003e: bool = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-property\"\u003e_video_mode\u003c/span\u003e = video_mode\n\n        base_options = python.\u003cspan class=\"hljs-title class_\"\u003eBaseOptions\u003c/span\u003e(\n            model_asset_path=\u003cspan class=\"hljs-string\"\u003e'gesture_recognizer.task'\u003c/span\u003e\n        )\n        options = vision.\u003cspan class=\"hljs-title class_\"\u003eGestureRecognizerOptions\u003c/span\u003e(\n            base_options=base_options,\n            running_mode=mp.\u003cspan class=\"hljs-property\"\u003etasks\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evision\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eRunningMode\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eVIDEO\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003e_video_mode\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e mp.\u003cspan class=\"hljs-property\"\u003etasks\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evision\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eRunningMode\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eIMAGE\u003c/span\u003e\n        )\n        self.\u003cspan class=\"hljs-property\"\u003erecognizer\u003c/span\u003e = vision.\u003cspan class=\"hljs-property\"\u003eGestureRecognizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ecreate_from_options\u003c/span\u003e(options)\n\n\n    def \u003cspan class=\"hljs-title function_\"\u003edetect\u003c/span\u003e(self, \u003cspan class=\"hljs-attr\"\u003eimage\u003c/span\u003e: npt.\u003cspan class=\"hljs-property\"\u003eNDArray\u003c/span\u003e[np.\u003cspan class=\"hljs-property\"\u003euint8\u003c/span\u003e]) -\u003e \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e:\n        image = mp.\u003cspan class=\"hljs-title class_\"\u003eImage\u003c/span\u003e(image_format=mp.\u003cspan class=\"hljs-property\"\u003eImageFormat\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eSRGB\u003c/span\u003e, data=image)\n  \n        # 제스처 검출 모델로부터 결과 가져오기\n        recognition_result = self.\u003cspan class=\"hljs-property\"\u003erecognizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erecognize\u003c/span\u003e(image)\n  \n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i, gesture \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eenumerate\u003c/span\u003e(recognition_result.\u003cspan class=\"hljs-property\"\u003egestures\u003c/span\u003e):\n            # 인식된 제스처 중 상위 제스처 가져오기\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"최상위 제스처 결과: \"\u003c/span\u003e, gesture[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003ecategory_name\u003c/span\u003e)\n  \n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e recognition_result.\u003cspan class=\"hljs-property\"\u003ehand_landmarks\u003c/span\u003e:\n            # \u003cspan class=\"hljs-title class_\"\u003eMediaPipe\u003c/span\u003e에서 손 랜드마크 가져오기\n            hand_landmarks = recognition_result.\u003cspan class=\"hljs-property\"\u003ehand_landmarks\u003c/span\u003e\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"손 랜드마크: \"\u003c/span\u003e + \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(hand_landmarks))\n  \n            # \u003cspan class=\"hljs-title class_\"\u003eMediaPipe\u003c/span\u003e에서 손 연결 정보 가져오기\n            mp_hands_connections = mp.\u003cspan class=\"hljs-property\"\u003esolutions\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ehands\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eHAND_CONNECTIONS\u003c/span\u003e\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"손 연결 정보: \"\u003c/span\u003e + \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(mp_hands_connections))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGestureDetectorLogger 클래스 내의 detect 함수는 이미지를 인자로 받아 모델 결과를 출력하며, 인식된 최상위 제스처와 감지된 손 랜드마크를 강조합니다. 모델에 대한 추가 정보는 해당 모델 카드를 참조하세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e아래 코드를 사용하여 직접 시도해볼 수 있어요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003erun_from_sample_image\u003c/span\u003e(path)-\u003e \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e:\n    image = cv2.\u003cspan class=\"hljs-title function_\"\u003eimread\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(path))\n    show_image = cv2.\u003cspan class=\"hljs-title function_\"\u003ecvtColor\u003c/span\u003e(image, cv2.\u003cspan class=\"hljs-property\"\u003eCOLOR_BGR2RGB\u003c/span\u003e)\n    logger = \u003cspan class=\"hljs-title class_\"\u003eGestureDetectorLogger\u003c/span\u003e(video_mode=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n    logger.\u003cspan class=\"hljs-title function_\"\u003edetect_and_log\u003c/span\u003e(show_image)\n\n# 샘플 이미지로 제스처 인식 실행하기\n\u003cspan class=\"hljs-title function_\"\u003erun_from_sample_image\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eSAMPLE_IMAGE_PATH\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e재실행을 사용하여 확인, 디버그 및 데모하기\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 단계는 솔루션의 신뢰성과 효과성을 보장하는 데 도움이 됩니다. 모델을 준비한 상태로 결과를 시각화하여 정확성을 확인하고 잠재적인 문제를 해결하며 능력을 시연할 수 있습니다. 결과를 시각화해서 Rerun SDK를 사용하면 간단하고 빠르게 가능합니다.\u003c/p\u003e\n\u003ch2\u003eRerun을 어떻게 사용할까요?\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRerun SDK를 사용하여 코드에서 로깅하여 다중 데이터를 스트림으로 전송\u003c/li\u003e\n\u003cli\u003e현지 또는 원격으로 라이브 또는 녹화된 스트림을 시각화하고 상호 작용\u003c/li\u003e\n\u003cli\u003e레이아웃을 대화식으로 구축하고 시각화를 사용자 정의\u003c/li\u003e\n\u003cli\u003e필요할 때 Rerun을 확장\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e코드 작성에 앞서, Rerun Viewer를 설치하기 위해 해당 페이지를 방문하는 것이 좋습니다. 그런 다음, Rerun SDK에 대해 읽어보는 것을 권해드립니다. 파이썬 빠른 시작 가이드와 파이썬에서 데이터 기록하기를 읽어보세요. 이러한 초기 단계는 원활한 설정을 보장하고 다가오는 코드 실행에 도움이 될 것입니다.\u003c/p\u003e\n\u003ch2\u003e비디오 또는 실시간 실행\u003c/h2\u003e\n\u003cp\u003e비디오 스트리밍에는 OpenCV가 사용됩니다. 특정 비디오의 파일 경로를 선택하거나 0 또는 1의 인수를 제공하여 자체 카메라에 액세스할 수 있습니다 (기본 카메라를 사용하려면 0을 사용하고, 맥에서는 1을 사용할 수 있습니다).\u003c/p\u003e\n\u003cp\u003e타임라인의 소개를 강조하는 것이 중요합니다. Rerun 타임라인의 기능은 데이터를 하나 이상의 타임라인과 연관시킬 수 있게 합니다. 결과적으로, 비디오의 각 프레임은 해당 타임스탬프와 연관되어 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003erun_from_video_capture\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003evid\u003c/span\u003e: int | str, \u003cspan class=\"hljs-attr\"\u003emax_frame_count\u003c/span\u003e: int | \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    비디오 스트림에서 탐지기를 실행합니다.\n\n    매개변수\n    ----------\n    vid:\n        탐지기가 실행될 비디오 스트림입니다. 기본 카메라에는 0/1을 사용하거나 비디오 파일의 경로를 지정하세요.\n    max_frame_count:\n        처리할 최대 프레임 수입니다. None이면 모든 프레임을 처리합니다.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    cap = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(vid)\n    fps = cap.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(cv2.\u003cspan class=\"hljs-property\"\u003eCAP_PROP_FPS\u003c/span\u003e)\n\n    detector = \u003cspan class=\"hljs-title class_\"\u003eGestureDetectorLogger\u003c/span\u003e(video_mode=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n        \u003cspan class=\"hljs-attr\"\u003eit\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIterable\u003c/span\u003e[int] = itertools.\u003cspan class=\"hljs-title function_\"\u003ecount\u003c/span\u003e() \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e max_frame_count is \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(max_frame_count)\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e frame_idx \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e tqdm.\u003cspan class=\"hljs-title function_\"\u003etqdm\u003c/span\u003e(it, desc=\u003cspan class=\"hljs-string\"\u003e\"프레임 처리 중\"\u003c/span\u003e):\n            ret, frame = cap.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e not \u003cspan class=\"hljs-attr\"\u003eret\u003c/span\u003e:\n                \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003eall\u003c/span\u003e(frame == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e):\n                \u003cspan class=\"hljs-keyword\"\u003econtinue\u003c/span\u003e\n\n            frame_time_nano = \u003cspan class=\"hljs-title function_\"\u003eint\u003c/span\u003e(cap.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(cv2.\u003cspan class=\"hljs-property\"\u003eCAP_PROP_POS_MSEC\u003c/span\u003e) * \u003cspan class=\"hljs-number\"\u003e1e6\u003c/span\u003e)\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e frame_time_nano == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n                frame_time_nano = \u003cspan class=\"hljs-title function_\"\u003eint\u003c/span\u003e(frame_idx * \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e / fps * \u003cspan class=\"hljs-number\"\u003e1e6\u003c/span\u003e)\n\n            frame = cv2.\u003cspan class=\"hljs-title function_\"\u003ecvtColor\u003c/span\u003e(frame, cv2.\u003cspan class=\"hljs-property\"\u003eCOLOR_BGR2RGB\u003c/span\u003e)\n\n            rr.\u003cspan class=\"hljs-title function_\"\u003eset_time_sequence\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"프레임 번호\"\u003c/span\u003e, frame_idx)\n            rr.\u003cspan class=\"hljs-title function_\"\u003eset_time_nanos\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"프레임 시간\"\u003c/span\u003e, frame_time_nano)\n            detector.\u003cspan class=\"hljs-title function_\"\u003edetect_and_log\u003c/span\u003e(frame, frame_time_nano)\n            rr.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(\n                \u003cspan class=\"hljs-string\"\u003e\"미디어/비디오\"\u003c/span\u003e,\n                rr.\u003cspan class=\"hljs-title class_\"\u003eImage\u003c/span\u003e(frame)\n            )\n\n    except \u003cspan class=\"hljs-title class_\"\u003eKeyboardInterrupt\u003c/span\u003e:\n        pass\n\n    cap.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\n    cv2.\u003cspan class=\"hljs-title function_\"\u003edestroyAllWindows\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e시각화를 위한 데이터 로깅\u003c/h2\u003e\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*c1Us-7PoWSP0rgVdlMQUhA.gif\"\u003e\n\u003cp\u003eRerun Viewer에서 데이터를 시각화하려면 Rerun SDK를 사용하여 데이터를 로깅하는 것이 중요합니다. 이전에 언급된 가이드는 이 프로세스에 대한 통찰을 제공합니다. 이 문맥에서는 정규화된 값으로 손 랜드마크 포인트를 추출한 다음, 이미지의 너비와 높이를 사용하여 이미지 좌표로 변환합니다. 이러한 좌표는 2D 포인트로 Rerun SDK에 로깅됩니다. 추가로, 랜드마크 간의 연결을 식별하고 2D 라인스트립으로 로깅합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e제스처 인식을 위해 결과는 콘솔에 출력됩니다. 그러나 소스 코드 안에서는 TextDocument 및 이모지를 사용하여 이러한 결과를 시청자에게 제시하는 방법을 탐구할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGestureDetectorLogger\u003c/span\u003e:\n\n    def \u003cspan class=\"hljs-title function_\"\u003edetect_and_log\u003c/span\u003e(self, \u003cspan class=\"hljs-attr\"\u003eimage\u003c/span\u003e: npt.\u003cspan class=\"hljs-property\"\u003eNDArray\u003c/span\u003e[np.\u003cspan class=\"hljs-property\"\u003euint8\u003c/span\u003e], \u003cspan class=\"hljs-attr\"\u003eframe_time_nano\u003c/span\u003e: int | \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e:\n        # 이미지에서 제스처 인식\n        height, width, _ = image.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e\n        image = mp.\u003cspan class=\"hljs-title class_\"\u003eImage\u003c/span\u003e(image_format=mp.\u003cspan class=\"hljs-property\"\u003eImageFormat\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eSRGB\u003c/span\u003e, data=image)\n\n        recognition_result = (\n            self.\u003cspan class=\"hljs-property\"\u003erecognizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erecognize_for_video\u003c/span\u003e(image, \u003cspan class=\"hljs-title function_\"\u003eint\u003c/span\u003e(frame_time_nano / \u003cspan class=\"hljs-number\"\u003e1e6\u003c/span\u003e))\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003e_video_mode\u003c/span\u003e\n            \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003erecognizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erecognize\u003c/span\u003e(image)\n        )\n\n        # 값 지우기\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e log_key \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e [\u003cspan class=\"hljs-string\"\u003e\"Media/Points\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"Media/Connections\"\u003c/span\u003e]:\n            rr.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(log_key, rr.\u003cspan class=\"hljs-title class_\"\u003eClear\u003c/span\u003e(recursive=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e))\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i, gesture \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eenumerate\u003c/span\u003e(recognition_result.\u003cspan class=\"hljs-property\"\u003egestures\u003c/span\u003e):\n            # 인식된 제스처를 기록\n            gesture_category = gesture[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003ecategory_name\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e recognition_result.\u003cspan class=\"hljs-property\"\u003egestures\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"None\"\u003c/span\u003e\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"제스처 카테고리:\"\u003c/span\u003e, gesture_category)\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e recognition_result.\u003cspan class=\"hljs-property\"\u003ehand_landmarks\u003c/span\u003e:\n            hand_landmarks = recognition_result.\u003cspan class=\"hljs-property\"\u003ehand_landmarks\u003c/span\u003e\n\n            # 정규화된 좌표를 이미지 좌표로 변환\n            points = self.\u003cspan class=\"hljs-title function_\"\u003econvert_landmarks_to_image_coordinates\u003c/span\u003e(hand_landmarks, width, height)\n\n            # 이미지 및 \u003cspan class=\"hljs-title class_\"\u003eHand\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eEntity\u003c/span\u003e에 점 기록\n            rr.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(\n               \u003cspan class=\"hljs-string\"\u003e\"Media/Points\"\u003c/span\u003e,\n                rr.\u003cspan class=\"hljs-title class_\"\u003ePoints2D\u003c/span\u003e(points, radii=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, colors=[\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n            )\n\n            # \u003cspan class=\"hljs-title class_\"\u003eMediaPipe\u003c/span\u003e에서 손 연결 가져오기\n            mp_hands_connections = mp.\u003cspan class=\"hljs-property\"\u003esolutions\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ehands\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eHAND_CONNECTIONS\u003c/span\u003e\n            points1 = [points[connection[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e connection \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e mp_hands_connections]\n            points2 = [points[connection[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e connection \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e mp_hands_connections]\n\n            # 이미지와 \u003cspan class=\"hljs-title class_\"\u003eHand\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eEntity\u003c/span\u003e에 연결 기록\n            rr.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(\n               \u003cspan class=\"hljs-string\"\u003e\"Media/Connections\"\u003c/span\u003e,\n                rr.\u003cspan class=\"hljs-title class_\"\u003eLineStrips2D\u003c/span\u003e(\n                   np.\u003cspan class=\"hljs-title function_\"\u003estack\u003c/span\u003e((points1, points2), axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e),\n                   colors=[\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e165\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n                )\n             )\n\n    def \u003cspan class=\"hljs-title function_\"\u003econvert_landmarks_to_image_coordinates\u003c/span\u003e(hand_landmarks, width, height):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [(\u003cspan class=\"hljs-title function_\"\u003eint\u003c/span\u003e(lm.\u003cspan class=\"hljs-property\"\u003ex\u003c/span\u003e * width), \u003cspan class=\"hljs-title function_\"\u003eint\u003c/span\u003e(lm.\u003cspan class=\"hljs-property\"\u003ey\u003c/span\u003e * height)) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e hand_landmark \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e hand_landmarks \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e lm \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e hand_landmark]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e3D Points\u003c/h2\u003e\n\u003cp\u003e마지막으로, 손 랜드마크를 3D 포인트로 표시하는 방법을 살펴봅니다. 먼저 init 함수에서 Annotation Context의 키포인트를 사용하여 포인트 사이의 연결을 정의한 다음 3D 포인트로 기록합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*rNILX857c8TfScr6t7KKgQ.gif\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGestureDetectorLogger\u003c/span\u003e:\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, video_mode: \u003cspan class=\"hljs-built_in\"\u003ebool\u003c/span\u003e = \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\u003c/span\u003e):\n        \u003cspan class=\"hljs-comment\"\u003e# ... existing code ...\u003c/span\u003e\n        rr.log(\n            \u003cspan class=\"hljs-string\"\u003e\"/\"\u003c/span\u003e,\n            rr.AnnotationContext(\n                rr.ClassDescription(\n                    info=rr.AnnotationInfo(\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, label=\u003cspan class=\"hljs-string\"\u003e\"Hand3D\"\u003c/span\u003e),\n                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS\n                )\n            ),\n            timeless=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n        )\n        rr.log(\u003cspan class=\"hljs-string\"\u003e\"Hand3D\"\u003c/span\u003e, rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003edetect_and_log\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, image: npt.NDArray[np.uint8], frame_time_nano: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e | \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        \u003cspan class=\"hljs-comment\"\u003e# ... existing code ...\u003c/span\u003e\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e landmark_positions_3d \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n                rr.log(\n                    \u003cspan class=\"hljs-string\"\u003e\"Hand3D/Points\"\u003c/span\u003e,\n                    rr.Points3D(landmark_positions_3d, radii=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, class_ids=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, keypoint_ids=[i \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(landmark_positions_3d))])\n                )\n\n        \u003cspan class=\"hljs-comment\"\u003e# ... existing code ...\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e준비 완료! 마법이 시작됩니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# For image\u003c/span\u003e\nrun_from_sample_image(IMAGE_PATH)\n\n\u003cspan class=\"hljs-comment\"\u003e# For saved video\u003c/span\u003e\nrun_from_video_capture(VIDEO_PATH)\n\n\u003cspan class=\"hljs-comment\"\u003e# For Real-Time\u003c/span\u003e\nrun_from_video_capture(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# mac may need 1\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 예제의 전체 소스 코드는 GitHub에서 확인할 수 있습니다. 탐색하고 변경하며 구현의 내부 작업을 이해하는 데 자유롭게 사용하세요.\u003c/p\u003e\n\u003ch1\u003e손 추적 및 제스처 인식을 넘어서\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*zc2gezkjPuJMjuToD4gBOw.gif\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e마침내, 다양한 애플리케이션 범위에서 다양한 종류의 다중 모달 데이터를 시각화하는 데 관심이 있다면, Rerun Examples를 살펴보고 탐구할 것을 권장합니다. 이러한 예제는 잠재적인 현실 세계 사례를 강조하고 그러한 시각화 기술의 실용적인 응용에 대한 소중한 통찰력을 제공합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e만약 이 글이 유익하고 통찰력이 있었다면, 더 많은 내용을 기대해주세요! 나는 로봇공학과 컴퓨터 비전 시각화 게시물에 대해 깊이 있는 내용을 정기적으로 공유하고 있습니다. 놓치고 싶지 않은 미래 업데이트와 흥미로운 프로젝트를 위해 팔로우해주세요!\u003c/p\u003e\n\u003cp\u003e또한, LinkedIn에서 저를 찾을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e비슷한 글:\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>