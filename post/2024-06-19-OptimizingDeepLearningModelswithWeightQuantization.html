<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>딥 러닝 모델 최적화를 위한 가중치 양자화 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="딥 러닝 모델 최적화를 위한 가중치 양자화 | itposting" data-gatsby-head="true"/><meta property="og:title" content="딥 러닝 모델 최적화를 위한 가중치 양자화 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization" data-gatsby-head="true"/><meta name="twitter:title" content="딥 러닝 모델 최적화를 위한 가중치 양자화 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 06:40" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">딥 러닝 모델 최적화를 위한 가중치 양자화</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="딥 러닝 모델 최적화를 위한 가중치 양자화" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">16<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png" alt="Image"></p>
<h1>📚딥러닝에서 양자화란?</h1>
<p>딥러닝에서 양자화에 대해 이야기해보겠습니다. 딥러닝에서 양자화가 왜 중요한지 궁금했던 적이 있나요? 딥러닝과 대규모 언어 모델(LLMs)이 아주 강력하다고는 하지만 많은 도전 과제를 가지고 있어요. 이러한 모델들이 크기 때문에, 많은 계산 성능과 메모리가 필요하여 자원이 제한된 곳에서 사용하기 어려워집니다. 게다가, 예측을 할 때 많은 에너지를 소비할 수 있어서, 한정된 컴퓨팅 자원으로 추론을 하는 것이 불가능해질 수도 있어요.</p>
<p>양자화는 이러한 문제를 해결하기 위해 모델의 크기를 줄여 더 쉽게 다루고, 거의 동일한 성능을 유지할 수 있도록 돕습니다. 이 과정은 모델의 매개변수 수와 데이터 유형의 정밀도를 수정하는 것을 포함합니다. 이를 통해 모델은 가볍고 빠르게 되어, 더 많은 곳에서 실행되고 더 적은 에너지를 사용할 수 있게 됩니다.</p>
<div class="content-ad"></div>
<p>모델의 크기는 매개변수(크기)의 수를 값들의 정밀도(데이터 형식)로 곱해서 계산됩니다.</p>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_1.png" alt="image"></p>
<p>그래서 모델의 크기를 효율적으로 줄이는 방법에 대한 중요한 질문은 무엇일까요? 음, 이를 위한 몇 가지 방법이 있습니다. 매개변수의 수를 줄이거나 데이터 형식을 낮추는 것이 가능합니다. 그러나 매개변수의 수를 줄이는 것은 모델을 더 작고 단순하게 만드는 것을 의미하며, 이는 모델의 품질에 상당한 영향을 줄 수 있어 매우 tricky할 수 있습니다. 더 나은 옵션은 데이터 형식의 정밀도를 조절하는 것입니다. 이것이 양자화가 등장하는 이유입니다 - 이를 통해 모델 가중치를 낮은 정밀도 형식으로 저장할 수 있습니다. 이 방법은 모델의 효과를 유지하면서 가볍고 빠르게 만들어줍니다.</p>
<p>아래는 양자화가 딥러닝에서 중요한 이유인 몇 가지 주요 이유입니다.</p>
<div class="content-ad"></div>
<ul>
<li>효율성: 양자화는 모델 내의 숫자 값의 정밀도를 부동 소수점에서 정수로 줄입니다. 이겈 간단해 보이지만, 계산을 훨씬 쉽고 빠르게 만들어주어 일을 빨리 처리할 수 있게 해줍니다!</li>
<li>메모리 절약: 부동 소수점에서 정수로 변환할 때 비트 수를 줄이면, 모델 크기를 크게 축소할 수 있습니다. 이것은 저장 공간과 메모리가 제한된 스마트폰이나 임베디드 시스템과 같은 기기에 모델을 배포할 때 아주 유용합니다.</li>
<li>에너지 소비: 모델 크기가 작아지면 모델을 실행하는 데 더 적은 계산 능력이 필요합니다. 이는 특히 배터리로 작동하는 기기에 모델을 배포할 때 유용합니다.</li>
<li>모델 배포: 모델이 작고 더 빠르게 실행될 때, 전용 대규모 서버 대신 다양한 장소에서 모델을 사용하기 쉬워집니다. 이는 자율 주행 자동차나 실시간 번역 서비스와 같이 빠른 응답이 필요한 작업에 중요합니다.</li>
</ul>
<h2>양자화 종류</h2>
<p>딥러닝에서 양자화는 일반적으로 세 가지 주요 유형으로 구분됩니다:</p>
<ul>
<li>사후 학습 정적 양자화 (PTQ): PTQ는 이미 훈련된 모델을 추가로 훈련하지 않고(가중치 및 활성화 모두) 줄이는 작업을 수행합니다. 사용하기 매우 간단하고, 훈련을 마친 후 모델을 빠르게 작게 만들어주는 데 도움이 됩니다. 단지 기억해 두세요! 훈련 중에 모델을 양자화하지 않기 때문에 원본 모델과 성능에 차이가 있을 수 있습니다.</li>
</ul>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_2.png" alt="그림"></p>
<ul>
<li>집행 후 다이나믹 양자화 또는 다이나믹 양자화: 이 방법은 훈련이 완료된 후에 모델 가중치를 줄이고, 활성화를 동적으로 처리합니다(추론 중에). 이 방법은 다른 유형과 크기의 입력을 다루는 모델에 아주 편리합니다. 그러나 모델이 실행되는 동안 활성화를 실시간으로 조정하기 때문에 정적 양자화보다 약간 느릴 수 있습니다. 또한, 이 방법의 또 다른 단점은 모든 장치가 이 동적 접근을 처리할 수 없다는 점이므로 어디에 이 방법을 사용할지 계획할 때 고려해야 합니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_3.png" alt="그림"></p>
<ul>
<li>양자화 인식 훈련(QAT): 마지막 공통 방법은 QAT입니다. 이는 양자화를 직접 훈련 과정에 통합하여 모델 성능을 유지합니다. 이것은 모델 최적화 중 양자화 효과를 고려함으로써 위 두 가지 방법보다 성능을 더 잘 보존할 수 있습니다. 결과적으로, QAT는 조금 더 많은 시간과 에너지를 요구합니다. 학습 작업 및 양자화를 동시에 조정하기 때문에 더 오래 훈련에 걸리고 구현하기는 훨씬 복잡합니다. 정확도가 필요한 경우, QAT는 모델을 효과적이고 효율적으로 유지하는 데 큰 차이를 만들 수 있습니다.</li>
</ul>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_4.png" alt="image"></p>
<h1>📖 부동 소수점 숫자 구성</h1>
<p>데이터 형식을 변경하면 모델 크기를 줄이는 이유에 대해 자세히 살펴보겠습니다. 컴퓨터에서 숫자에 대해 이야기할 때, 0과 1에 대해 모두 이야기합니다. 이진 인코딩 시스템은 컴퓨터 작업의 기초이며, 정수 및 부동 소수점 숫자와 같은 다양한 숫자 표현은 이러한 비트를 구성하는 특정 방법을 갖고 있습니다.</p>
<h2>정수 표현</h2>
<div class="content-ad"></div>
<p>정수에 대한 가장 일반적인 형식은 부호 있는 정수와 부호 없는 정수입니다.</p>
<p>부호 없는 정수:</p>
<ul>
<li>비트: 모든 비트가 숫자의 크기를 나타냅니다.</li>
<li>범위: 0부터 2n-1까지 (여기서 n은 비트의 수)입니다.</li>
</ul>
<p>부호 있는 정수:</p>
<div class="content-ad"></div>
<ul>
<li>첫 번째 비트는 숫자가 양수 (0)인지 음수 (1)인지를 나타냅니다.</li>
<li>나머지 비트는 숫자의 크기 또는 이른바 크기를 보여줍니다. 여기서 이진값은 음수 숫자에 대해 반전되고 1이 더해집니다.</li>
<li>범위: -2ⁿ⁻¹ ~ 2ⁿ⁻¹-1</li>
</ul>
<h2>부동 소수점 표현</h2>
<ul>
<li>부호 비트 (1 비트): 숫자의 부호를 나타냅니다; 0은 양수이고 1은 음수입니다.</li>
<li>지수: 바이어스로 조정된 지수를 나타냅니다. 저장된 지수에서 바이어스를 빼면 실제 지수가 계산됩니다. 지수는 사실적으로 숫자의 중요한(또는 가수) 부분을 2의 거듭제곱으로 확장하여 부동 소수점 숫자가 매우 크거나 매우 작은 값을 간결한 형식으로 표현할 수 있도록 합니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_5.png" alt="이미지"></p>
<div class="content-ad"></div>
<ul>
<li>유의적/맨티사: 숫자의 정밀도를 나타냅니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_6.png" alt="image"></p>
<h2>다른 데이터 유형의 생성</h2>
<p>Float32: 숫자를 나타내는 데 32비트를 사용합니다.</p>
<div class="content-ad"></div>
<ul>
<li>부호에는 1 비트가 사용됩니다</li>
<li>지수에는 8 비트가 사용됩니다</li>
<li>나머지 23 비트는 유효숫자를 나타냅니다</li>
<li>FP32는 높은 정밀도를 제공하지만, 계산 및 메모리 사용량이 많은 것이 단점입니다.</li>
</ul>
<p>이미지 링크:
<img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_7.png" alt="이미지"></p>
<p>Float16: 숫자를 저장하는 데 16 비트를 사용합니다</p>
<ul>
<li>부호에는 1이 사용됩니다</li>
<li>지수에는 5가 사용됩니다</li>
<li>유효숫자에 10이 사용됩니다</li>
<li>이로 인해 더 효율적인 메모리 사용 및 빠른 연산이 가능하지만, 범위 및 정밀도가 줄어들어 숫자의 불안정성을 초래할 수 있고, 이는 모델 정확도에 영향을 줄 수 있습니다.</li>
</ul>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_8.png">
<p>"float"이 종종 "전체 정밀도"(4 바이트)로 불리는 반면, "float16"은 "반 정밀도"(2 바이트)로 불립니다.</p>
<h2>일반적인 하위 정밀도 데이터 유형</h2>
<p>양자화를 수행하는 두 가지 일반적인 방법이 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>float32 -` float16</li>
<li>float32 -` int8</li>
</ul>
<h2>양자화의 효과</h2>
<p>대규모 모델인 BLOOM과 같은 경우, 약 1760억 개의 파라미터를 갖고 있는 모델을 다룬다고 상상해봅시다. float32를 사용하면 모델 크기는 176*10**9 x 4 바이트 = 704GB가 됩니다. 그러나 float16로 전환하면 352GB로 줄어들고, int8로 전환하면 176GB로 줄어듭니다. 이는 메모리 공간에서 굉장한 절감을 의미합니다. 176GB라도 여전히 많은 개인 컴퓨터에 대한 큰 도전이 될 수 있습니다.</p>
<p>float32에서 float16으로 양자화</p>
<div class="content-ad"></div>
<p>float32에서 float16로 변경하는 것은 꽤 간단합니다. 왜냐하면 두 형식 모두 숫자를 표현하는 방식이 비슷하기 때문이죠. 그러나 양자화를 구현하기 전에 몇 가지 고려해야 할 사항이 있습니다:</p>
<ul>
<li>소프트웨어 및 하드웨어 호환성: 먼저, 사용 중인 패키지가 float16을 처리할 수 있는지 확인해보세요. 또한, 하드웨어가 지원하는지도 확인해야 합니다. NVIDIA의 튜링 및 암페어 또는 구글의 TPU와 같은 현대 GPU 및 TPU는 float16과 잘 작동하도록 제작되었기 때문에 학습 및 추론 프로세스가 속도가 향상됩니다. 그러나 Intel CPU는 저장 유형으로 float16을 지원하고 있지만, 연산은 float32로 변환한 후에 수행됩니다.</li>
<li>정밀도 요구 사항: 모델이 얼마나 정밀해야 하는지를 고려해보세요. 다른 말로, 낮은 정밀도에 얼마나 민감한지 생각해보세요. 일부 의료 영상 처리와 같이 모든 작은 세부 사항이 중요한 작업/모델의 경우, float16과 같은 낮은 정밀도로 내려가면 중요한 세부 사항이 손실되어 모델의 성능에 영향을 줄 수 있습니다.</li>
</ul>
<p>float32에서 int8로의 양자화</p>
<p>float32에서 int8로의 양자화는 더 어렵습니다. 왜냐하면 int8은 256가지만 다룰 수 있고, 이는 float32가 다루는 광대한 범위와는 비교할 수 없이 작습니다. float32는 약 -3.4e38 ~ 3.4e38 범위에서 약 40억 개의 숫자를 처리합니다. 이 과제는 float32 값의 특정 범위를 int8의 매우 제한된 공간에 어떻게 압축할지 찾는 것입니다.</p>
<div class="content-ad"></div>
<p>계속해서 진행하고, 우리가 int8로 효과적으로 양자화하는 방법에 대해 자세히 살펴볼 거예요.</p>
<h1>int8로 양자화하는 방법</h1>
<h2>균일 양자화</h2>
<p>이 방법은 입력을 출력으로 매핑하는 간단한 선형 함수를 사용합니다. 선에 놓인 간격이 동일한 점들을 상상해보세요 — 균일 양자화는 이들이 변환될 때 모두 잘 정렬되어 있도록 유지합니다. 빠르고 쉽지만, 여기 주목할 점이 있어요: 데이터가 처음부터 고르게 분포되어 있지 않다면, 데이터의 분포를 항상 잘 보존하지는 못할 수도 있어요.</p>
<div class="content-ad"></div>
<p>모든 부동 소수점 값을 두 숫자 α와 β 사이에 매핑하는 것이 균일 양자화의 작동 원리입니다. 이 두 값을 α와 β라고 부르겠습니다. 그리고 그 값을 [-2ᵇ⁻¹, 2ᵇ⁻¹–1]의 일정 범위로 변환합니다. 이 범위를 벗어나는 값이 있다면 가장 가까운 한계값으로 잘립니다 — 이것을 클리핑이라고 합니다.</p>
<p>부동 소수점 숫자(xf)를 8비트 표현(xq)으로 변환할 때에는 스케일 팩터(S)를 사용합니다. 이는 원본 데이터를 int8의 새로운 형식 [-128, 127]에 맞춰주는 데 도움을 줍니다. 그리고 원본 데이터의 0은 새 데이터의 0과 일치하게 됩니다. 이것이 대칭 양자화라고 부르는 개념입니다.</p>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_9.png" alt="image"></p>
<h2>양자화 스케일(S)를 계산하는 방법?</h2>
<div class="content-ad"></div>
<p>아래는 Markdown 형식으로 바뀐 텍스트입니다:</p>
<p>Compute the max value of xf:</p>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_10.png" alt="image"></p>
<p>Compute the quantization scale (S):</p>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_11.png" alt="image"></p>
<div class="content-ad"></div>
<p>컨버팅 비티에이엘 테이블:</p>





















<table><thead><tr><th>값</th><th>양자화된 값</th></tr></thead><tbody><tr><td>3.14</td><td>3.0</td></tr><tr><td>-2.718</td><td>-2.5</td></tr><tr><td>6.626</td><td>6.5</td></tr></tbody></table>
<p>오리지널 값으로 돌아가기:</p>





















<table><thead><tr><th>양자화된 값</th><th>값</th></tr></thead><tbody><tr><td>3.0</td><td>3.14</td></tr><tr><td>-2.5</td><td>-2.718</td></tr><tr><td>6.5</td><td>6.626</td></tr></tbody></table>
<div class="content-ad"></div>
<p>대칭 양자화는 제로 주변을 균일하게 취급합니다. 즉, 데이터의 상승 및 하락(양수 및 음수 값)을 균형 있게 처리하여 모든 것이 균형을 이룹니다. 데이터가 제로 중심이거나 즉, 제로 양쪽으로 고르게 퍼져있을 때 특히 유용합니다.</p>
<p>하지만 여기서 중요한 점은 대칭 양자화가 제로 주변에 깔끔하게 정렬되지 않은 데이터에는 부적합할 수 있다는 것입니다. 데이터가 더 치우쳐져 있다면, 이 방법은 범위의 모든 부분을 동일하게 처리하기 때문에 더 많은 양자화 오류를 발생시킬 수 있습니다.</p>
<p>이 문제를 해결하기 위해, 비균일 또는 비대칭 양자화가 있습니다. 때로는 이를 아핀 양자화라고도 부릅니다. 이 기술은 데이터의 다른 부분에 대해 서로 다른 방식으로 스케일과 제로 포인트를 조정하기 때문에 대칭적으로 분포되지 않은 데이터 집합에 더 적합합니다.</p>
<p>간단한 예제로 이를 시도해 봅시다. 우리가 NumPy의 랜덤 정규 함수를 사용하여 가중치 배열을 만들었기 때문에 배열은 제로 중심입니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># 원본 가중치 배열
weights = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">normal</span>(size=(<span class="hljs-number">20000</span>)).<span class="hljs-title function_">astype</span>(np.<span class="hljs-property">float32</span>)
weights = torch.<span class="hljs-title function_">from_numpy</span>(weights)
<span class="hljs-title function_">print</span>(weights.<span class="hljs-title function_">mean</span>(), weights.<span class="hljs-title function_">min</span>(), weights.<span class="hljs-title function_">max</span>())
>>> <span class="hljs-title function_">tensor</span>(<span class="hljs-number">0.0057</span>) <span class="hljs-title function_">tensor</span>(-<span class="hljs-number">3.9224</span>) <span class="hljs-title function_">tensor</span>(<span class="hljs-number">4.4791</span>)
</code></pre>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_14.png" alt="Image"></p>
<pre><code class="hljs language-js"># 대칭 방식을 사용하여 양자화
weights_sym_quant, weights_sym_dequant = <span class="hljs-title function_">symmetric_quantize</span>(weights)
<span class="hljs-title function_">print</span>(weights_sym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">mean</span>(), weights_sym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">min</span>(), weights_sym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">max</span>())
<span class="hljs-title function_">print</span>(weights_sym_dequant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">mean</span>(), weights_sym_dequant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">min</span>(), weights_sym_dequant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">max</span>())
>>> <span class="hljs-title function_">tensor</span>(<span class="hljs-number">0.1585</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(-<span class="hljs-number">111.</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(<span class="hljs-number">127.</span>, dtype=torch.<span class="hljs-property">float64</span>)
>>> <span class="hljs-title function_">tensor</span>(<span class="hljs-number">0.0056</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(-<span class="hljs-number">3.9148</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(<span class="hljs-number">4.4791</span>, dtype=torch.<span class="hljs-property">float64</span>)
</code></pre>
<p>그런 다음 대칭 양자화 함수를 적용하면, 새로 양자화된 배열도 거의 0에 가까운 평균값을 가지며, 최솟값은 -111이고 최댓값은 127입니다.</p>
<div class="content-ad"></div>
<p>이제, 우리는 데이터를 원래의 부동 소수점 범위로 되돌리는 시도를 할 것입니다. 이것이 바로 양자화 해제(dequantization)라고 불리는 과정입니다. 양자화를 해제한 후에는, 양자화 해제된 배열의 평균, 최소값 및 최대값이 대략 원래 값과 동일해야 합니다.</p>
<h2>비균일 양자화</h2>
<p>비대칭 양자화의 경우, 양자화 값을 계산할 때 정수가 추가됩니다. 이것을 제로 포인트 (Z)라고 합니다. Z는 float32 영역에서 0의 값과 대응합니다.</p>
<p>양자화된 값 계산:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_15.png" alt="이미지"></p>
<p>스케일(S) 값을 계산하세요:</p>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_16.png" alt="이미지"></p>
<p>제로포인트(Z) 값을 계산하세요:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_17.png" alt="Optimizing Deep Learning Model with Weight Quantization"></p>
<p>비대칭 양자화는 범위의 다른 부분에 대해 스케일과 제로 포인트를 다르게 조정하여 비대칭 데이터 분포를 효과적으로 처리할 수 있습니다. 그러나 스케일과 제로 포인트 2개의 매개변수가 필요하기 때문에 구현 및 최적화 과정이 복잡해질 수 있고, 양자화 및 역양자화 단계에서 추가적인 계산 능력이 필요할 수 있습니다.</p>
<p>비대칭 양자화는 데이터 분포를 조정함으로써 데이터 범위 내에서 스케일과 제로 포인트를 다르게 조정하여 불규칙한 데이터 분포를 훌륭히 처리할 수 있습니다. 데이터가 양자화 다리를 건널 때 더 편안하게 걷도록 데이터의 신발을 맞춤 제작하는 것과 같은 원리입니다!</p>
<p>하지만 여기서 중요한 점은 두 가지 매개변수 - 스케일과 제로 포인트 - 를 사용자 정의하기 때문에 설정 및 세밀한 조정이 약간 까다로울 수 있습니다. 게다가, 양자화 및 역양자화 시에 조금 더 많은 계산 노력이 필요할 수 있습니다.</p>
<div class="content-ad"></div>
<h1>비대칭 방식을 사용하여 양자화하기 - 정규 분포 데이터</h1>
<p>weights_assym_quant, weights_assym_dequant = assymmetric_quantize(weights)
print(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())</p>
<blockquote>
<blockquote>
<blockquote>
<p>tensor(-8.8287, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)
tensor(0.0056, dtype=torch.float64) tensor(-3.9207, dtype=torch.float64) tensor(4.4808, dtype=torch.float64)</p>
</blockquote>
</blockquote>
</blockquote>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_18.png" alt="이미지"></p>
<p>정규 분포 배열 예제를 살펴보았지만, 이것은 간단한 경우입니다. 좀 더 어려운 비정규 분포를 가진 경우를 시도해 보겠습니다.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># 비정규 분포 데이터 생성</span>
skewed_weights = np.random.exponential(scale=<span class="hljs-number">2</span>, size=<span class="hljs-number">20000</span>) - <span class="hljs-number">7</span> <span class="hljs-comment"># 데이터를 음수 값과 양수 값을 모두 가지도록 이동</span>
skewed_weights = torch.from_numpy(skewed_weights)
<span class="hljs-built_in">print</span>(skewed_weights.mean(), skewed_weights.<span class="hljs-built_in">min</span>(), skewed_weights.<span class="hljs-built_in">max</span>())
<span class="hljs-meta">>>> </span>tensor(-<span class="hljs-number">5.0192</span>, dtype=torch.float64) tensor(-<span class="hljs-number">6.9999</span>, dtype=torch.float64) tensor(<span class="hljs-number">16.4827</span>, dtype=torch.float64)
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># 대칭 방식을 사용하여 양자화
weights_sym_quant, weights_sym_dequant = <span class="hljs-title function_">symmetric_quantize</span>(skewed_weights)
<span class="hljs-title function_">print</span>(weights_sym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">mean</span>(), weights_sym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">min</span>(), weights_sym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">max</span>())
>>> <span class="hljs-title function_">tensor</span>(-<span class="hljs-number">38.6737</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(-<span class="hljs-number">54.</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(<span class="hljs-number">127.</span>, dtype=torch.<span class="hljs-property">float64</span>)
</code></pre>
<p>이 분포는 정규분포가 아니기 때문에 양자화된 가중치의 평균 값은 -38.67, 최솟값은 -54이며 최대값은 127입니다. 문제는 전체 int8의 범위가 완전히 활용되지 않는다는 것입니다. 최솟값이 -64인데 이는 양자화가 사용 가능한 비트를 최대로 활용하지 못한다는 것을 의미합니다. 이로 인해 많은 서로 다른 값을 동일한 양자화된 값으로 반올림하여 고유성과 데이터 내의 세부 정보를 상실할 수 있습니다.</p>
<p>가중치를 다시 부동소수점으로 역양자화할 때, 평균값은 대략적으로 원래 가중치의 값에 도달합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># 비대칭 방법을 사용하여 양자화 - 정상 분포 데이터
weights_assym_quant, weights_assym_dequant = <span class="hljs-title function_">assymmetric_quantize</span>(skewed_weights)
<span class="hljs-title function_">print</span>(weights_assym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">mean</span>(), weights_assym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">min</span>(), weights_assym_quant.<span class="hljs-title function_">double</span>().<span class="hljs-title function_">max</span>())
>>> <span class="hljs-title function_">tensor</span>(-<span class="hljs-number">106.5096</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(-<span class="hljs-number">128.</span>, dtype=torch.<span class="hljs-property">float64</span>) <span class="hljs-title function_">tensor</span>(<span class="hljs-number">127.</span>, dtype=torch.<span class="hljs-property">float64</span>)
</code></pre>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_20.png" alt="이미지"></p>
<p>양자화된 값들을 원래의 부동 소수점 범위로 돌리면 어떻게 되는지 알아보겠습니다. 대칭 방법에서의 값들은 원래 데이터와 비교했을 때 그렇게 고른 분포를 보여주지 않는데, 비대칭 방법의 경우와는 다르게 퍼져 있는 것이 흥미롭습니다.</p>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_21.png" alt="이미지"></p>
<div class="content-ad"></div>
<h1>📝 코드 구현</h1>
<p>파이토치 양자화를 사용하여 양자화 예제를 작업해 보겠습니다.</p>
<p>이 예제에서는 MobileNetV2 모델과 MINIST 데이터셋을 사용할 것입니다. 데이터셋에 대한 자세한 내용은 여기에서, 그리고 데이터셋을 로드하는 방법은 여기에서 확인할 수 있습니다.</p>
<p>MobileNetV2를 양자화하려면 네트워크에 일부 수정을 구현해야 합니다.</p>
<div class="content-ad"></div>
<ul>
<li>In InvertedResidual 블록의 torch.add가 nn.quantized.FloatFunctional()로 대체되었습니다.</li>
</ul>
<pre><code class="hljs language-js">- self.<span class="hljs-property">skip_add</span> = torch.<span class="hljs-title function_">add</span>()
+ self.<span class="hljs-property">skip_add</span> = nn.<span class="hljs-property">quantized</span>.<span class="hljs-title class_">FloatFunctional</span>()
</code></pre>
<ul>
<li>양자화 전에 Conv+BN 및 Conv+BN+Relu 모듈을 결합하는 fuse_model() 메서드가 추가되어 메모리 액세스를 줄이고 수치 정확도를 향상시켜 모델의 효율성을 높입니다. 이 실천은 양자화된 모델에서 일반적입니다.</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MobileNetV2</span>(nn.<span class="hljs-property">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self, num_classes=<span class="hljs-number">10</span>, width_mult=<span class="hljs-number">1.0</span>, inverted_residual_setting=<span class="hljs-title class_">None</span>, round_nearest=<span class="hljs-number">8</span>):
+     self.<span class="hljs-property">quant</span> = <span class="hljs-title class_">QuantStub</span>()
+     self.<span class="hljs-property">dequant</span> = <span class="hljs-title class_">DeQuantStub</span>()
</code></pre>
<div class="content-ad"></div>
<p>PyTorch 프레임워크를 사용하여 모델을 양자화하는 일반적인 흐름은 다음과 같습니다:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># 양자화하기 전에 Conv+BN 및 Conv+BN+Relu 모듈을 퓨즈합니다 (이 작업은 숫자를 변경하지 않습니다)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">fuse_model</span>(<span class="hljs-params">self, is_qat=<span class="hljs-literal">False</span></span>):
    fuse_modules = torch.ao.quantization.fuse_modules_qat <span class="hljs-keyword">if</span> is_qat <span class="hljs-keyword">else</span> torch.ao.quantization.fuse_modules
    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == ConvBNReLU:
            fuse_modules(m, [<span class="hljs-string">'0'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'2'</span>], inplace=<span class="hljs-literal">True</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == InvertedResidual:
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(m.conv)):
                <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m.conv[idx]) == nn.Conv2d:
                    fuse_modules(m.conv, [<span class="hljs-built_in">str</span>(idx), <span class="hljs-built_in">str</span>(idx + <span class="hljs-number">1</span>)], inplace=<span class="hljs-literal">True</span>)
</code></pre>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_22.png" alt="이미지"></p>
<ul>
<li>QConfig를 사용하여 연산자가 어떻게 관찰되어야 하는지 구성합니다. 이 코드에서는 단순한 최소/최대 관찰자를 사용하여 양자화 매개변수를 결정합니다.</li>
</ul>
<div class="content-ad"></div>
<pre><code class="hljs language-js">quantized_model.<span class="hljs-property">qconfig</span> = torch.<span class="hljs-property">ao</span>.<span class="hljs-property">quantization</span>.<span class="hljs-property">default_qconfig</span>
</code></pre>
<ol start="2">
<li>준비하기: 지정된 qconfig를 기반으로 Observer/FakeQuantize 모듈을 모델에 삽입합니다.</li>
</ol>
<pre><code class="hljs language-js">torch.<span class="hljs-property">ao</span>.<span class="hljs-property">quantization</span>.<span class="hljs-title function_">prepare</span>(quantized_model, inplace=<span class="hljs-title class_">True</span>)
</code></pre>
<ol start="3">
<li>모델을 캘리브레이션하여 가중치와 활성화에 대한 양자화 매개변수를 결정합니다. 이는 훈련 데이터셋으로 수행됩니다.</li>
</ol>
<div class="content-ad"></div>
<p>evaluate(quantized_model, criterion, data_loader, neval_batches=num_calibration_batches)</p>
<ol start="4">
<li>Convert the calibrated model to a quantized model.</li>
</ol>
<p>torch.ao.quantization.convert(quantized_model, inplace=True)</p>
<p>We will load the pretrained model for the MNIST dataset as the original model, quantize this model, and compare the results in both size and performance. Performance is evaluated using Top 1 and Top 5 Accuracy.</p>
<div class="content-ad"></div>
<h2>👑결과👑</h2>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_23.png" alt="결과 이미지"></p>
<p>모델 크기가 8.9MB에서 2.35MB로 줄어든 건 정말 놀라운 일이에요! 거의 4배나 크기가 줄었어요! 🌟</p>
<p>성능도 매우 좋아서, 양자화된 모델의 최상위 1위와 최상위 5위 정확도는 원본과 비슷합니다. 최대/최소 옵서버만 사용해서 양자화 매개변수를 선택한 것에도 불구하고요. 그래서 거의 공간을 차지하지 않으면서도 약속받는 결과를 확인할 수 있어요!</p>
<div class="content-ad"></div>
<p>다른 옵저버의 결과를 확인해보면, 이 옵저버와 비교하여 성능이 어떤지 알 수 있어요. 새 옵저버는 자동으로 양자화 매개변수를 결정할 거에요.</p>
<p>디폴트 qconfig를 사용하는 대신에, 구성을 x86 아키텍처로 설정할 거에요. 이 아키텍처는 가중치를 채널 단위로 양자화하고, 활성화도를 수집하고 최적의 양자화 매개변수를 선택하는 히스토그램을 사용해요. 나머지 흐름은 동일하게 유지돼요.</p>
<pre><code class="hljs language-js">per_channel_quantized_model.<span class="hljs-property">qconfig</span> = torch.<span class="hljs-property">ao</span>.<span class="hljs-property">quantization</span>.<span class="hljs-title function_">get_default_qconfig</span>(<span class="hljs-string">'x86'</span>)
</code></pre>
<p><img src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_24.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>새 양자화 아키텍처로 얻는 결과는 Top 1 및 Top 5 성능 모두 강력한 출력을 유지한다는 것을 관찰할 수 있습니다. 그리고 가장 좋은 부분은 또 다른 양자화 방법과 모델 크기를 거의 동일하게 유지할 수 있다는 것입니다.</p>
<p><strong>Notebook</strong>: [링크]</p>
<h1>📕 최종 생각</h1>
<p>마무리하며, 사후 훈련 동적 양자화는 머신러닝 모델을 배포하기 위해 최적화하는 편리하고 효율적인 요령입니다. 이 방법은 모든 훈련이 완료된 후 가중치와 활성화를 조정함으로써 원래의 부동 소수점 모델과 가끔 수용 가능한 성능을 보장하며 동등한 성능을 나타낼 수 있습니다. AI 프로젝트를 빠르고 가벼워 만들고 싶다면, 이 방법이 진정한 게임 체인저가 될 수 있을 것입니다!</p>
<div class="content-ad"></div>
<h1>참고 자료</h1>
<ul>
<li>
<p><a href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/" rel="nofollow" target="_blank">Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with TensorRT</a></p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/quantization.html" rel="nofollow" target="_blank">PyTorch 공식 문서 - 양자화</a></p>
</li>
<li>
<p>MNIST 상업적 이용을 위한 라이선스: GNU General Public License v3.0. 링크: <a href="#">MNIST 라이선스</a></p>
</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"딥 러닝 모델 최적화를 위한 가중치 양자화","description":"","date":"2024-06-19 06:40","slug":"2024-06-19-OptimizingDeepLearningModelswithWeightQuantization","content":"\n\n![Image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png)\n\n# 📚딥러닝에서 양자화란?\n\n딥러닝에서 양자화에 대해 이야기해보겠습니다. 딥러닝에서 양자화가 왜 중요한지 궁금했던 적이 있나요? 딥러닝과 대규모 언어 모델(LLMs)이 아주 강력하다고는 하지만 많은 도전 과제를 가지고 있어요. 이러한 모델들이 크기 때문에, 많은 계산 성능과 메모리가 필요하여 자원이 제한된 곳에서 사용하기 어려워집니다. 게다가, 예측을 할 때 많은 에너지를 소비할 수 있어서, 한정된 컴퓨팅 자원으로 추론을 하는 것이 불가능해질 수도 있어요.\n\n양자화는 이러한 문제를 해결하기 위해 모델의 크기를 줄여 더 쉽게 다루고, 거의 동일한 성능을 유지할 수 있도록 돕습니다. 이 과정은 모델의 매개변수 수와 데이터 유형의 정밀도를 수정하는 것을 포함합니다. 이를 통해 모델은 가볍고 빠르게 되어, 더 많은 곳에서 실행되고 더 적은 에너지를 사용할 수 있게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델의 크기는 매개변수(크기)의 수를 값들의 정밀도(데이터 형식)로 곱해서 계산됩니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_1.png)\n\n그래서 모델의 크기를 효율적으로 줄이는 방법에 대한 중요한 질문은 무엇일까요? 음, 이를 위한 몇 가지 방법이 있습니다. 매개변수의 수를 줄이거나 데이터 형식을 낮추는 것이 가능합니다. 그러나 매개변수의 수를 줄이는 것은 모델을 더 작고 단순하게 만드는 것을 의미하며, 이는 모델의 품질에 상당한 영향을 줄 수 있어 매우 tricky할 수 있습니다. 더 나은 옵션은 데이터 형식의 정밀도를 조절하는 것입니다. 이것이 양자화가 등장하는 이유입니다 - 이를 통해 모델 가중치를 낮은 정밀도 형식으로 저장할 수 있습니다. 이 방법은 모델의 효과를 유지하면서 가볍고 빠르게 만들어줍니다.\n\n아래는 양자화가 딥러닝에서 중요한 이유인 몇 가지 주요 이유입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 효율성: 양자화는 모델 내의 숫자 값의 정밀도를 부동 소수점에서 정수로 줄입니다. 이겈 간단해 보이지만, 계산을 훨씬 쉽고 빠르게 만들어주어 일을 빨리 처리할 수 있게 해줍니다!\n- 메모리 절약: 부동 소수점에서 정수로 변환할 때 비트 수를 줄이면, 모델 크기를 크게 축소할 수 있습니다. 이것은 저장 공간과 메모리가 제한된 스마트폰이나 임베디드 시스템과 같은 기기에 모델을 배포할 때 아주 유용합니다.\n- 에너지 소비: 모델 크기가 작아지면 모델을 실행하는 데 더 적은 계산 능력이 필요합니다. 이는 특히 배터리로 작동하는 기기에 모델을 배포할 때 유용합니다.\n- 모델 배포: 모델이 작고 더 빠르게 실행될 때, 전용 대규모 서버 대신 다양한 장소에서 모델을 사용하기 쉬워집니다. 이는 자율 주행 자동차나 실시간 번역 서비스와 같이 빠른 응답이 필요한 작업에 중요합니다.\n\n## 양자화 종류\n\n딥러닝에서 양자화는 일반적으로 세 가지 주요 유형으로 구분됩니다:\n\n- 사후 학습 정적 양자화 (PTQ): PTQ는 이미 훈련된 모델을 추가로 훈련하지 않고(가중치 및 활성화 모두) 줄이는 작업을 수행합니다. 사용하기 매우 간단하고, 훈련을 마친 후 모델을 빠르게 작게 만들어주는 데 도움이 됩니다. 단지 기억해 두세요! 훈련 중에 모델을 양자화하지 않기 때문에 원본 모델과 성능에 차이가 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_2.png)\n\n- 집행 후 다이나믹 양자화 또는 다이나믹 양자화: 이 방법은 훈련이 완료된 후에 모델 가중치를 줄이고, 활성화를 동적으로 처리합니다(추론 중에). 이 방법은 다른 유형과 크기의 입력을 다루는 모델에 아주 편리합니다. 그러나 모델이 실행되는 동안 활성화를 실시간으로 조정하기 때문에 정적 양자화보다 약간 느릴 수 있습니다. 또한, 이 방법의 또 다른 단점은 모든 장치가 이 동적 접근을 처리할 수 없다는 점이므로 어디에 이 방법을 사용할지 계획할 때 고려해야 합니다.\n\n![그림](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_3.png)\n\n- 양자화 인식 훈련(QAT): 마지막 공통 방법은 QAT입니다. 이는 양자화를 직접 훈련 과정에 통합하여 모델 성능을 유지합니다. 이것은 모델 최적화 중 양자화 효과를 고려함으로써 위 두 가지 방법보다 성능을 더 잘 보존할 수 있습니다. 결과적으로, QAT는 조금 더 많은 시간과 에너지를 요구합니다. 학습 작업 및 양자화를 동시에 조정하기 때문에 더 오래 훈련에 걸리고 구현하기는 훨씬 복잡합니다. 정확도가 필요한 경우, QAT는 모델을 효과적이고 효율적으로 유지하는 데 큰 차이를 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_4.png)\n\n# 📖 부동 소수점 숫자 구성\n\n데이터 형식을 변경하면 모델 크기를 줄이는 이유에 대해 자세히 살펴보겠습니다. 컴퓨터에서 숫자에 대해 이야기할 때, 0과 1에 대해 모두 이야기합니다. 이진 인코딩 시스템은 컴퓨터 작업의 기초이며, 정수 및 부동 소수점 숫자와 같은 다양한 숫자 표현은 이러한 비트를 구성하는 특정 방법을 갖고 있습니다.\n\n## 정수 표현\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정수에 대한 가장 일반적인 형식은 부호 있는 정수와 부호 없는 정수입니다.\n\n부호 없는 정수:\n\n* 비트: 모든 비트가 숫자의 크기를 나타냅니다.\n* 범위: 0부터 2n-1까지 (여기서 n은 비트의 수)입니다.\n\n부호 있는 정수:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 첫 번째 비트는 숫자가 양수 (0)인지 음수 (1)인지를 나타냅니다.\n- 나머지 비트는 숫자의 크기 또는 이른바 크기를 보여줍니다. 여기서 이진값은 음수 숫자에 대해 반전되고 1이 더해집니다.\n- 범위: -2ⁿ⁻¹ ~ 2ⁿ⁻¹-1\n\n## 부동 소수점 표현\n\n- 부호 비트 (1 비트): 숫자의 부호를 나타냅니다; 0은 양수이고 1은 음수입니다.\n- 지수: 바이어스로 조정된 지수를 나타냅니다. 저장된 지수에서 바이어스를 빼면 실제 지수가 계산됩니다. 지수는 사실적으로 숫자의 중요한(또는 가수) 부분을 2의 거듭제곱으로 확장하여 부동 소수점 숫자가 매우 크거나 매우 작은 값을 간결한 형식으로 표현할 수 있도록 합니다.\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 유의적/맨티사: 숫자의 정밀도를 나타냅니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_6.png)\n\n## 다른 데이터 유형의 생성\n\nFloat32: 숫자를 나타내는 데 32비트를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 부호에는 1 비트가 사용됩니다\n- 지수에는 8 비트가 사용됩니다\n- 나머지 23 비트는 유효숫자를 나타냅니다\n- FP32는 높은 정밀도를 제공하지만, 계산 및 메모리 사용량이 많은 것이 단점입니다.\n\n이미지 링크:\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_7.png)\n\nFloat16: 숫자를 저장하는 데 16 비트를 사용합니다\n\n- 부호에는 1이 사용됩니다\n- 지수에는 5가 사용됩니다\n- 유효숫자에 10이 사용됩니다\n- 이로 인해 더 효율적인 메모리 사용 및 빠른 연산이 가능하지만, 범위 및 정밀도가 줄어들어 숫자의 불안정성을 초래할 수 있고, 이는 모델 정확도에 영향을 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_8.png\" /\u003e\n\n\"float\"이 종종 \"전체 정밀도\"(4 바이트)로 불리는 반면, \"float16\"은 \"반 정밀도\"(2 바이트)로 불립니다.\n\n## 일반적인 하위 정밀도 데이터 유형\n\n양자화를 수행하는 두 가지 일반적인 방법이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- float32 -` float16\n- float32 -` int8\n\n## 양자화의 효과\n\n대규모 모델인 BLOOM과 같은 경우, 약 1760억 개의 파라미터를 갖고 있는 모델을 다룬다고 상상해봅시다. float32를 사용하면 모델 크기는 176*10**9 x 4 바이트 = 704GB가 됩니다. 그러나 float16로 전환하면 352GB로 줄어들고, int8로 전환하면 176GB로 줄어듭니다. 이는 메모리 공간에서 굉장한 절감을 의미합니다. 176GB라도 여전히 많은 개인 컴퓨터에 대한 큰 도전이 될 수 있습니다.\n\nfloat32에서 float16으로 양자화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nfloat32에서 float16로 변경하는 것은 꽤 간단합니다. 왜냐하면 두 형식 모두 숫자를 표현하는 방식이 비슷하기 때문이죠. 그러나 양자화를 구현하기 전에 몇 가지 고려해야 할 사항이 있습니다:\n\n- 소프트웨어 및 하드웨어 호환성: 먼저, 사용 중인 패키지가 float16을 처리할 수 있는지 확인해보세요. 또한, 하드웨어가 지원하는지도 확인해야 합니다. NVIDIA의 튜링 및 암페어 또는 구글의 TPU와 같은 현대 GPU 및 TPU는 float16과 잘 작동하도록 제작되었기 때문에 학습 및 추론 프로세스가 속도가 향상됩니다. 그러나 Intel CPU는 저장 유형으로 float16을 지원하고 있지만, 연산은 float32로 변환한 후에 수행됩니다.\n- 정밀도 요구 사항: 모델이 얼마나 정밀해야 하는지를 고려해보세요. 다른 말로, 낮은 정밀도에 얼마나 민감한지 생각해보세요. 일부 의료 영상 처리와 같이 모든 작은 세부 사항이 중요한 작업/모델의 경우, float16과 같은 낮은 정밀도로 내려가면 중요한 세부 사항이 손실되어 모델의 성능에 영향을 줄 수 있습니다.\n\nfloat32에서 int8로의 양자화\n\nfloat32에서 int8로의 양자화는 더 어렵습니다. 왜냐하면 int8은 256가지만 다룰 수 있고, 이는 float32가 다루는 광대한 범위와는 비교할 수 없이 작습니다. float32는 약 -3.4e38 ~ 3.4e38 범위에서 약 40억 개의 숫자를 처리합니다. 이 과제는 float32 값의 특정 범위를 int8의 매우 제한된 공간에 어떻게 압축할지 찾는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계속해서 진행하고, 우리가 int8로 효과적으로 양자화하는 방법에 대해 자세히 살펴볼 거예요.\n\n# int8로 양자화하는 방법\n\n## 균일 양자화\n\n이 방법은 입력을 출력으로 매핑하는 간단한 선형 함수를 사용합니다. 선에 놓인 간격이 동일한 점들을 상상해보세요 — 균일 양자화는 이들이 변환될 때 모두 잘 정렬되어 있도록 유지합니다. 빠르고 쉽지만, 여기 주목할 점이 있어요: 데이터가 처음부터 고르게 분포되어 있지 않다면, 데이터의 분포를 항상 잘 보존하지는 못할 수도 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 부동 소수점 값을 두 숫자 α와 β 사이에 매핑하는 것이 균일 양자화의 작동 원리입니다. 이 두 값을 α와 β라고 부르겠습니다. 그리고 그 값을 [-2ᵇ⁻¹, 2ᵇ⁻¹–1]의 일정 범위로 변환합니다. 이 범위를 벗어나는 값이 있다면 가장 가까운 한계값으로 잘립니다 — 이것을 클리핑이라고 합니다.\n\n부동 소수점 숫자(xf)를 8비트 표현(xq)으로 변환할 때에는 스케일 팩터(S)를 사용합니다. 이는 원본 데이터를 int8의 새로운 형식 [-128, 127]에 맞춰주는 데 도움을 줍니다. 그리고 원본 데이터의 0은 새 데이터의 0과 일치하게 됩니다. 이것이 대칭 양자화라고 부르는 개념입니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_9.png)\n\n## 양자화 스케일(S)를 계산하는 방법?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 바뀐 텍스트입니다:\n\nCompute the max value of xf:\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_10.png)\n\nCompute the quantization scale (S):\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨버팅 비티에이엘 테이블:\n\n\n| 값       | 양자화된 값 |\n|----------|------------|\n|   3.14   |    3.0     |\n|  -2.718  |   -2.5     |\n|   6.626  |    6.5     |\n\n\n오리지널 값으로 돌아가기:\n\n\n| 양자화된 값 | 값       |\n|------------|----------|\n|    3.0     |   3.14   |\n|   -2.5     |  -2.718  |\n|    6.5     |   6.626  |\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대칭 양자화는 제로 주변을 균일하게 취급합니다. 즉, 데이터의 상승 및 하락(양수 및 음수 값)을 균형 있게 처리하여 모든 것이 균형을 이룹니다. 데이터가 제로 중심이거나 즉, 제로 양쪽으로 고르게 퍼져있을 때 특히 유용합니다.\n\n하지만 여기서 중요한 점은 대칭 양자화가 제로 주변에 깔끔하게 정렬되지 않은 데이터에는 부적합할 수 있다는 것입니다. 데이터가 더 치우쳐져 있다면, 이 방법은 범위의 모든 부분을 동일하게 처리하기 때문에 더 많은 양자화 오류를 발생시킬 수 있습니다.\n\n이 문제를 해결하기 위해, 비균일 또는 비대칭 양자화가 있습니다. 때로는 이를 아핀 양자화라고도 부릅니다. 이 기술은 데이터의 다른 부분에 대해 서로 다른 방식으로 스케일과 제로 포인트를 조정하기 때문에 대칭적으로 분포되지 않은 데이터 집합에 더 적합합니다.\n\n간단한 예제로 이를 시도해 봅시다. 우리가 NumPy의 랜덤 정규 함수를 사용하여 가중치 배열을 만들었기 때문에 배열은 제로 중심입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 원본 가중치 배열\nweights = np.random.normal(size=(20000)).astype(np.float32)\nweights = torch.from_numpy(weights)\nprint(weights.mean(), weights.min(), weights.max())\n\u003e\u003e\u003e tensor(0.0057) tensor(-3.9224) tensor(4.4791)\n```\n\n![Image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_14.png)\n\n```js\n# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = symmetric_quantize(weights)\nprint(weights_sym_quant.double().mean(), weights_sym_quant.double().min(), weights_sym_quant.double().max())\nprint(weights_sym_dequant.double().mean(), weights_sym_dequant.double().min(), weights_sym_dequant.double().max())\n\u003e\u003e\u003e tensor(0.1585, dtype=torch.float64) tensor(-111., dtype=torch.float64) tensor(127., dtype=torch.float64)\n\u003e\u003e\u003e tensor(0.0056, dtype=torch.float64) tensor(-3.9148, dtype=torch.float64) tensor(4.4791, dtype=torch.float64)\n```\n그런 다음 대칭 양자화 함수를 적용하면, 새로 양자화된 배열도 거의 0에 가까운 평균값을 가지며, 최솟값은 -111이고 최댓값은 127입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제, 우리는 데이터를 원래의 부동 소수점 범위로 되돌리는 시도를 할 것입니다. 이것이 바로 양자화 해제(dequantization)라고 불리는 과정입니다. 양자화를 해제한 후에는, 양자화 해제된 배열의 평균, 최소값 및 최대값이 대략 원래 값과 동일해야 합니다.\n\n## 비균일 양자화\n\n비대칭 양자화의 경우, 양자화 값을 계산할 때 정수가 추가됩니다. 이것을 제로 포인트 (Z)라고 합니다. Z는 float32 영역에서 0의 값과 대응합니다.\n\n양자화된 값 계산:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_15.png)\n\n스케일(S) 값을 계산하세요:\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_16.png)\n\n제로포인트(Z) 값을 계산하세요:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Optimizing Deep Learning Model with Weight Quantization](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_17.png)\n\n비대칭 양자화는 범위의 다른 부분에 대해 스케일과 제로 포인트를 다르게 조정하여 비대칭 데이터 분포를 효과적으로 처리할 수 있습니다. 그러나 스케일과 제로 포인트 2개의 매개변수가 필요하기 때문에 구현 및 최적화 과정이 복잡해질 수 있고, 양자화 및 역양자화 단계에서 추가적인 계산 능력이 필요할 수 있습니다.\n\n비대칭 양자화는 데이터 분포를 조정함으로써 데이터 범위 내에서 스케일과 제로 포인트를 다르게 조정하여 불규칙한 데이터 분포를 훌륭히 처리할 수 있습니다. 데이터가 양자화 다리를 건널 때 더 편안하게 걷도록 데이터의 신발을 맞춤 제작하는 것과 같은 원리입니다!\n\n하지만 여기서 중요한 점은 두 가지 매개변수 - 스케일과 제로 포인트 - 를 사용자 정의하기 때문에 설정 및 세밀한 조정이 약간 까다로울 수 있습니다. 게다가, 양자화 및 역양자화 시에 조금 더 많은 계산 노력이 필요할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 비대칭 방식을 사용하여 양자화하기 - 정규 분포 데이터\nweights_assym_quant, weights_assym_dequant = assymmetric_quantize(weights)\nprint(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())\n\u003e\u003e\u003e tensor(-8.8287, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)\n\u003e\u003e\u003e tensor(0.0056, dtype=torch.float64) tensor(-3.9207, dtype=torch.float64) tensor(4.4808, dtype=torch.float64)\n\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_18.png)\n\n정규 분포 배열 예제를 살펴보았지만, 이것은 간단한 경우입니다. 좀 더 어려운 비정규 분포를 가진 경우를 시도해 보겠습니다.\n\n```python\n# 비정규 분포 데이터 생성\nskewed_weights = np.random.exponential(scale=2, size=20000) - 7 # 데이터를 음수 값과 양수 값을 모두 가지도록 이동\nskewed_weights = torch.from_numpy(skewed_weights)\nprint(skewed_weights.mean(), skewed_weights.min(), skewed_weights.max())\n\u003e\u003e\u003e tensor(-5.0192, dtype=torch.float64) tensor(-6.9999, dtype=torch.float64) tensor(16.4827, dtype=torch.float64)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = symmetric_quantize(skewed_weights)\nprint(weights_sym_quant.double().mean(), weights_sym_quant.double().min(), weights_sym_quant.double().max())\n\u003e\u003e\u003e tensor(-38.6737, dtype=torch.float64) tensor(-54., dtype=torch.float64) tensor(127., dtype=torch.float64)\n```\n\n이 분포는 정규분포가 아니기 때문에 양자화된 가중치의 평균 값은 -38.67, 최솟값은 -54이며 최대값은 127입니다. 문제는 전체 int8의 범위가 완전히 활용되지 않는다는 것입니다. 최솟값이 -64인데 이는 양자화가 사용 가능한 비트를 최대로 활용하지 못한다는 것을 의미합니다. 이로 인해 많은 서로 다른 값을 동일한 양자화된 값으로 반올림하여 고유성과 데이터 내의 세부 정보를 상실할 수 있습니다.\n\n가중치를 다시 부동소수점으로 역양자화할 때, 평균값은 대략적으로 원래 가중치의 값에 도달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 비대칭 방법을 사용하여 양자화 - 정상 분포 데이터\nweights_assym_quant, weights_assym_dequant = assymmetric_quantize(skewed_weights)\nprint(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())\n\u003e\u003e\u003e tensor(-106.5096, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_20.png)\n\n양자화된 값들을 원래의 부동 소수점 범위로 돌리면 어떻게 되는지 알아보겠습니다. 대칭 방법에서의 값들은 원래 데이터와 비교했을 때 그렇게 고른 분포를 보여주지 않는데, 비대칭 방법의 경우와는 다르게 퍼져 있는 것이 흥미롭습니다.\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_21.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 📝 코드 구현\n\n파이토치 양자화를 사용하여 양자화 예제를 작업해 보겠습니다.\n\n이 예제에서는 MobileNetV2 모델과 MINIST 데이터셋을 사용할 것입니다. 데이터셋에 대한 자세한 내용은 여기에서, 그리고 데이터셋을 로드하는 방법은 여기에서 확인할 수 있습니다.\n\nMobileNetV2를 양자화하려면 네트워크에 일부 수정을 구현해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- In InvertedResidual 블록의 torch.add가 nn.quantized.FloatFunctional()로 대체되었습니다.\n\n```js\n- self.skip_add = torch.add()\n+ self.skip_add = nn.quantized.FloatFunctional()\n```\n\n- 양자화 전에 Conv+BN 및 Conv+BN+Relu 모듈을 결합하는 fuse_model() 메서드가 추가되어 메모리 액세스를 줄이고 수치 정확도를 향상시켜 모델의 효율성을 높입니다. 이 실천은 양자화된 모델에서 일반적입니다.\n\n```js\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=10, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n+     self.quant = QuantStub()\n+     self.dequant = DeQuantStub()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPyTorch 프레임워크를 사용하여 모델을 양자화하는 일반적인 흐름은 다음과 같습니다:\n\n```python\n# 양자화하기 전에 Conv+BN 및 Conv+BN+Relu 모듈을 퓨즈합니다 (이 작업은 숫자를 변경하지 않습니다)\ndef fuse_model(self, is_qat=False):\n    fuse_modules = torch.ao.quantization.fuse_modules_qat if is_qat else torch.ao.quantization.fuse_modules\n    for m in self.modules():\n        if type(m) == ConvBNReLU:\n            fuse_modules(m, ['0', '1', '2'], inplace=True)\n        if type(m) == InvertedResidual:\n            for idx in range(len(m.conv)):\n                if type(m.conv[idx]) == nn.Conv2d:\n                    fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_22.png)\n\n- QConfig를 사용하여 연산자가 어떻게 관찰되어야 하는지 구성합니다. 이 코드에서는 단순한 최소/최대 관찰자를 사용하여 양자화 매개변수를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nquantized_model.qconfig = torch.ao.quantization.default_qconfig\n```\n\n2. 준비하기: 지정된 qconfig를 기반으로 Observer/FakeQuantize 모듈을 모델에 삽입합니다.\n\n```js\ntorch.ao.quantization.prepare(quantized_model, inplace=True)\n```\n\n3. 모델을 캘리브레이션하여 가중치와 활성화에 대한 양자화 매개변수를 결정합니다. 이는 훈련 데이터셋으로 수행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nevaluate(quantized_model, criterion, data_loader, neval_batches=num_calibration_batches)\n\n\n4. Convert the calibrated model to a quantized model.\n\n\ntorch.ao.quantization.convert(quantized_model, inplace=True)\n\n\nWe will load the pretrained model for the MNIST dataset as the original model, quantize this model, and compare the results in both size and performance. Performance is evaluated using Top 1 and Top 5 Accuracy.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 👑결과👑\n\n![결과 이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_23.png)\n\n모델 크기가 8.9MB에서 2.35MB로 줄어든 건 정말 놀라운 일이에요! 거의 4배나 크기가 줄었어요! 🌟\n\n성능도 매우 좋아서, 양자화된 모델의 최상위 1위와 최상위 5위 정확도는 원본과 비슷합니다. 최대/최소 옵서버만 사용해서 양자화 매개변수를 선택한 것에도 불구하고요. 그래서 거의 공간을 차지하지 않으면서도 약속받는 결과를 확인할 수 있어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 옵저버의 결과를 확인해보면, 이 옵저버와 비교하여 성능이 어떤지 알 수 있어요. 새 옵저버는 자동으로 양자화 매개변수를 결정할 거에요.\n\n디폴트 qconfig를 사용하는 대신에, 구성을 x86 아키텍처로 설정할 거에요. 이 아키텍처는 가중치를 채널 단위로 양자화하고, 활성화도를 수집하고 최적의 양자화 매개변수를 선택하는 히스토그램을 사용해요. 나머지 흐름은 동일하게 유지돼요.\n\n```js\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_24.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새 양자화 아키텍처로 얻는 결과는 Top 1 및 Top 5 성능 모두 강력한 출력을 유지한다는 것을 관찰할 수 있습니다. 그리고 가장 좋은 부분은 또 다른 양자화 방법과 모델 크기를 거의 동일하게 유지할 수 있다는 것입니다.\n\n**Notebook**: [링크]\n\n# 📕 최종 생각\n\n마무리하며, 사후 훈련 동적 양자화는 머신러닝 모델을 배포하기 위해 최적화하는 편리하고 효율적인 요령입니다. 이 방법은 모든 훈련이 완료된 후 가중치와 활성화를 조정함으로써 원래의 부동 소수점 모델과 가끔 수용 가능한 성능을 보장하며 동등한 성능을 나타낼 수 있습니다. AI 프로젝트를 빠르고 가벼워 만들고 싶다면, 이 방법이 진정한 게임 체인저가 될 수 있을 것입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- [Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with TensorRT](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/)\n  \n- [PyTorch 공식 문서 - 양자화](https://pytorch.org/docs/stable/quantization.html)\n  \n- MNIST 상업적 이용을 위한 라이선스: GNU General Public License v3.0. 링크: [MNIST 라이선스](#)","ogImage":{"url":"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png"},"coverImage":"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png","tag":["Tech"],"readingTime":16},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003ch1\u003e📚딥러닝에서 양자화란?\u003c/h1\u003e\n\u003cp\u003e딥러닝에서 양자화에 대해 이야기해보겠습니다. 딥러닝에서 양자화가 왜 중요한지 궁금했던 적이 있나요? 딥러닝과 대규모 언어 모델(LLMs)이 아주 강력하다고는 하지만 많은 도전 과제를 가지고 있어요. 이러한 모델들이 크기 때문에, 많은 계산 성능과 메모리가 필요하여 자원이 제한된 곳에서 사용하기 어려워집니다. 게다가, 예측을 할 때 많은 에너지를 소비할 수 있어서, 한정된 컴퓨팅 자원으로 추론을 하는 것이 불가능해질 수도 있어요.\u003c/p\u003e\n\u003cp\u003e양자화는 이러한 문제를 해결하기 위해 모델의 크기를 줄여 더 쉽게 다루고, 거의 동일한 성능을 유지할 수 있도록 돕습니다. 이 과정은 모델의 매개변수 수와 데이터 유형의 정밀도를 수정하는 것을 포함합니다. 이를 통해 모델은 가볍고 빠르게 되어, 더 많은 곳에서 실행되고 더 적은 에너지를 사용할 수 있게 됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e모델의 크기는 매개변수(크기)의 수를 값들의 정밀도(데이터 형식)로 곱해서 계산됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e그래서 모델의 크기를 효율적으로 줄이는 방법에 대한 중요한 질문은 무엇일까요? 음, 이를 위한 몇 가지 방법이 있습니다. 매개변수의 수를 줄이거나 데이터 형식을 낮추는 것이 가능합니다. 그러나 매개변수의 수를 줄이는 것은 모델을 더 작고 단순하게 만드는 것을 의미하며, 이는 모델의 품질에 상당한 영향을 줄 수 있어 매우 tricky할 수 있습니다. 더 나은 옵션은 데이터 형식의 정밀도를 조절하는 것입니다. 이것이 양자화가 등장하는 이유입니다 - 이를 통해 모델 가중치를 낮은 정밀도 형식으로 저장할 수 있습니다. 이 방법은 모델의 효과를 유지하면서 가볍고 빠르게 만들어줍니다.\u003c/p\u003e\n\u003cp\u003e아래는 양자화가 딥러닝에서 중요한 이유인 몇 가지 주요 이유입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e효율성: 양자화는 모델 내의 숫자 값의 정밀도를 부동 소수점에서 정수로 줄입니다. 이겈 간단해 보이지만, 계산을 훨씬 쉽고 빠르게 만들어주어 일을 빨리 처리할 수 있게 해줍니다!\u003c/li\u003e\n\u003cli\u003e메모리 절약: 부동 소수점에서 정수로 변환할 때 비트 수를 줄이면, 모델 크기를 크게 축소할 수 있습니다. 이것은 저장 공간과 메모리가 제한된 스마트폰이나 임베디드 시스템과 같은 기기에 모델을 배포할 때 아주 유용합니다.\u003c/li\u003e\n\u003cli\u003e에너지 소비: 모델 크기가 작아지면 모델을 실행하는 데 더 적은 계산 능력이 필요합니다. 이는 특히 배터리로 작동하는 기기에 모델을 배포할 때 유용합니다.\u003c/li\u003e\n\u003cli\u003e모델 배포: 모델이 작고 더 빠르게 실행될 때, 전용 대규모 서버 대신 다양한 장소에서 모델을 사용하기 쉬워집니다. 이는 자율 주행 자동차나 실시간 번역 서비스와 같이 빠른 응답이 필요한 작업에 중요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e양자화 종류\u003c/h2\u003e\n\u003cp\u003e딥러닝에서 양자화는 일반적으로 세 가지 주요 유형으로 구분됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e사후 학습 정적 양자화 (PTQ): PTQ는 이미 훈련된 모델을 추가로 훈련하지 않고(가중치 및 활성화 모두) 줄이는 작업을 수행합니다. 사용하기 매우 간단하고, 훈련을 마친 후 모델을 빠르게 작게 만들어주는 데 도움이 됩니다. 단지 기억해 두세요! 훈련 중에 모델을 양자화하지 않기 때문에 원본 모델과 성능에 차이가 있을 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_2.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e집행 후 다이나믹 양자화 또는 다이나믹 양자화: 이 방법은 훈련이 완료된 후에 모델 가중치를 줄이고, 활성화를 동적으로 처리합니다(추론 중에). 이 방법은 다른 유형과 크기의 입력을 다루는 모델에 아주 편리합니다. 그러나 모델이 실행되는 동안 활성화를 실시간으로 조정하기 때문에 정적 양자화보다 약간 느릴 수 있습니다. 또한, 이 방법의 또 다른 단점은 모든 장치가 이 동적 접근을 처리할 수 없다는 점이므로 어디에 이 방법을 사용할지 계획할 때 고려해야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_3.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e양자화 인식 훈련(QAT): 마지막 공통 방법은 QAT입니다. 이는 양자화를 직접 훈련 과정에 통합하여 모델 성능을 유지합니다. 이것은 모델 최적화 중 양자화 효과를 고려함으로써 위 두 가지 방법보다 성능을 더 잘 보존할 수 있습니다. 결과적으로, QAT는 조금 더 많은 시간과 에너지를 요구합니다. 학습 작업 및 양자화를 동시에 조정하기 때문에 더 오래 훈련에 걸리고 구현하기는 훨씬 복잡합니다. 정확도가 필요한 경우, QAT는 모델을 효과적이고 효율적으로 유지하는 데 큰 차이를 만들 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e📖 부동 소수점 숫자 구성\u003c/h1\u003e\n\u003cp\u003e데이터 형식을 변경하면 모델 크기를 줄이는 이유에 대해 자세히 살펴보겠습니다. 컴퓨터에서 숫자에 대해 이야기할 때, 0과 1에 대해 모두 이야기합니다. 이진 인코딩 시스템은 컴퓨터 작업의 기초이며, 정수 및 부동 소수점 숫자와 같은 다양한 숫자 표현은 이러한 비트를 구성하는 특정 방법을 갖고 있습니다.\u003c/p\u003e\n\u003ch2\u003e정수 표현\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e정수에 대한 가장 일반적인 형식은 부호 있는 정수와 부호 없는 정수입니다.\u003c/p\u003e\n\u003cp\u003e부호 없는 정수:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e비트: 모든 비트가 숫자의 크기를 나타냅니다.\u003c/li\u003e\n\u003cli\u003e범위: 0부터 2n-1까지 (여기서 n은 비트의 수)입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e부호 있는 정수:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e첫 번째 비트는 숫자가 양수 (0)인지 음수 (1)인지를 나타냅니다.\u003c/li\u003e\n\u003cli\u003e나머지 비트는 숫자의 크기 또는 이른바 크기를 보여줍니다. 여기서 이진값은 음수 숫자에 대해 반전되고 1이 더해집니다.\u003c/li\u003e\n\u003cli\u003e범위: -2ⁿ⁻¹ ~ 2ⁿ⁻¹-1\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e부동 소수점 표현\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e부호 비트 (1 비트): 숫자의 부호를 나타냅니다; 0은 양수이고 1은 음수입니다.\u003c/li\u003e\n\u003cli\u003e지수: 바이어스로 조정된 지수를 나타냅니다. 저장된 지수에서 바이어스를 빼면 실제 지수가 계산됩니다. 지수는 사실적으로 숫자의 중요한(또는 가수) 부분을 2의 거듭제곱으로 확장하여 부동 소수점 숫자가 매우 크거나 매우 작은 값을 간결한 형식으로 표현할 수 있도록 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e유의적/맨티사: 숫자의 정밀도를 나타냅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_6.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e다른 데이터 유형의 생성\u003c/h2\u003e\n\u003cp\u003eFloat32: 숫자를 나타내는 데 32비트를 사용합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e부호에는 1 비트가 사용됩니다\u003c/li\u003e\n\u003cli\u003e지수에는 8 비트가 사용됩니다\u003c/li\u003e\n\u003cli\u003e나머지 23 비트는 유효숫자를 나타냅니다\u003c/li\u003e\n\u003cli\u003eFP32는 높은 정밀도를 제공하지만, 계산 및 메모리 사용량이 많은 것이 단점입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이미지 링크:\n\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eFloat16: 숫자를 저장하는 데 16 비트를 사용합니다\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e부호에는 1이 사용됩니다\u003c/li\u003e\n\u003cli\u003e지수에는 5가 사용됩니다\u003c/li\u003e\n\u003cli\u003e유효숫자에 10이 사용됩니다\u003c/li\u003e\n\u003cli\u003e이로 인해 더 효율적인 메모리 사용 및 빠른 연산이 가능하지만, 범위 및 정밀도가 줄어들어 숫자의 불안정성을 초래할 수 있고, 이는 모델 정확도에 영향을 줄 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_8.png\"\u003e\n\u003cp\u003e\"float\"이 종종 \"전체 정밀도\"(4 바이트)로 불리는 반면, \"float16\"은 \"반 정밀도\"(2 바이트)로 불립니다.\u003c/p\u003e\n\u003ch2\u003e일반적인 하위 정밀도 데이터 유형\u003c/h2\u003e\n\u003cp\u003e양자화를 수행하는 두 가지 일반적인 방법이 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003efloat32 -` float16\u003c/li\u003e\n\u003cli\u003efloat32 -` int8\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e양자화의 효과\u003c/h2\u003e\n\u003cp\u003e대규모 모델인 BLOOM과 같은 경우, 약 1760억 개의 파라미터를 갖고 있는 모델을 다룬다고 상상해봅시다. float32를 사용하면 모델 크기는 176*10**9 x 4 바이트 = 704GB가 됩니다. 그러나 float16로 전환하면 352GB로 줄어들고, int8로 전환하면 176GB로 줄어듭니다. 이는 메모리 공간에서 굉장한 절감을 의미합니다. 176GB라도 여전히 많은 개인 컴퓨터에 대한 큰 도전이 될 수 있습니다.\u003c/p\u003e\n\u003cp\u003efloat32에서 float16으로 양자화\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003efloat32에서 float16로 변경하는 것은 꽤 간단합니다. 왜냐하면 두 형식 모두 숫자를 표현하는 방식이 비슷하기 때문이죠. 그러나 양자화를 구현하기 전에 몇 가지 고려해야 할 사항이 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e소프트웨어 및 하드웨어 호환성: 먼저, 사용 중인 패키지가 float16을 처리할 수 있는지 확인해보세요. 또한, 하드웨어가 지원하는지도 확인해야 합니다. NVIDIA의 튜링 및 암페어 또는 구글의 TPU와 같은 현대 GPU 및 TPU는 float16과 잘 작동하도록 제작되었기 때문에 학습 및 추론 프로세스가 속도가 향상됩니다. 그러나 Intel CPU는 저장 유형으로 float16을 지원하고 있지만, 연산은 float32로 변환한 후에 수행됩니다.\u003c/li\u003e\n\u003cli\u003e정밀도 요구 사항: 모델이 얼마나 정밀해야 하는지를 고려해보세요. 다른 말로, 낮은 정밀도에 얼마나 민감한지 생각해보세요. 일부 의료 영상 처리와 같이 모든 작은 세부 사항이 중요한 작업/모델의 경우, float16과 같은 낮은 정밀도로 내려가면 중요한 세부 사항이 손실되어 모델의 성능에 영향을 줄 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efloat32에서 int8로의 양자화\u003c/p\u003e\n\u003cp\u003efloat32에서 int8로의 양자화는 더 어렵습니다. 왜냐하면 int8은 256가지만 다룰 수 있고, 이는 float32가 다루는 광대한 범위와는 비교할 수 없이 작습니다. float32는 약 -3.4e38 ~ 3.4e38 범위에서 약 40억 개의 숫자를 처리합니다. 이 과제는 float32 값의 특정 범위를 int8의 매우 제한된 공간에 어떻게 압축할지 찾는 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e계속해서 진행하고, 우리가 int8로 효과적으로 양자화하는 방법에 대해 자세히 살펴볼 거예요.\u003c/p\u003e\n\u003ch1\u003eint8로 양자화하는 방법\u003c/h1\u003e\n\u003ch2\u003e균일 양자화\u003c/h2\u003e\n\u003cp\u003e이 방법은 입력을 출력으로 매핑하는 간단한 선형 함수를 사용합니다. 선에 놓인 간격이 동일한 점들을 상상해보세요 — 균일 양자화는 이들이 변환될 때 모두 잘 정렬되어 있도록 유지합니다. 빠르고 쉽지만, 여기 주목할 점이 있어요: 데이터가 처음부터 고르게 분포되어 있지 않다면, 데이터의 분포를 항상 잘 보존하지는 못할 수도 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e모든 부동 소수점 값을 두 숫자 α와 β 사이에 매핑하는 것이 균일 양자화의 작동 원리입니다. 이 두 값을 α와 β라고 부르겠습니다. 그리고 그 값을 [-2ᵇ⁻¹, 2ᵇ⁻¹–1]의 일정 범위로 변환합니다. 이 범위를 벗어나는 값이 있다면 가장 가까운 한계값으로 잘립니다 — 이것을 클리핑이라고 합니다.\u003c/p\u003e\n\u003cp\u003e부동 소수점 숫자(xf)를 8비트 표현(xq)으로 변환할 때에는 스케일 팩터(S)를 사용합니다. 이는 원본 데이터를 int8의 새로운 형식 [-128, 127]에 맞춰주는 데 도움을 줍니다. 그리고 원본 데이터의 0은 새 데이터의 0과 일치하게 됩니다. 이것이 대칭 양자화라고 부르는 개념입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_9.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e양자화 스케일(S)를 계산하는 방법?\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 Markdown 형식으로 바뀐 텍스트입니다:\u003c/p\u003e\n\u003cp\u003eCompute the max value of xf:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_10.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eCompute the quantization scale (S):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_11.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e컨버팅 비티에이엘 테이블:\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e값\u003c/th\u003e\u003cth\u003e양자화된 값\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e3.14\u003c/td\u003e\u003ctd\u003e3.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e-2.718\u003c/td\u003e\u003ctd\u003e-2.5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e6.626\u003c/td\u003e\u003ctd\u003e6.5\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e오리지널 값으로 돌아가기:\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e양자화된 값\u003c/th\u003e\u003cth\u003e값\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e3.0\u003c/td\u003e\u003ctd\u003e3.14\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e-2.5\u003c/td\u003e\u003ctd\u003e-2.718\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e6.5\u003c/td\u003e\u003ctd\u003e6.626\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e대칭 양자화는 제로 주변을 균일하게 취급합니다. 즉, 데이터의 상승 및 하락(양수 및 음수 값)을 균형 있게 처리하여 모든 것이 균형을 이룹니다. 데이터가 제로 중심이거나 즉, 제로 양쪽으로 고르게 퍼져있을 때 특히 유용합니다.\u003c/p\u003e\n\u003cp\u003e하지만 여기서 중요한 점은 대칭 양자화가 제로 주변에 깔끔하게 정렬되지 않은 데이터에는 부적합할 수 있다는 것입니다. 데이터가 더 치우쳐져 있다면, 이 방법은 범위의 모든 부분을 동일하게 처리하기 때문에 더 많은 양자화 오류를 발생시킬 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이 문제를 해결하기 위해, 비균일 또는 비대칭 양자화가 있습니다. 때로는 이를 아핀 양자화라고도 부릅니다. 이 기술은 데이터의 다른 부분에 대해 서로 다른 방식으로 스케일과 제로 포인트를 조정하기 때문에 대칭적으로 분포되지 않은 데이터 집합에 더 적합합니다.\u003c/p\u003e\n\u003cp\u003e간단한 예제로 이를 시도해 봅시다. 우리가 NumPy의 랜덤 정규 함수를 사용하여 가중치 배열을 만들었기 때문에 배열은 제로 중심입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 원본 가중치 배열\nweights = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enormal\u003c/span\u003e(size=(\u003cspan class=\"hljs-number\"\u003e20000\u003c/span\u003e)).\u003cspan class=\"hljs-title function_\"\u003eastype\u003c/span\u003e(np.\u003cspan class=\"hljs-property\"\u003efloat32\u003c/span\u003e)\nweights = torch.\u003cspan class=\"hljs-title function_\"\u003efrom_numpy\u003c/span\u003e(weights)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(weights.\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(), weights.\u003cspan class=\"hljs-title function_\"\u003emin\u003c/span\u003e(), weights.\u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e())\n\u003e\u003e\u003e \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.0057\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e3.9224\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e4.4791\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_14.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = \u003cspan class=\"hljs-title function_\"\u003esymmetric_quantize\u003c/span\u003e(weights)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(weights_sym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(), weights_sym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emin\u003c/span\u003e(), weights_sym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e())\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(weights_sym_dequant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(), weights_sym_dequant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emin\u003c/span\u003e(), weights_sym_dequant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e())\n\u003e\u003e\u003e \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.1585\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e111.\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e127.\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e)\n\u003e\u003e\u003e \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.0056\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e3.9148\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e4.4791\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그런 다음 대칭 양자화 함수를 적용하면, 새로 양자화된 배열도 거의 0에 가까운 평균값을 가지며, 최솟값은 -111이고 최댓값은 127입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제, 우리는 데이터를 원래의 부동 소수점 범위로 되돌리는 시도를 할 것입니다. 이것이 바로 양자화 해제(dequantization)라고 불리는 과정입니다. 양자화를 해제한 후에는, 양자화 해제된 배열의 평균, 최소값 및 최대값이 대략 원래 값과 동일해야 합니다.\u003c/p\u003e\n\u003ch2\u003e비균일 양자화\u003c/h2\u003e\n\u003cp\u003e비대칭 양자화의 경우, 양자화 값을 계산할 때 정수가 추가됩니다. 이것을 제로 포인트 (Z)라고 합니다. Z는 float32 영역에서 0의 값과 대응합니다.\u003c/p\u003e\n\u003cp\u003e양자화된 값 계산:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_15.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e스케일(S) 값을 계산하세요:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_16.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e제로포인트(Z) 값을 계산하세요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_17.png\" alt=\"Optimizing Deep Learning Model with Weight Quantization\"\u003e\u003c/p\u003e\n\u003cp\u003e비대칭 양자화는 범위의 다른 부분에 대해 스케일과 제로 포인트를 다르게 조정하여 비대칭 데이터 분포를 효과적으로 처리할 수 있습니다. 그러나 스케일과 제로 포인트 2개의 매개변수가 필요하기 때문에 구현 및 최적화 과정이 복잡해질 수 있고, 양자화 및 역양자화 단계에서 추가적인 계산 능력이 필요할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e비대칭 양자화는 데이터 분포를 조정함으로써 데이터 범위 내에서 스케일과 제로 포인트를 다르게 조정하여 불규칙한 데이터 분포를 훌륭히 처리할 수 있습니다. 데이터가 양자화 다리를 건널 때 더 편안하게 걷도록 데이터의 신발을 맞춤 제작하는 것과 같은 원리입니다!\u003c/p\u003e\n\u003cp\u003e하지만 여기서 중요한 점은 두 가지 매개변수 - 스케일과 제로 포인트 - 를 사용자 정의하기 때문에 설정 및 세밀한 조정이 약간 까다로울 수 있습니다. 게다가, 양자화 및 역양자화 시에 조금 더 많은 계산 노력이 필요할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e비대칭 방식을 사용하여 양자화하기 - 정규 분포 데이터\u003c/h1\u003e\n\u003cp\u003eweights_assym_quant, weights_assym_dequant = assymmetric_quantize(weights)\nprint(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cblockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003etensor(-8.8287, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)\ntensor(0.0056, dtype=torch.float64) tensor(-3.9207, dtype=torch.float64) tensor(4.4808, dtype=torch.float64)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/blockquote\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_18.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e정규 분포 배열 예제를 살펴보았지만, 이것은 간단한 경우입니다. 좀 더 어려운 비정규 분포를 가진 경우를 시도해 보겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# 비정규 분포 데이터 생성\u003c/span\u003e\nskewed_weights = np.random.exponential(scale=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, size=\u003cspan class=\"hljs-number\"\u003e20000\u003c/span\u003e) - \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003e# 데이터를 음수 값과 양수 값을 모두 가지도록 이동\u003c/span\u003e\nskewed_weights = torch.from_numpy(skewed_weights)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(skewed_weights.mean(), skewed_weights.\u003cspan class=\"hljs-built_in\"\u003emin\u003c/span\u003e(), skewed_weights.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e())\n\u003cspan class=\"hljs-meta\"\u003e\u003e\u003e\u003e \u003c/span\u003etensor(-\u003cspan class=\"hljs-number\"\u003e5.0192\u003c/span\u003e, dtype=torch.float64) tensor(-\u003cspan class=\"hljs-number\"\u003e6.9999\u003c/span\u003e, dtype=torch.float64) tensor(\u003cspan class=\"hljs-number\"\u003e16.4827\u003c/span\u003e, dtype=torch.float64)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = \u003cspan class=\"hljs-title function_\"\u003esymmetric_quantize\u003c/span\u003e(skewed_weights)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(weights_sym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(), weights_sym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emin\u003c/span\u003e(), weights_sym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e())\n\u003e\u003e\u003e \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e38.6737\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e54.\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e127.\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 분포는 정규분포가 아니기 때문에 양자화된 가중치의 평균 값은 -38.67, 최솟값은 -54이며 최대값은 127입니다. 문제는 전체 int8의 범위가 완전히 활용되지 않는다는 것입니다. 최솟값이 -64인데 이는 양자화가 사용 가능한 비트를 최대로 활용하지 못한다는 것을 의미합니다. 이로 인해 많은 서로 다른 값을 동일한 양자화된 값으로 반올림하여 고유성과 데이터 내의 세부 정보를 상실할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e가중치를 다시 부동소수점으로 역양자화할 때, 평균값은 대략적으로 원래 가중치의 값에 도달합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 비대칭 방법을 사용하여 양자화 - 정상 분포 데이터\nweights_assym_quant, weights_assym_dequant = \u003cspan class=\"hljs-title function_\"\u003eassymmetric_quantize\u003c/span\u003e(skewed_weights)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(weights_assym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(), weights_assym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emin\u003c/span\u003e(), weights_assym_quant.\u003cspan class=\"hljs-title function_\"\u003edouble\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e())\n\u003e\u003e\u003e \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e106.5096\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e128.\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e) \u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e127.\u003c/span\u003e, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat64\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_20.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e양자화된 값들을 원래의 부동 소수점 범위로 돌리면 어떻게 되는지 알아보겠습니다. 대칭 방법에서의 값들은 원래 데이터와 비교했을 때 그렇게 고른 분포를 보여주지 않는데, 비대칭 방법의 경우와는 다르게 퍼져 있는 것이 흥미롭습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_21.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e📝 코드 구현\u003c/h1\u003e\n\u003cp\u003e파이토치 양자화를 사용하여 양자화 예제를 작업해 보겠습니다.\u003c/p\u003e\n\u003cp\u003e이 예제에서는 MobileNetV2 모델과 MINIST 데이터셋을 사용할 것입니다. 데이터셋에 대한 자세한 내용은 여기에서, 그리고 데이터셋을 로드하는 방법은 여기에서 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eMobileNetV2를 양자화하려면 네트워크에 일부 수정을 구현해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eIn InvertedResidual 블록의 torch.add가 nn.quantized.FloatFunctional()로 대체되었습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e- self.\u003cspan class=\"hljs-property\"\u003eskip_add\u003c/span\u003e = torch.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e()\n+ self.\u003cspan class=\"hljs-property\"\u003eskip_add\u003c/span\u003e = nn.\u003cspan class=\"hljs-property\"\u003equantized\u003c/span\u003e.\u003cspan class=\"hljs-title class_\"\u003eFloatFunctional\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e양자화 전에 Conv+BN 및 Conv+BN+Relu 모듈을 결합하는 fuse_model() 메서드가 추가되어 메모리 액세스를 줄이고 수치 정확도를 향상시켜 모델의 효율성을 높입니다. 이 실천은 양자화된 모델에서 일반적입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMobileNetV2\u003c/span\u003e(nn.\u003cspan class=\"hljs-property\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, num_classes=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, width_mult=\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, inverted_residual_setting=\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, round_nearest=\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e):\n+     self.\u003cspan class=\"hljs-property\"\u003equant\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eQuantStub\u003c/span\u003e()\n+     self.\u003cspan class=\"hljs-property\"\u003edequant\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eDeQuantStub\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003ePyTorch 프레임워크를 사용하여 모델을 양자화하는 일반적인 흐름은 다음과 같습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# 양자화하기 전에 Conv+BN 및 Conv+BN+Relu 모듈을 퓨즈합니다 (이 작업은 숫자를 변경하지 않습니다)\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003efuse_model\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, is_qat=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\u003c/span\u003e):\n    fuse_modules = torch.ao.quantization.fuse_modules_qat \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e is_qat \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e torch.ao.quantization.fuse_modules\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e m \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.modules():\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e(m) == ConvBNReLU:\n            fuse_modules(m, [\u003cspan class=\"hljs-string\"\u003e'0'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'1'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'2'\u003c/span\u003e], inplace=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e(m) == InvertedResidual:\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e idx \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(m.conv)):\n                \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e(m.conv[idx]) == nn.Conv2d:\n                    fuse_modules(m.conv, [\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(idx), \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(idx + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)], inplace=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_22.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQConfig를 사용하여 연산자가 어떻게 관찰되어야 하는지 구성합니다. 이 코드에서는 단순한 최소/최대 관찰자를 사용하여 양자화 매개변수를 결정합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003equantized_model.\u003cspan class=\"hljs-property\"\u003eqconfig\u003c/span\u003e = torch.\u003cspan class=\"hljs-property\"\u003eao\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equantization\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edefault_qconfig\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e준비하기: 지정된 qconfig를 기반으로 Observer/FakeQuantize 모듈을 모델에 삽입합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etorch.\u003cspan class=\"hljs-property\"\u003eao\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equantization\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eprepare\u003c/span\u003e(quantized_model, inplace=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e모델을 캘리브레이션하여 가중치와 활성화에 대한 양자화 매개변수를 결정합니다. 이는 훈련 데이터셋으로 수행됩니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eevaluate(quantized_model, criterion, data_loader, neval_batches=num_calibration_batches)\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eConvert the calibrated model to a quantized model.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003etorch.ao.quantization.convert(quantized_model, inplace=True)\u003c/p\u003e\n\u003cp\u003eWe will load the pretrained model for the MNIST dataset as the original model, quantize this model, and compare the results in both size and performance. Performance is evaluated using Top 1 and Top 5 Accuracy.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e👑결과👑\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_23.png\" alt=\"결과 이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e모델 크기가 8.9MB에서 2.35MB로 줄어든 건 정말 놀라운 일이에요! 거의 4배나 크기가 줄었어요! 🌟\u003c/p\u003e\n\u003cp\u003e성능도 매우 좋아서, 양자화된 모델의 최상위 1위와 최상위 5위 정확도는 원본과 비슷합니다. 최대/최소 옵서버만 사용해서 양자화 매개변수를 선택한 것에도 불구하고요. 그래서 거의 공간을 차지하지 않으면서도 약속받는 결과를 확인할 수 있어요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다른 옵저버의 결과를 확인해보면, 이 옵저버와 비교하여 성능이 어떤지 알 수 있어요. 새 옵저버는 자동으로 양자화 매개변수를 결정할 거에요.\u003c/p\u003e\n\u003cp\u003e디폴트 qconfig를 사용하는 대신에, 구성을 x86 아키텍처로 설정할 거에요. 이 아키텍처는 가중치를 채널 단위로 양자화하고, 활성화도를 수집하고 최적의 양자화 매개변수를 선택하는 히스토그램을 사용해요. 나머지 흐름은 동일하게 유지돼요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eper_channel_quantized_model.\u003cspan class=\"hljs-property\"\u003eqconfig\u003c/span\u003e = torch.\u003cspan class=\"hljs-property\"\u003eao\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equantization\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eget_default_qconfig\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'x86'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_24.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e새 양자화 아키텍처로 얻는 결과는 Top 1 및 Top 5 성능 모두 강력한 출력을 유지한다는 것을 관찰할 수 있습니다. 그리고 가장 좋은 부분은 또 다른 양자화 방법과 모델 크기를 거의 동일하게 유지할 수 있다는 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNotebook\u003c/strong\u003e: [링크]\u003c/p\u003e\n\u003ch1\u003e📕 최종 생각\u003c/h1\u003e\n\u003cp\u003e마무리하며, 사후 훈련 동적 양자화는 머신러닝 모델을 배포하기 위해 최적화하는 편리하고 효율적인 요령입니다. 이 방법은 모든 훈련이 완료된 후 가중치와 활성화를 조정함으로써 원래의 부동 소수점 모델과 가끔 수용 가능한 성능을 보장하며 동등한 성능을 나타낼 수 있습니다. AI 프로젝트를 빠르고 가벼워 만들고 싶다면, 이 방법이 진정한 게임 체인저가 될 수 있을 것입니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\" rel=\"nofollow\" target=\"_blank\"\u003eAchieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with TensorRT\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://pytorch.org/docs/stable/quantization.html\" rel=\"nofollow\" target=\"_blank\"\u003ePyTorch 공식 문서 - 양자화\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMNIST 상업적 이용을 위한 라이선스: GNU General Public License v3.0. 링크: \u003ca href=\"#\"\u003eMNIST 라이선스\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-OptimizingDeepLearningModelswithWeightQuantization"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>