<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>미세 조정에 대한 깊이 있는 탐구 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-ADeepDiveintoFine-Tuning" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="미세 조정에 대한 깊이 있는 탐구 | itposting" data-gatsby-head="true"/><meta property="og:title" content="미세 조정에 대한 깊이 있는 탐구 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-ADeepDiveintoFine-Tuning" data-gatsby-head="true"/><meta name="twitter:title" content="미세 조정에 대한 깊이 있는 탐구 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 03:55" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">미세 조정에 대한 깊이 있는 탐구</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="미세 조정에 대한 깊이 있는 탐구" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">21<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-ADeepDiveintoFine-Tuning&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>"컴포트 존"을 벗어나는 것 - LLM에 대한 도메인 적응 접근 방식에 대한 심층 탐구 3/3</h2>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png" alt="이미지"></p>
<p>특정 도메인이나 사용 사례에 대한 대형 언어 모델 (LLMs)의 도메인 적응을 탐색하고 계십니까? 이 3부작 블로그 시리즈에서는 도메인 적응의 동기에 대해 설명하고 이를 수행하는 다양한 옵션에 대해 깊이 파고들었습니다. 또한 인기있는 트레이드오프를 다루는 도메인 적응 여정을 마스터할 수 있는 상세 안내서가 제공됩니다.</p>
<p>부분 1: 도메인 적응 소개 - 동기, 옵션, 트레이드오프
부분 2: 컨텍스트 학습에 대한 심층 탐구
부분 3: 파인튜닝에 대한 심층 탐구 - 여기 있습니다!</p>
<p>참고: 모든 이미지는 별도로 언급되지 않는 한 저자가 제공했습니다.</p>
<h1>복습</h1>
<p>이 블로그 시리즈의 이전 부분에서는 인컨텍스트 학습의 개념을 탐색하여 대형 언어 모델(Large Language Models, LLMs)의 "쾌적 영역" 제한을 극복하는 강력한 방법으로 다뤘습니다. 이러한 기술이 어떻게 사용될 수 있는지, 작업을 변환하고 모델의 전문 분야로 이동하여 성능 향상 및 Helpfulness, Honesty, Harmlessness와 같은 주요 설계 원칙과의 일치를 도모하는 방법에 대해 논의했습니다. 이번 세 번째 부분에서는 두 번째 도메인 적응 접근 방법인 파인튜닝으로 주목을 기울일 것입니다. 파인튜닝의 세부 사항을 탐구하면서, 모델의 "쾌적 영역"을 확장하고 특정 도메인 및 작업에 적응시켜 성능을 향상시킬 수 있는 방법에 대해 알아볼 것입니다. 프롬프트 엔지니어링과 파인튜닝 사이의 트레이드오프에 대해 논의하고, 데이터 속도, 작업 모호성 및 기타 고려 사항과 같은 요소에 기반하여 언제 어느 접근 방식을 선택해야 하는지에 대한 안내를 제공할 것입니다.</p>
<h1>트랜스포머 101</h1>
<p>최첨단 LLM은 대부분 트랜스포머 아키텍처에 기반을 두고 있어요. 이 아키텍처는 2017년 Vaswani 등이 제안한 이후, 자연어 처리 분야를 혁명시킨 깊은 신경망 아키텍처의 가족입니다. 이 아키텍처 패밀리의 핵심 차별점은 콘텍스트에서 사용된 단어나 자연어의 더 큰 조각의 의미를 포착하는 데 뛰어난 "어텐션" 개념입니다.</p>
<p>트랜스포머 아키텍처는 두 가지 근본적으로 다른 구성 요소로 구성되어 있어요. 한쪽에는 "인코더" 블록이 있어 자연어의 의미를 이른바 문맥화된 임베딩으로 번역하는 데 집중합니다. 이는 벡터 공간에서의 수학적 표현입니다. 인코더 모델은 이러한 벡터 표현을 하위 결정론적 또는 확률적 작업인 분류 문제, NER 또는 의미 검색과 같은 곳에서 활용하는 데 특히 유용합니다. 반면, 디코더 블록은 다음 토큰 예측에 훈련되어 있어, 재귀적으로 사용될 경우 텍스트를 생성할 수 있는 능력을 가지고 있습니다. 이 블록은 텍스트 생성에 의존하는 모든 작업에 사용될 수 있어요. 이러한 구성 요소는 서로 독립적으로 사용할 수 있지만, 조합해서 사용할 수도 있습니다. 오늘날 생성적 AI 분야에서 언급되는 대부분의 모델이 디코더 전용 모델입니다. 그래서 이 블로그 글은 이 유형의 모델에 초점을 맞출 거예요.</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_1.png" alt="ADeepDiveintoFine-Tuning"></p>
<h1>E2E 파인 튜닝 파이프라인</h1>
<p>Fine-tuning은 LLaMA2와 같은 기본 모델에 특정 분야 전문성을 효율적으로 주입하기 위해 전이 학습을 활용합니다. 이 과정은 도메인별 데이터에 대한 모델의 가중치를 업데이트하면서 전체 네트워크 구조를 변경하지 않는 방식으로 이루어집니다. 대규모 데이터셋과 컴퓨팅 파워가 필요한 전체 사전 학습과는 달리, fine-tuning은 매우 샘플 및 컴퓨팅 효율적입니다. 전체적으로, 이 과정은 다음 단계로 분해될 수 있습니다:</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_2.png" alt="Fine-Tuning Phases"></p>
<ul>
<li>데이터 수집과 선택: 모델에 투입될 전용 데이터 집합을 신중하게 선택해야 합니다. 더불어, fine-tuning 목적에 따라 데이터가 아직 사용 가능하지 않을 수 있으며 목적에 맞게 수집되어야 합니다. 데이터의 양적 또는 질적 특성이 다른 경우도 있습니다. 또한 데이터 품질과 함께 데이터 소스, 기밀성 및 지적 재산권, 라이선스, 저작권, 개인 식별 정보 등의 측면을 고려해야 합니다.</li>
</ul>
<p>LLM 사전 학습은 일반적으로 웹 스크랩 및 정돈된 말뭉치를 활용합니다. fine-tuning은 도메인 적응 방식으로 데이터셋이 주로 조직, 지식 또는 작업 특정 도메인의 레이블 또는 미레이블 데이터로 구성되어 있다는 것을 의미합니다.</p>
<p>이 데이터는 다양한 곳에서 수집될 수 있습니다(문서 저장소, 사람이 작성한 콘텐츠 등). 미세 조정을 위해서는 품질에 대해 신중하게 선택하는 것이 중요하지만 위에서 언급한 것처럼 기밀 유지 및 지적 재산권(IP), 라이센스, 저작권, 개인 식별 정보(PII) 등과 같은 주제를 고려해야 합니다.</p>
<p>또 다른 중요한 차원은 훈련 데이터 집합을 라벨이 지정된(labeled) 및 지정되지 않은(unlabeled) 부분(선호도 포함)으로 분류하는 것입니다. 도메인 적응 미세 조정은 라벨이 지정되지 않은 텍스트 데이터가 필요합니다(그림 4를 참조하십시오). 다시 말해, 관련 콘텐츠와 충분한 품질로 간주되는 어떤 자연어의 전문 문서를 사용할 수 있습니다. 실제 사용 사례에 따라 사용자 매뉴얼, 내부 문서 또는 심지어 법적 계약서 등이 될 수 있습니다.</p>
<p>한편, 명시적으로 레이블이 지정된 데이터셋인 지시-맥락-응답 데이터셋과 같은 것들은 지도 미세 조정 방법에 사용될 수 있습니다. 최근에는 모델을 실제 사용자 피드백에 맞추기 위한 강화 학습 방법이 큰 성과를 보여주며, 이는 인간 또는 기계가 생성한 선호 데이터를 활용합니다. 예를 들어, 이진 인간 피드백(좋아요/싫어요)이나 다중 응답 순위와 같은 것들이 있습니다.</p>
<p>레이블이 지정되지 않은 데이터와는 달리, 레이블이 지정된 데이터셋은 특히 규모와 충분한 도메인 전문 지식을 갖추기 위해서 수집하기 어렵고 비용이 많이 듭니다. HuggingFace Datasets와 같은 오픈 소스 데이터 허브는 레이블이 지정된 데이터셋에 대한 좋은 소스일 수 있습니다, 특히 적절한 인간 인구 그룹의 광범위한 부분이 동의한 지역에서(예: 레드팀에 대한 독성 데이터셋)이고, 오픈 소스 데이터셋을 사용하여 모델의 실제 사용자 선호도를 대변할 수 있습니다.</p>
<p>그럼에도 불구하고, 많은 사용 사례는 보다 구체적이며 오픈 소스 프록시 데이터셋만으로 충분하지 않습니다. 실제 인간들이 레이블을 지정한 데이터셋이 필요한 경우도 있습니다. Amazon SageMaker Ground Truth와 같은 도구를 사용하면, 완전히 관리되는 사용자 인터페이스 및 워크플로 또는 전체 직원을 제공함으로써 데이터 수집을 돕을 수 있습니다.</p>
<p>최근에는 합성 데이터 수집이 미세 조정 분야에서 점점 더 중요해지고 있습니다. 강력한 LLM(Large Language Model)을 사용하여 레이블이 지정된 데이터셋을 합성적으로 생성하는 것이 실제로 이루어지고 있습니다. 이는 SFT 또는 선호 정렬을 위한 것일 수 있습니다. 이 방법은 이미 유망한 결과를 보여주었지만, 현재로선 추가적인 연구가 더 필요하며, 실무에서 규모에 맞게 유용함을 입증해야 합니다.</p>
<ul>
<li>데이터 전처리: 선택한 데이터는 하류 학습 알고리즘에게 "잘 소화될 수 있도록" 전처리 되어야 합니다. 인기 있는 전처리 단계는 다음과 같습니다:
<ul>
<li>품질 관련 전처리, 예를 들어 포맷 지정, 중복 제거, PII 필터링</li>
<li>섬세한 조정 접근 방식 관련 전처리: 예를 들어 감독형 섬세 조정을 위한 프롬프트 템플릿으로 랜더링</li>
<li>NLP 관련 전처리, 예를 들어 토큰화, 임베딩, 쪼개기 (문맥 창에 따라)</li>
</ul>
</li>
<li>모델 학습: 선택한 섬세 조정 방식에 따라 딥 뉴럴 네트워크를 학습합니다. 아래에서 자세히 논의할 인기 있는 섬세 조정 방식은 다음과 같습니다:
<ul>
<li>계속된 사전 훈련, 즉 도메인 적응 섬세 조정: 풀 텍스트 데이터에 대한 훈련, 다음 토큰 예측 작업과 연동된 정렬</li>
<li>감독형 섬세 조정: 레이블된 데이터를 활용한 섬세 조정 접근 방식, 타깃 레이블로 정렬</li>
<li>선호도 정렬 접근 방식: 선호 데이터를 활용한 섬세 조정 접근 방식, 모델/시스템의 실제 사용자가 정의한 원하는 동작과 일치시킴</li>
</ul>
</li>
</ul>
<p>이어서 단계별로 자세히 살펴보겠습니다. 학습 접근 방식과 다양한 섬세 조정 접근 방식 소개부터 데이터셋 및 데이터 처리 요구사항으로 이동하기 전에 시작해보겠습니다.</p>
<h1>학습</h1>
<p>이 섹션에서는 디코더 트랜스포머 모델을 학습하는 방법을 탐구합니다. 이는 사전 학습과 섬세 조정 모두에 적용됩니다.
레이블이 지정되지 않은 데이터로 비지도 학습이나 레이블이 지정된 데이터로 지도 학습과 같은 전통적인 ML 학습 방식과 달리, 트랜스포머 모델의 학습은 자가지도 학습이라고 불리는 혼합 접근 방식을 활용합니다. 이는 레이블이 지정되지 않은 텍스트 데이터가 공급되더라도, 알고리즘이 실제로는 특정 입력 토큰을 가림으로써 내재적으로 자기 감독하고 있기 때문입니다. 아래의 입력 토큰 시퀀스 "Berlin is the capital of Germany."을 고려하면, 이는 y가 가려진 토큰이고 X가 나머지인 지도 학습 예제로 이어집니다.</p>
<p>위에서 언급한 자가 지도 학습 접근 방식은 모델 가중치를 언어 모델링(LM) 특정 손실 함수로 최적화합니다. 인코더 모델 훈련은 무작위로 토큰을 마스킹하여 양방향 컨텍스트를 활용하는 마스킹 언어 모델링(MLM)을 사용하며, 디코더 전용 모델은 시퀀스의 가장 오른쪽 토큰을 항상 마스킹하여 단방향 컨텍스트를 가지는 인과 언어 모델링(CLM) 방식으로 매핑됩니다. 간단히 말하면, 이는 이전의 의미적 문맥을 바탕으로 이후 토큰을 예측하는 방식으로 자가 회귀적으로 훈련된다는 것을 의미합니다. 이외에도 순열 언어 모델링(PLM)과 같은 다른 LM 접근 방식이 있으며, 해당 방식은 모델을 사용하여 무작위로 섞인 토큰 시퀀스를 정렬된 순서로 되돌리도록 조건부화하는 것입니다.</p>
<p>CLM 작업을 대리로 사용하여 예측과 실제 값이 생성되며, 이를 사용하여 예측 손실을 계산할 수 있습니다. 따라서 모델 어휘 전체의 토큰에 대한 예측 확률 분포는 그라운드 트루스인 실제 값과 비교되며, 실제 값을 나타내는 토큰에 대한 확률이 1.0인 희소 벡터입니다. 사용한 실제 손실 함수는 특정 모델 아키텍처에 따라 다르지만, 토큰 예측과 같은 범주형 문제 공간에서 잘 작동하는 교차 엔트로피나 헷갈림 손실과 같은 손실 함수가 일반적으로 사용됩니다. 손실 함수는 깊은 신경망 역전파에 의한 경사 하강을 수행하여 매 반복할 때마다 손실을 점진적으로 최소화하고 모델 가중치를 우리의 교육 목표로 최적화하는 데 활용됩니다.</p>
<h1>세밀한 조정 변형 - 시나리오</h1>
<p>이론에는 식상하니, 실전으로 넘어가 봐요. 생물기술 분야의 기관으로 가정해보겠습니다. COVID-19 백신 연구를 중심으로 다양한 NLP 사용 사례를 위한 기반 모델로 LLM을 활용하려고 하는 중입니다. 불행하게도 이 도메인은 표준 제공형 사전 훈련된 LLM의 "편안한 영역"에 포함되지 않아 예상 수준 이하의 성능을 낸다고 하네요. 다음 섹션에서 우리가 상상한 시나리오에서 LLaMA2의 성능을 기대 이상으로 끌어올리는 데 도움을 줄 수 있는 다양한 세밀한 조정 방법에 대해 논의해볼 거예요.</p>
<h1>세밀한 조정 변형 - 계속된 사전 훈련인 도메인 적응 세밀한 조정</h1>
<p>제목이 나타내는 대로, 이 분야가 "계속된 사전 훈련"이라는 용어로 수렴하려고 하는 가운데, 이 섹션에서 논의되는 세밀한 조정 방법에 대한 명확한 용어는 아직 커뮤니티에서 합의되지 않은 상태입니다. 그렇다면 이 세밀한 조정 방법이 정확히 어떤 것을 의미하는 걸까요?</p>
<p>바이오테크 분야의 연구 논문들은 글쓰기 스타일이 꽤 독특하며, 도메인 특화 지식과 산업 또는 기관 별 약어가 가득합니다 (예: Polack et al, 2020; Figure 7 참조).</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_7.png" alt="Figure 7"></p>
<p>반면에, 메타 LLaMA 모델의 사전 훈련 데이터 혼합물의 자세한 조사(Touvron et al., 2023; Figure 8)와 TII Falcon 모델 패밀리(Almazrouei et al., 2023; Figure 9)는 2.5% 및 2%만큼 일반용도의 LLMs가 연구 또는 심지어 바이오테크 도메인의 데이터를 매우 적게 포함하고 있음을 나타냅니다 (LLaMA 3 패밀리의 사전 훈련 데이터 혼합물은 블로그 게시 시점에 공개되지 않았습니다).</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_8.png" alt="Figure 8"></p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_9.png" alt="2024-06-19-ADeepDiveintoFine-Tuning_9.png"></p>
<p>따라서, 이와 같은 차이를 줄이기 위해 파인 튜닝을 활용하여 모델의 "편안 영역"을 확대하여 구체적인 작업에 대한 성능을 향상시켜야 합니다. 지속적인 사전 훈련은 정확히 위에서 언급한 차원에서 뛰어난 성과를 보여줍니다. 이것은 특정 데이터 집합에 대해 미리 훈련된 LLM의 매개변수 지식에 모델의 반응을 조정하기 위해 도메인별 정보 (도메인별 언어, 약어 등) 또는 원시 전문 텍스트에 암시적으로 포함된 정보와 같은 도메인별 정보를 주입하기 위한 기술로, 평문 텍스트 데이터로 구성된 특정 데이터 집합에 대해 미리 훈련된 LLM을 조정하는 과정을 포함합니다. 이 접근 방식에서 사전 훈련된 디코더 모델은 레이블이 없는 텍스트 데이터를 사용하여 다음 토큰 예측을 위해 파인 튜닝됩니다. 이로써 미지의 텍스트 데이터를 사용한 지속적인 사전 훈련은 사전 훈련에 가장 유사한 파인 튜닝 접근 방식이 됩니다.</p>
<p>우리의 예시에서는, 언급된 논문의 내용과 유사한 분야의 관련 문헌을 결합하여 연결된 텍스트 파일로 변환할 수 있습니다. 튜닝 목표 및 기타 요구 사항에 따라 필요없는 콘텐츠(예: 저자, 목차 등)의 제거, 중복 데이터 제거, 또는 PII 감소와 같은 데이터 정리 단계를 적용할 수 있습니다. 마지막으로, 데이터 집합은 모델 훈련에 사용되기 전에 일부 NLP-특정 전 처리(예: 토큰 분리, 컨텍스트 창에 따른 청킹 등 -위 참조-)을 거칩니다. 훈련 자체는 이전 장에서 논의된 것과 같이 전통적인 CLM 기반 훈련입니다. 바이오테크 도메인의 연구 논문 집합에 계속된 사전 훈련을 통해 LLaMA2를 조정한 후, 이제 해당 도메인에서 이를 "BioLLaMA2" 텍스트 완성 모델로 활용할 수 있습니다.</p>
<h1>파인 튜닝 변형 — 감독된 파인 튜닝 (SFT)</h1>
<p>안타깝게도, 우리 사람들은 해결하고 싶은 문제를 순수한 텍스트 완성/토큰 예측 형태로 제시하는 것을 선호하지 않아요. 대화를 선호하며, 특히 일을 처리하고자 할 때에는 수다스럽거나 지시적인 행동을 하는 경향이 있는 사람들이랍니다.</p>
<p>그래서 모델이 간단한 다음 토큰 예측을 넘어서도 미래를 예측할 때에는 약간의 세련된 요소가 필요해요. 여기서 감독된 세밀 조정 방식이 등장해요. 감독된 세밀 조정(SFT)은 사전 훈련된 LLM을 특정 데이터셋과 레이블이 지정된 예제들과 일치시키는 과정을 말해요. 이 기술은 모델의 응답을 특정 도메인이나 작업(예: 상기한 대화적 성격 또는 지시 따름)에 맞게 조정하는 데 중요하답니다. 대상 응용 프로그램을 잘 대표하는 데이터셋으로 학습함으로써, SFT는 LLM이 심층적인 이해를 개발하고 전문적인 요구 사항과 행동에 부합하는 더 정확한 출력물을 생산할 수 있게 해줘요.</p>
<p>상기한 것들 외에도, 좋은 SFT의 예시는 Q&#x26;A 모델의 훈련, 개체 인식과 같은 데이터 추출 작업, 또는 해로운 응답을 방지하는 레드팀 활동 등이 있을 수 있어요.</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_10.png" alt="이미지"></p>
<p>위에서 이해한 대로, SFT에는 레이블이 지정된 데이터셋이 필요합니다. 오픈 소스에는 일반 목적의 레이블이 지정된 데이터셋이 많이 있지만, 모델을 귀하의 특정 사용 사례, 산업 또는 지식 도메인에 가장 적합하게 맞추기 위해 사용자 정의 데이터셋을 수동으로 작성하는 것이 의미가 있을 수 있습니다. 최근에는 Claude 3나 GPT-4와 같은 강력한 LLM을 사용하여 이러한 데이터셋을 만드는 접근 방식이 인간 레이블링 대안으로 발전되어왔습니다.</p>
<p>"dolly-15k" 데이터셋은 Databricks 직원들에 의해 수동으로 만들어진 인기 있는 일반 목적의 오픈 소스 명령어 미세 조정 데이터셋입니다. 이 데이터셋에는 명령문과 문맥이 원하는 응답과 함께 레이블이 지정된 대략 15,000개의 예시가 포함되어 있습니다. 이 데이터셋은 우리의 BioLLaMA2 모델을 지시어를 따르는 작업, 예를 들어 닫힌 Q&#x26;A 작업에 맞게 조정하는 데 사용될 수 있습니다. 지시어를 따르는 SFT를 위해, 데이터셋의 각 항목을 전체 텍스트 프롬프트로 변환하고, 모델을 맞추고자하는 작업을 나타내는 프롬프트 구조에 포함하여 진행할 것입니다. 이는 다음과 같이 보일 수 있습니다:</p>
<pre><code class="hljs language-js">### 명령:
{item.<span class="hljs-property">instruction</span>}
### 문맥:
{item.<span class="hljs-property">context</span>}
### 응답:
{item.<span class="hljs-property">response</span>}
</code></pre>
<p>프롬프트 템플릿은 모델 패밀리에 따라 다양할 수 있으며, 일부 모델은 해시태그보다 HTML 태그나 다른 특수 문자를 선호할 수 있습니다. 이 절차는 데이터셋의 각 항목에 대해 적용되며, 모든 항목이 큰 텍스트 조각으로 연결되기 전에 수행됩니다. 최종적으로 위에서 설명한 NLP 특정 전 처리 후, 이 파일은 다음 토큰 예측을 활용하여 모델로 훈련될 수 있으며, CLM 기반의 훈련 목표를 사용할 것입니다. 이 특정 프롬프트 구조에 지속적으로 노출되므로 모델은 그에 따라 작동하고 해당 방식으로 행동하는 것을 배울 것입니다 — 이 경우에는 지시어를 따릅니다. BioLLaMA2를 dolly-15k 데이터셋에 맞춘 후, BioLLaMA2-instruct 모델은 프롬프트를 통해 제출된 지시를 철저히 따를 것입니다.</p>
<h1>파인튜닝 변형 — 인간 선호도 조정 기술 (RLHF/PPO, DPO, KTO, ORPO)</h1>
<p>BioLLaMA2와 함께, 우리는 생물 기술 연구 영역에 적합한 모델을 가지고 있으며, 사용자가 기대하는 대로 편리하게 우리의 지시에 따릅니다. 하지만 기다려봐 — 모델이 실제 사용자와 일치하는 것일까? 지금까지 논의된 파인튜닝 접근 방식의 핵심 문제가 돋보입니다. 사용한 데이터셋은 우리가 사용자가 좋아하거나 필요로 한다고 생각하는 것의 대리인입니다: 선별된 연구 논문에서의 내용, 언어, 머리글, 그리고 일부 Databricks 직원들이 제작한 dolly-15k의 원하는 지시 행동과 같은 것들입니다. 이는 사용자 중심의 제품 개발 개념과 대비됩니다. 이는 기민한 제품 개발의 핵심적이고 잘 수립된 원리 중 하나입니다. 실제 대상 사용자로부터 반복적으로 피드백을 받는 것이 훌륭한 제품을 개발할 때 매우 성공적이라는 것이 입증되었습니다. 사실, 사용자를 위한 훌륭한 경험을 구축하려는 경우 우리가 해야 할 일이지 않을까요!</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_11.png" alt="이미지"></p>
<p>이를 염두에 두고, 연구자들은 인간 피드백을 성능 향상에 통합하는 방법을 찾는 데 상당한 노력을 기울였습니다. 이에따라, 그들은 (심층적인) 강화 학습 (RL) 과 상당한 중첩이 있다는 것을 깨달았는데, 이것은 환경 내에서 행동을 수행하는 자율 에이전트들에 대한 것으로, 다음 상태를 생성하며 항상 보상과 결합됩니다. 에이전트들은 훈련 단계에서 보상을 극대화하기 위해 점진적으로 최적화된 정책이나 가치지도에 따라 행동합니다.</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_12.png" alt="image"></p>
<p>이 개념은 LLM의 세계로 프로젝션될 때 LLM 자체가 에이전트로 작용하는 것으로 이어집니다. 추론 과정에서 자동 회귀 토큰 예측의 각 단계마다 모델의 어휘가 작업 공간이고 환경은 가능한 토큰 조합이 되는 행동을 수행합니다. 새로운 추론 주기마다 새로운 상태가 형성되며, 이는 이상적으로 어떤 인간 피드백과 관련된 보상과 함께 함께합니다.</p>
<p>이 아이디어에 기반하여 여러 인간의 선호 정렬 접근 방식이 제안되고 시험되었습니다. 다음에서는 가장 중요한 몇 가지 접근 방법에 대해 살펴보겠습니다:</p>
<h2>인간 피드백에 대한 강화학습 (RLHF) 및 근접 정책 최적화 (PPO)</h2>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_13.png" alt="여기에 이미지가 있습니다"></p>
<p>인간 피드백을 통한 강화 학습은 초기 창조적 AI 혹평 중요한 기술적 기반 중 하나였으며, Anthropi Claude나 GPT-3.5와 같은 대형 디코더 모델들의 위험의 동반자가 된 획기적인 성과를 거두고 사용자 맞춤 방향으로 추가적인 동기 부여를 제공했습니다. RLHF는 두 단계로 작동하며 Figures 13과 14에서 설명되어 있습니다.</p>
<p>1단계 (Figure 13): 먼저, 실제 RL을 통한 교육 접근에서 나중에 사용할 보상 모델을 훈련해야 합니다. 따라서, 목적과 일치하는 프롬프트 데이터 집합(우리의 BioLLaMA2-instruct 모델의 경우, 지시와 문맥의 쌍일 것입니다)이 모델에 공급되어 세부 조정되며, 단 하나가 아닌 두 개 이상의 추론 결과를 요청합니다. 이러한 결과는 최적화 목표에 따라 인간 레이블러에게 제공되고, 1등, 2등, 3등 등과 같이 점수 매기기를 기반으로 합니다. "Anthropic/hh-rlhf"와 같은 몇 가지 오픈 소스 기호 순위 데이터 집합도 있으며, 이것은 red-teaming 및 정직성 및 무해성 목표에 맞게 설계되어 있습니다. 정규화 단계와 보상 값으로의 변환 후, 보상 모델이 단일 샘플-보상 쌍을 기반으로 훈련되고, 여기서 샘플은 단일 모델 응답입니다. 보상 모델 아키텍처는 일반적으로 세부 값으로 잠재 공간을 투사하는 정보 대신 토큰에 대한 확률 분포가 됩니다. 그러나 이 모델의 매개 변수의 이상적인 크기는 여전히 연구 대상이며, 지난 시간에 모델 제공 업체에 의해 다양한 접근 방식이 선택되어 왔습니다.</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_14.png" alt="여기에 이미지가 있습니다"></p>
<p>단계 2 (그림 14): 새 보상 모델을 사용하여 실제 모델을 학습하는 단계로 넘어갑니다. 따라서 해당 모델을 조정하기 위해 다른 일련의 프롬프트가 모델을 통해 공급됩니다 (도면의 회색 상자), 결과적으로 각각 하나의 응답이 생성됩니다. 이후 이러한 응답은 각각 개별 보상을 검색하기 위해 보상 모델에 공급됩니다. 그런 다음, 보상이 할당된 모델의 답변을 최대화하기 위해 Proximal Policy Optimization (PPO)이라는 정책 기반 강화 학습 알고리즘이 사용되어 모델의 가중치를 점진적으로 조정합니다. CLM과는 달리, 그라디언트 하강 대신에 이 접근 방식은 이제 목표 (보상)를 최대화하려 하기 때문에 그라디언트 상승(또는 그라디언트 하강 오버 1 − 보상)을 활용합니다. 학습 중에 모델 동작의 지나친 드리프트를 방지하기 위한 알고리즘적 안정성을 높이기 위해서 PPO와 같은 강화 학습 기반 알고리즘에 의해 야기될 수 있는 너무 큰 드리프트를 예방하기 위해 예측 변화 벌점이 보상 항에 추가되어, 초기 언어 모델의 입력 프롬프트에 대한 예측 확률 분포에서 너무 많이 벗어나는 답변을 벌한다.</p>
<p>PPO와 함께 RLHF 이상의 다른 접근 방법이 개발되어 왔으며, 현재는 선호 정렬을 위해 가장 널리 사용되고 검증된 접근 방법입니다. 다음 몇 섹션에서는 조금 더 고급 수준으로 이러한 접근법 중 일부에 대해 자세히 살펴볼 것입니다. 이는 고급 독자를 대상으로 한 내용이므로 딥 러닝 및 강화 학습에 대한 경험 수준에 따라 직접 다음 섹션인 "의사결정 흐름도 - 선택할 모델, 선택할 파인튜닝 경로"로 건너뛰고자 할 수도 있습니다.</p>
<h2>직접 정책 최적화 (DPO)</h2>
<p>직접 정책 최적화(Direct Policy Optimization, DPO)은 RLHF에서 유래한 선호 정렬 접근 방식으로, RLHF의 두 가지 주요 약점을 해결하는 것이 목표입니다.</p>
<ul>
<li>먼저 보상 모델을 훈련하는 것은 추가적인 자원 투자를 필요로 하며, 보상 모델의 크기에 따라 상당히 중요할 수 있습니다.</li>
<li>PPO를 사용한 RLHF의 훈련 단계는 대규모 연산 클러스터가 필요합니다. 초기 LM, 조정된 LM, 보상 모델의 3개 복제본이 저지연 설정에서 동시에 호스팅 및 조정되어야 합니다.</li>
<li>RLHF는 불안정한 절차가 될 수 있습니다 (→ 예측 변동 페널티가 이를 완화하려고 시도합니다)</li>
</ul>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_15.png" alt="image"></p>
<p>DPO는 대안적인 선호 정렬 접근법으로 Rafailov 등(2023)에 의해 제안되었습니다. DPO의 주요 아이디어는 보상 모델 훈련을 건너뛰고 최종 선호 정렬 LLM을 선호 데이터에 직접 조정하는 것입니다. 이는 보상 모델의 매개변수화(보상 항목)를 손실 함수(figure 16)로 변환하는 몇 가지 수학적 수정을 적용하고 실제 보상 값들을 선호 데이터에 대한 확률 값으로 대체함으로써 달성됩니다.</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_16.png" alt="image"></p>
<p>이는 선호도에 맞는 모델로 나아가면서 계산 및 알고리즘 복잡성을 줄입니다. 논문은 RLHF와 비교하여 성능 향상도 보여주고 있지만, 이 방법은 최근에 제안된 것이기 때문에 결과는 실질적인 증명을 필요로 합니다.</p>
<h2>칸먼-트벽시 최적화 (KTO)</h2>
<p>RLHF와 DPO와 같은 인간 피드백과 언어 모델을 조정하는 기존 방법들은 입력에 대해 하나가 다른 것을 선호하는 출력 쌍처럼 선호 데이터가 필요합니다. 그러나 규모별로 고품질의 선호 데이터를 수집하는 것은 현실 세계에서 어렵고 비싸며 많은 노이즈, 불일치 및 비교 불가능성으로 인해, 서로 다른 인간 평가자들이 어떤 출력이 더 나은지에 대해 상이한 견해를 가질 수 있습니다. KTO는 Ethayarajh 등에 의해 (2024) 제안된 대안적 방법으로, 간단하고 풍부한 신호와 함께 작동할 수 있는 접근법으로 소개되었습니다 — 주어진 출력이 특정 입력에 대해 바람직한지 또는 바람직하지 않은지만 알고 있으면 되며, 출력 간의 상대적인 선호를 알 필요가 없습니다.</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_17.png" alt="이미지"></p>
<p>KTO(카네만-트버스키 최적화)는 세대의 상대적 "좋음"을 포착하는 보상 함수를 정의한 후, 모델을 최적화하여 카네만-트버스키 가치 함수하에서 이 보상의 기대값을 최대화하는 방식으로 작동합니다. 카네만과 트버스키의 전망 이론은 인간이 편향되었지만 명확히 정의된 방식으로 불확실한 결과에 대한 결정을 내리는 방법을 설명합니다. 이 이론은 이득에서 오목하고 손실에서 오목한 가치 함수에 의존하며, 수익과 손실을 분리하는 기준점이 존재합니다(17번 그림 참조). KTO는 선호도의 가능성을 극대화하는 것이 아니라 사람의 유틸리티 개념을 직접 최적화합니다.</p>
<p>KTO의 주요 혁신점은 선호 가능 여부에 대한 이진 신호만 필요하다는 점입니다. 이는 전체 선호도 쌍 대신 더 데이터 효율적으로 사용할 수 있도록 합니다. 바이너리 피드백 신호는 훨씬 풍부하고 수집 비용이 저렴하기 때문에 선호도 기반 방법보다 데이터 효율적입니다(18번 그림 참조).</p>
<p>KTO는 선호도 데이터가 부족하거나 비용이 많이 드는 상황에서 특히 유용하지만, 모델 출력의 품질에 대한 바이너리 피드백의 대량을 사용할 수 있는 경우입니다. 논문에 따르면, 모델 규모가 클 때 선호도 기반 방법인 DPO와 같은 성능을 극대화하거나 심지어 능가할 수 있습니다. 그러나 실제 환경에서 규모별로 확인되어야 합니다. KTO는 선호도 기능성을 극대화하는 것이 목표일 때 선호될 수 있습니다. 그러나 선호도 데이터가 노이즈가 적거나 추이가 없는 고 품질일 때라면 선호도 기반 방법이 여전히 더 나은 선택일 수 있습니다. KTO는 일부 경우에는 극도의 데이터 불균형을 처리하고 감독된 세밀 조정을 회피하는 측면에서 이론적 장점을 가지고 있습니다.</p>
<h2>Odds Ration Preference Optimization (ORPO)</h2>
<p>ORPO의 주된 동기는 기존의 선호 정렬 방법인 RLHF 및 DPO와 같은 제한 사항을 해결하는 데에 있습니다. 이러한 방법들은 종종 별도의 지도적 세세조정(SFT) 단계, 참조 모델 또는 보상 모델이 필요합니다. 홍 등(2024)의 논문은 SFT만 사용하면 원하지 않는 스타일로 토큰을 생성할 가능성이 높아진다고 주장합니다. 교차 엔트로피 손실이 비선호하는 응답에 대해 직접적인 처벌을 제공하지 않기 때문입니다. 동시에 그들은 SFT가 강력한 선호 정렬 모델에 수렴하는 데 중요하다고 주장합니다. 이로 인해 자원을 많이 소모하는 두 단계 정렬 프로세스가 발생합니다. 이러한 단계를 결합하여 ORPO는 SFT의 도메인 적응 이점을 유지하면서 동시에 선호 정렬 접근 방식에 의해 목표로 하는 원치 않는 생성 스타일을 파악하고 완화하려고 합니다. (그림 19 참조)</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_19.png" alt="image"></p>
<p>ORPO는 기존의 인과 언어 모델링에 연결된 손실(예: 교차 엔트로피 손실)에 오즈 비율이 기반된 처벌을 통합하는 혁신적인 선호 정렬 알고리즘을 소개합니다. ORPO의 목적 함수는 두 구성 요소로 이루어집니다: SFT 손실과 상대 비율 손실 (LOR). LOR 항목은 선호하는 응답과 비선호하는 응답을 생성하는 우도 사이의 오즈 비율을 최대화하여 모델에게 거부 응답에 높은 확률을 할당하는 것에 대한 처벌을 효과적으로 적용합니다.</p>
<p>ORPO는 미리 학습된 언어 모델을 세부 도메인이나 작업에 적응시키면서 모델의 출력이 인간의 선호와 일치하도록 보장하고 싶을 때 특히 유용합니다. 이는 UltraFeedback 또는 HH-RLHF 데이터셋과 같이 pairwise preference 데이터셋에 액세스할 수 있는 시나리오에서 적용할 수 있습니다. 이를 염두에 둔 ORPO는 별도의 참조 모델, 보상 모델 또는 두 단계의 미세 조정 접근 방식이 필요하지 않아 RLHF 및 DPO에 대한 더 효율적이고 효과적인 대안으로 설계되었습니다.</p>
<h1>의사결정 플로우 차트 - 어떤 모델을 선택하고 어떤 미세 조정 경로를 선택할 것인가</h1>
<p>여러 미세 조정 접근 방식을 심층적으로 살펴본 후에 특정 요구사항에 따라 시작할 모델과 선택할 접근 방법에 대한 명확한 질문이 제기됩니다. 미세 조정 목적에 적합한 올바른 모델을 선택하기 위한 접근 방법은 두 단계 접근 방식입니다. 첫 번째 단계는 어떤 미세 조정 의도도 없는 기본 모델을 선택하는 과정과 유사하며, 다음 차원을 고려하여 보다 적합한 모델을 선택합니다(완전하지 않음):</p>
<ul>
<li>사용할 플랫폼: 각 플랫폼은 해당 플랫폼을 통해 액세스할 수 있는 일련의 모델을 제공합니다. 이를 고려해야 합니다. 지역별 모델 가용성에 대한 차이가 있을 수 있음을 유의하십시오. 이에 대한 자세한 정보는 해당 플랫폼의 문서를 확인하시기 바랍니다.</li>
<li>성능: 조직은 특정 작업에 최소한의 모델을 사용해야 합니다. 이에 대해 일반적인 지침은 제공되지 않지만 세밀한 조정은 모델의 성능을 크게 향상시킬 수 있습니다 (더 작은 세밀하게 조정된 모델이 더 큰 일반적인 모델보다 성능을 능가할 수 있음). 기본 모델의 평가 결과를 활용하는 것은 중요한 지표가 될 수 있습니다.</li>
<li>예산 (TCO): 일반적으로 더 큰 모델은 더 많은 컴퓨팅 및 잠재적으로 멀티 GPU 인스턴스를 필요로 하며, 복수의 가속기에 걸쳐 교육 및 제공을 위해 이에 직접적인 영향을 미칩니다. 이는 교육 및 추론 비용, 교육 및 추론의 복잡성, 필요한 자원 및 기술 등, 모델의 전체 수명 주기 중 TCO의 일부로 고려되어야 합니다. 이는 단기 및 장기 예산 할당과 일치해야합니다.</li>
<li>라이선스 모델: 모델마다 도메인 및 상업적 이용에 따라 라이선스 제약사항이 있습니다. 이를 고려해야 합니다.</li>
<li>지배 체제, 윤리, 책임 있는 인공 지능: 모든 조직은 이러한 차원과 함께 규정 가이드라인을 가지고 있습니다. 모델 선택 시 반드시 고려되어야 합니다.</li>
</ul>
<p>예시: 조직은 기본 모델의 평가 결과를 바탕으로 LLaMA 2 모델을 고려하고, 기본 모델을 기반으로 한 Anthropic Claude 또는 AI21Labs Jurassic과 같은 프로프라이어터리 모델의 사용을 배제하기로 결정할 수 있습니다. 더 나아가, 그들은 이 모델의 7B-파라미터 버전만 사용하기로 결정하여 단일 GPU 인스턴스에서 교육 및 제공할 수 있도록 합니다.</p>
<p>두 번째 단계는 실험 단계를 위해 고려해야 할 1-몇 개 모델로 초기 모델 선택을 좁히는데 관련되어 있습니다. 구체적인 접근 방식을 선택할 최종 결정은 아래 그림에서 언어 모델의 미세 조정 수명주기에 대한 원하는 진입점에 따라 달라집니다.</p>
<p>따라서, 다음 차원을 고려해야 합니다:</p>
<ul>
<li>수행할 작업: 각각의 사용 사례는 특정 모델 동작을 요구합니다. 어떤 사용 사례에는 간단한 텍스트 완성 모델 (다음 토큰 예측)이 충분할 수 있지만, 대부분의 사용 사례에서는 수다성, 지시사항 준수 또는 다른 작업 특정 동작이 필요합니다. 이 요구 사항을 충족하기 위해 우리는 수행할 작업을 역발상하는 접근 방식을 취할 수 있습니다. 즉, 원하는 작업을 수행할 모델에 맞게 정의해야 합니다. 이는 일러스트를 기준으로 모델이 원하는 모델 동작과 일치하여 파란색, 주황색 또는 녹색 원 안에서 끝나야 함을 의미합니다. 동시에 세부 튜닝 여정을 흘러가는 흐름도의 가능한 경로와 함께 정의해야 합니다.</li>
<li>올바른 시작점 선택 (합리적인 범위 내에서): 세부 튜닝 여정이 끝나야 할 위치에 대해 명확해져야 하지만, 해당 흐름도에서 어디서든 출발할 수 있습니다. 그러나 이는 합리적이어야 합니다 — 수백만 개의 게시된 모델이 있는 모델 허브 시대에는 다른 사람이 이미 세부 튜닝 단계를 수행했고 결과 모델을 공유한 경우 체크하는 것이 의미있을 수 있습니다.</li>
<li>세부 튜닝은 반복적이고 재귀적인 프로세스입니다: 원하는 모델까지 여러 번의 연속적인 세부 튜닝 작업을 수행할 수 있습니다. 그러나 모델의 가중치에 무한한 양의 정보를 인코드할 수 없으므로 추월하는 것을 염두에 둬야 합니다. 이를 해소하기 위해 이 논문과 블로그에서 보여주는 LoRA와 같은 매개변수 효율 세부 튜닝 방법을 활용할 수 있습니다.</li>
<li>특정 작업 성능 향상에 초점을 맞춤: 세부 튜닝은 특정 작업에서 모델의 성능을 향상시키기 위해 수행됩니다. 언어 패턴(도메인별 언어, 약어 등)이나 귀하의 훈련 데이터에 암시적으로 포함된 정보에서 성능 향상을 원한다면 계속해서 사전 훈련을 진행해야 합니다. 특정 작업으로의 성능 향상을 원한다면 지도 세부 튜닝을 선택해야 합니다. 모델 동작을 실제 사용자와 일치시키고자 하는 경우 인간의 선호에 맞게 조정하는 것이 올바릅니다.</li>
<li>데이터 가용성: 훈련 데이터도 우리가 선택하는 경로에 영향을 미칠 것입니다. 일반적으로 조직은 레이블이 지정되지 않은 텍스트 데이터를 보유하고 있는 경우가 많습니다. 레이블이 지정된 데이터를 획득하는 것은 비용이 많이 들 수 있습니다. 흐름도를 통해 탐색할 때 이 차원을 고려해야 합니다.</li>
</ul>
<p>이와 같은 역발상 방식을 흘러가는 접근과 상기된 흐름도를 통해 시작할 모델과 세부 튜닝 흐름도를 따라갈 경로를 식별할 수 있습니다.</p>
<p>좀 더 명확히 이해하실 수 있도록 두 가지 예제를 제공해 드리겠습니다:</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_22.png" alt="image"></p>
<p>예시 1: 위의 튜닝 섹션에 설명된 예시를 따라가면, 특정 사용 사례에 맞는 가르침 모델을 형성할 수 있습니다. 그러면 실제 사용자의 선호도에 부합되며, 바이오테크 분야의 성능을 향상시키고 싶습니다. 연구 논문 형태의 레이블되지 않은 데이터가 제공됩니다. 우리는 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 LLaMA-2-7b 가르침 모델을 발표하지 않았으므로, 텍스트 완성 모델인 LLaMA-2-7b-base에서 출발합니다. 그런 다음, 연구 논문 코퍼스에서 계속된 사전 훈련을 수행한 후, dolly-15k 데이터셋과 같은 오픈 소스 가르침 데이터셋에서 지도 학습 미세 조정을 수행합니다. 이로써 LLaMA-2-7B-base의 가르침으로 조정된 바이오테크 버전인 BioLLaMA-2-7b-instruct가 생성됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 선호도 데이터셋을 수집하고 보상 모델을 훈련한 후, PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.</p>
<p><img src="/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_23.png" alt="image"></p>
<p>예시 2: 이 예시에서는 채팅 모델을 사용하려고 합니다. 그러나 실제 사용자의 선호도에 부합되도록 조정하고 싶습니다. 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 제공하는 사용자 지정 채팅 모델인 LLaMA-2-7b-chat을 시작점으로 사용할 수 있음을 알게 됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 사용자로부터 선호도 데이터셋을 수집하고, 보상 모델을 훈련한 후 PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.</p>
<h1>결론</h1>
<p>생성 모델 인공지능은 기업과 조직에 많은 흥미로운 사용 사례를 제공합니다. 그러나 이러한 응용 프로그램은 대개 개별 소비자용으로 레시피나 연설을 생성하는 것과 같은 것보다 훨씬 복잡합니다. 기업에서는 인공지능이 조직의 특정 도메인 지식, 프로세스 및 데이터를 이해해야 합니다. 기존 기업 시스템 및 애플리케이션과 통합되어야 합니다. 또한 다양한 직원과 역할에 맞는 맞춤형 경험을 제공하면서 안전하게 작동해야 합니다. 기업 환경에서 생성 모델 인공지능을 성공적으로 구현하려면 기술이 조직의 고유한 요구 사항에 맞게 신중하게 설계되고 맞춤화되어야 합니다. 일반적으로 공개적으로 학습된 모델을 사용하는 것만으로 충분하지 않을 것입니다.</p>
<p>이 블로그 글에서는 도메인 적응이 모델이 "편안한 영역"을 벗어난 작업에 직면했을 때 이러한 갭을 메우는 데 도움이 되는 방법으로 어떻게 도메인 적응이 도움이 될 수 있는지에 대해 설명했습니다. 문맥 내 학습과 세밀한 조정을 통해 도메인 적응을 위한 두 가지 강력한 접근법에 대해 깊이 파고들었습니다. 마지막으로, 이러한 접근법 사이에서 선택할 때 고려해야 할 절충안에 대해 논의했습니다.</p>
<p>강력한 인공지능 능력과 실제 비즈니스 요구 사항 사이의 이 갭을 성공적으로 메우는 것은 기업을 위해 생성 모델 인공지능의 궁극적인 잠재력을 발휘하는 데 중요합니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"미세 조정에 대한 깊이 있는 탐구","description":"","date":"2024-06-19 03:55","slug":"2024-06-19-ADeepDiveintoFine-Tuning","content":"\n\n## \"컴포트 존\"을 벗어나는 것 - LLM에 대한 도메인 적응 접근 방식에 대한 심층 탐구 3/3\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png)\n\n특정 도메인이나 사용 사례에 대한 대형 언어 모델 (LLMs)의 도메인 적응을 탐색하고 계십니까? 이 3부작 블로그 시리즈에서는 도메인 적응의 동기에 대해 설명하고 이를 수행하는 다양한 옵션에 대해 깊이 파고들었습니다. 또한 인기있는 트레이드오프를 다루는 도메인 적응 여정을 마스터할 수 있는 상세 안내서가 제공됩니다.\n\n부분 1: 도메인 적응 소개 - 동기, 옵션, 트레이드오프\n부분 2: 컨텍스트 학습에 대한 심층 탐구\n부분 3: 파인튜닝에 대한 심층 탐구 - 여기 있습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참고: 모든 이미지는 별도로 언급되지 않는 한 저자가 제공했습니다.\n\n# 복습\n\n이 블로그 시리즈의 이전 부분에서는 인컨텍스트 학습의 개념을 탐색하여 대형 언어 모델(Large Language Models, LLMs)의 \"쾌적 영역\" 제한을 극복하는 강력한 방법으로 다뤘습니다. 이러한 기술이 어떻게 사용될 수 있는지, 작업을 변환하고 모델의 전문 분야로 이동하여 성능 향상 및 Helpfulness, Honesty, Harmlessness와 같은 주요 설계 원칙과의 일치를 도모하는 방법에 대해 논의했습니다. 이번 세 번째 부분에서는 두 번째 도메인 적응 접근 방법인 파인튜닝으로 주목을 기울일 것입니다. 파인튜닝의 세부 사항을 탐구하면서, 모델의 \"쾌적 영역\"을 확장하고 특정 도메인 및 작업에 적응시켜 성능을 향상시킬 수 있는 방법에 대해 알아볼 것입니다. 프롬프트 엔지니어링과 파인튜닝 사이의 트레이드오프에 대해 논의하고, 데이터 속도, 작업 모호성 및 기타 고려 사항과 같은 요소에 기반하여 언제 어느 접근 방식을 선택해야 하는지에 대한 안내를 제공할 것입니다.\n\n# 트랜스포머 101\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최첨단 LLM은 대부분 트랜스포머 아키텍처에 기반을 두고 있어요. 이 아키텍처는 2017년 Vaswani 등이 제안한 이후, 자연어 처리 분야를 혁명시킨 깊은 신경망 아키텍처의 가족입니다. 이 아키텍처 패밀리의 핵심 차별점은 콘텍스트에서 사용된 단어나 자연어의 더 큰 조각의 의미를 포착하는 데 뛰어난 \"어텐션\" 개념입니다.\n\n트랜스포머 아키텍처는 두 가지 근본적으로 다른 구성 요소로 구성되어 있어요. 한쪽에는 \"인코더\" 블록이 있어 자연어의 의미를 이른바 문맥화된 임베딩으로 번역하는 데 집중합니다. 이는 벡터 공간에서의 수학적 표현입니다. 인코더 모델은 이러한 벡터 표현을 하위 결정론적 또는 확률적 작업인 분류 문제, NER 또는 의미 검색과 같은 곳에서 활용하는 데 특히 유용합니다. 반면, 디코더 블록은 다음 토큰 예측에 훈련되어 있어, 재귀적으로 사용될 경우 텍스트를 생성할 수 있는 능력을 가지고 있습니다. 이 블록은 텍스트 생성에 의존하는 모든 작업에 사용될 수 있어요. 이러한 구성 요소는 서로 독립적으로 사용할 수 있지만, 조합해서 사용할 수도 있습니다. 오늘날 생성적 AI 분야에서 언급되는 대부분의 모델이 디코더 전용 모델입니다. 그래서 이 블로그 글은 이 유형의 모델에 초점을 맞출 거예요.\n\n\n![ADeepDiveintoFine-Tuning](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_1.png)\n\n\n# E2E 파인 튜닝 파이프라인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFine-tuning은 LLaMA2와 같은 기본 모델에 특정 분야 전문성을 효율적으로 주입하기 위해 전이 학습을 활용합니다. 이 과정은 도메인별 데이터에 대한 모델의 가중치를 업데이트하면서 전체 네트워크 구조를 변경하지 않는 방식으로 이루어집니다. 대규모 데이터셋과 컴퓨팅 파워가 필요한 전체 사전 학습과는 달리, fine-tuning은 매우 샘플 및 컴퓨팅 효율적입니다. 전체적으로, 이 과정은 다음 단계로 분해될 수 있습니다:\n\n![Fine-Tuning Phases](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_2.png)\n\n- 데이터 수집과 선택: 모델에 투입될 전용 데이터 집합을 신중하게 선택해야 합니다. 더불어, fine-tuning 목적에 따라 데이터가 아직 사용 가능하지 않을 수 있으며 목적에 맞게 수집되어야 합니다. 데이터의 양적 또는 질적 특성이 다른 경우도 있습니다. 또한 데이터 품질과 함께 데이터 소스, 기밀성 및 지적 재산권, 라이선스, 저작권, 개인 식별 정보 등의 측면을 고려해야 합니다.\n\nLLM 사전 학습은 일반적으로 웹 스크랩 및 정돈된 말뭉치를 활용합니다. fine-tuning은 도메인 적응 방식으로 데이터셋이 주로 조직, 지식 또는 작업 특정 도메인의 레이블 또는 미레이블 데이터로 구성되어 있다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_3.png\" /\u003e\n\n이 데이터는 다양한 곳에서 수집될 수 있습니다(문서 저장소, 사람이 작성한 콘텐츠 등). 미세 조정을 위해서는 품질에 대해 신중하게 선택하는 것이 중요하지만 위에서 언급한 것처럼 기밀 유지 및 지적 재산권(IP), 라이센스, 저작권, 개인 식별 정보(PII) 등과 같은 주제를 고려해야 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_4.png\" /\u003e\n\n또 다른 중요한 차원은 훈련 데이터 집합을 라벨이 지정된(labeled) 및 지정되지 않은(unlabeled) 부분(선호도 포함)으로 분류하는 것입니다. 도메인 적응 미세 조정은 라벨이 지정되지 않은 텍스트 데이터가 필요합니다(그림 4를 참조하십시오). 다시 말해, 관련 콘텐츠와 충분한 품질로 간주되는 어떤 자연어의 전문 문서를 사용할 수 있습니다. 실제 사용 사례에 따라 사용자 매뉴얼, 내부 문서 또는 심지어 법적 계약서 등이 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한편, 명시적으로 레이블이 지정된 데이터셋인 지시-맥락-응답 데이터셋과 같은 것들은 지도 미세 조정 방법에 사용될 수 있습니다. 최근에는 모델을 실제 사용자 피드백에 맞추기 위한 강화 학습 방법이 큰 성과를 보여주며, 이는 인간 또는 기계가 생성한 선호 데이터를 활용합니다. 예를 들어, 이진 인간 피드백(좋아요/싫어요)이나 다중 응답 순위와 같은 것들이 있습니다.\n\n레이블이 지정되지 않은 데이터와는 달리, 레이블이 지정된 데이터셋은 특히 규모와 충분한 도메인 전문 지식을 갖추기 위해서 수집하기 어렵고 비용이 많이 듭니다. HuggingFace Datasets와 같은 오픈 소스 데이터 허브는 레이블이 지정된 데이터셋에 대한 좋은 소스일 수 있습니다, 특히 적절한 인간 인구 그룹의 광범위한 부분이 동의한 지역에서(예: 레드팀에 대한 독성 데이터셋)이고, 오픈 소스 데이터셋을 사용하여 모델의 실제 사용자 선호도를 대변할 수 있습니다.\n\n그럼에도 불구하고, 많은 사용 사례는 보다 구체적이며 오픈 소스 프록시 데이터셋만으로 충분하지 않습니다. 실제 인간들이 레이블을 지정한 데이터셋이 필요한 경우도 있습니다. Amazon SageMaker Ground Truth와 같은 도구를 사용하면, 완전히 관리되는 사용자 인터페이스 및 워크플로 또는 전체 직원을 제공함으로써 데이터 수집을 돕을 수 있습니다.\n\n최근에는 합성 데이터 수집이 미세 조정 분야에서 점점 더 중요해지고 있습니다. 강력한 LLM(Large Language Model)을 사용하여 레이블이 지정된 데이터셋을 합성적으로 생성하는 것이 실제로 이루어지고 있습니다. 이는 SFT 또는 선호 정렬을 위한 것일 수 있습니다. 이 방법은 이미 유망한 결과를 보여주었지만, 현재로선 추가적인 연구가 더 필요하며, 실무에서 규모에 맞게 유용함을 입증해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 전처리: 선택한 데이터는 하류 학습 알고리즘에게 \"잘 소화될 수 있도록\" 전처리 되어야 합니다. 인기 있는 전처리 단계는 다음과 같습니다:\n  - 품질 관련 전처리, 예를 들어 포맷 지정, 중복 제거, PII 필터링\n  - 섬세한 조정 접근 방식 관련 전처리: 예를 들어 감독형 섬세 조정을 위한 프롬프트 템플릿으로 랜더링\n  - NLP 관련 전처리, 예를 들어 토큰화, 임베딩, 쪼개기 (문맥 창에 따라)\n- 모델 학습: 선택한 섬세 조정 방식에 따라 딥 뉴럴 네트워크를 학습합니다. 아래에서 자세히 논의할 인기 있는 섬세 조정 방식은 다음과 같습니다:\n  - 계속된 사전 훈련, 즉 도메인 적응 섬세 조정: 풀 텍스트 데이터에 대한 훈련, 다음 토큰 예측 작업과 연동된 정렬\n  - 감독형 섬세 조정: 레이블된 데이터를 활용한 섬세 조정 접근 방식, 타깃 레이블로 정렬\n  - 선호도 정렬 접근 방식: 선호 데이터를 활용한 섬세 조정 접근 방식, 모델/시스템의 실제 사용자가 정의한 원하는 동작과 일치시킴\n\n이어서 단계별로 자세히 살펴보겠습니다. 학습 접근 방식과 다양한 섬세 조정 접근 방식 소개부터 데이터셋 및 데이터 처리 요구사항으로 이동하기 전에 시작해보겠습니다.\n\n# 학습\n\n이 섹션에서는 디코더 트랜스포머 모델을 학습하는 방법을 탐구합니다. 이는 사전 학습과 섬세 조정 모두에 적용됩니다.\n레이블이 지정되지 않은 데이터로 비지도 학습이나 레이블이 지정된 데이터로 지도 학습과 같은 전통적인 ML 학습 방식과 달리, 트랜스포머 모델의 학습은 자가지도 학습이라고 불리는 혼합 접근 방식을 활용합니다. 이는 레이블이 지정되지 않은 텍스트 데이터가 공급되더라도, 알고리즘이 실제로는 특정 입력 토큰을 가림으로써 내재적으로 자기 감독하고 있기 때문입니다. 아래의 입력 토큰 시퀀스 \"Berlin is the capital of Germany.\"을 고려하면, 이는 y가 가려진 토큰이고 X가 나머지인 지도 학습 예제로 이어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_5.png\" /\u003e\n\n위에서 언급한 자가 지도 학습 접근 방식은 모델 가중치를 언어 모델링(LM) 특정 손실 함수로 최적화합니다. 인코더 모델 훈련은 무작위로 토큰을 마스킹하여 양방향 컨텍스트를 활용하는 마스킹 언어 모델링(MLM)을 사용하며, 디코더 전용 모델은 시퀀스의 가장 오른쪽 토큰을 항상 마스킹하여 단방향 컨텍스트를 가지는 인과 언어 모델링(CLM) 방식으로 매핑됩니다. 간단히 말하면, 이는 이전의 의미적 문맥을 바탕으로 이후 토큰을 예측하는 방식으로 자가 회귀적으로 훈련된다는 것을 의미합니다. 이외에도 순열 언어 모델링(PLM)과 같은 다른 LM 접근 방식이 있으며, 해당 방식은 모델을 사용하여 무작위로 섞인 토큰 시퀀스를 정렬된 순서로 되돌리도록 조건부화하는 것입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_6.png\" /\u003e\n\nCLM 작업을 대리로 사용하여 예측과 실제 값이 생성되며, 이를 사용하여 예측 손실을 계산할 수 있습니다. 따라서 모델 어휘 전체의 토큰에 대한 예측 확률 분포는 그라운드 트루스인 실제 값과 비교되며, 실제 값을 나타내는 토큰에 대한 확률이 1.0인 희소 벡터입니다. 사용한 실제 손실 함수는 특정 모델 아키텍처에 따라 다르지만, 토큰 예측과 같은 범주형 문제 공간에서 잘 작동하는 교차 엔트로피나 헷갈림 손실과 같은 손실 함수가 일반적으로 사용됩니다. 손실 함수는 깊은 신경망 역전파에 의한 경사 하강을 수행하여 매 반복할 때마다 손실을 점진적으로 최소화하고 모델 가중치를 우리의 교육 목표로 최적화하는 데 활용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 세밀한 조정 변형 - 시나리오\n\n이론에는 식상하니, 실전으로 넘어가 봐요. 생물기술 분야의 기관으로 가정해보겠습니다. COVID-19 백신 연구를 중심으로 다양한 NLP 사용 사례를 위한 기반 모델로 LLM을 활용하려고 하는 중입니다. 불행하게도 이 도메인은 표준 제공형 사전 훈련된 LLM의 \"편안한 영역\"에 포함되지 않아 예상 수준 이하의 성능을 낸다고 하네요. 다음 섹션에서 우리가 상상한 시나리오에서 LLaMA2의 성능을 기대 이상으로 끌어올리는 데 도움을 줄 수 있는 다양한 세밀한 조정 방법에 대해 논의해볼 거예요.\n\n# 세밀한 조정 변형 - 계속된 사전 훈련인 도메인 적응 세밀한 조정\n\n제목이 나타내는 대로, 이 분야가 \"계속된 사전 훈련\"이라는 용어로 수렴하려고 하는 가운데, 이 섹션에서 논의되는 세밀한 조정 방법에 대한 명확한 용어는 아직 커뮤니티에서 합의되지 않은 상태입니다. 그렇다면 이 세밀한 조정 방법이 정확히 어떤 것을 의미하는 걸까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n바이오테크 분야의 연구 논문들은 글쓰기 스타일이 꽤 독특하며, 도메인 특화 지식과 산업 또는 기관 별 약어가 가득합니다 (예: Polack et al, 2020; Figure 7 참조).\n\n![Figure 7](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_7.png)\n\n반면에, 메타 LLaMA 모델의 사전 훈련 데이터 혼합물의 자세한 조사(Touvron et al., 2023; Figure 8)와 TII Falcon 모델 패밀리(Almazrouei et al., 2023; Figure 9)는 2.5% 및 2%만큼 일반용도의 LLMs가 연구 또는 심지어 바이오테크 도메인의 데이터를 매우 적게 포함하고 있음을 나타냅니다 (LLaMA 3 패밀리의 사전 훈련 데이터 혼합물은 블로그 게시 시점에 공개되지 않았습니다).\n\n![Figure 8](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-19-ADeepDiveintoFine-Tuning_9.png](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_9.png)\n\n따라서, 이와 같은 차이를 줄이기 위해 파인 튜닝을 활용하여 모델의 \"편안 영역\"을 확대하여 구체적인 작업에 대한 성능을 향상시켜야 합니다. 지속적인 사전 훈련은 정확히 위에서 언급한 차원에서 뛰어난 성과를 보여줍니다. 이것은 특정 데이터 집합에 대해 미리 훈련된 LLM의 매개변수 지식에 모델의 반응을 조정하기 위해 도메인별 정보 (도메인별 언어, 약어 등) 또는 원시 전문 텍스트에 암시적으로 포함된 정보와 같은 도메인별 정보를 주입하기 위한 기술로, 평문 텍스트 데이터로 구성된 특정 데이터 집합에 대해 미리 훈련된 LLM을 조정하는 과정을 포함합니다. 이 접근 방식에서 사전 훈련된 디코더 모델은 레이블이 없는 텍스트 데이터를 사용하여 다음 토큰 예측을 위해 파인 튜닝됩니다. 이로써 미지의 텍스트 데이터를 사용한 지속적인 사전 훈련은 사전 훈련에 가장 유사한 파인 튜닝 접근 방식이 됩니다.\n\n우리의 예시에서는, 언급된 논문의 내용과 유사한 분야의 관련 문헌을 결합하여 연결된 텍스트 파일로 변환할 수 있습니다. 튜닝 목표 및 기타 요구 사항에 따라 필요없는 콘텐츠(예: 저자, 목차 등)의 제거, 중복 데이터 제거, 또는 PII 감소와 같은 데이터 정리 단계를 적용할 수 있습니다. 마지막으로, 데이터 집합은 모델 훈련에 사용되기 전에 일부 NLP-특정 전 처리(예: 토큰 분리, 컨텍스트 창에 따른 청킹 등 -위 참조-)을 거칩니다. 훈련 자체는 이전 장에서 논의된 것과 같이 전통적인 CLM 기반 훈련입니다. 바이오테크 도메인의 연구 논문 집합에 계속된 사전 훈련을 통해 LLaMA2를 조정한 후, 이제 해당 도메인에서 이를 \"BioLLaMA2\" 텍스트 완성 모델로 활용할 수 있습니다.\n\n# 파인 튜닝 변형 — 감독된 파인 튜닝 (SFT)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안타깝게도, 우리 사람들은 해결하고 싶은 문제를 순수한 텍스트 완성/토큰 예측 형태로 제시하는 것을 선호하지 않아요. 대화를 선호하며, 특히 일을 처리하고자 할 때에는 수다스럽거나 지시적인 행동을 하는 경향이 있는 사람들이랍니다.\n\n그래서 모델이 간단한 다음 토큰 예측을 넘어서도 미래를 예측할 때에는 약간의 세련된 요소가 필요해요. 여기서 감독된 세밀 조정 방식이 등장해요. 감독된 세밀 조정(SFT)은 사전 훈련된 LLM을 특정 데이터셋과 레이블이 지정된 예제들과 일치시키는 과정을 말해요. 이 기술은 모델의 응답을 특정 도메인이나 작업(예: 상기한 대화적 성격 또는 지시 따름)에 맞게 조정하는 데 중요하답니다. 대상 응용 프로그램을 잘 대표하는 데이터셋으로 학습함으로써, SFT는 LLM이 심층적인 이해를 개발하고 전문적인 요구 사항과 행동에 부합하는 더 정확한 출력물을 생산할 수 있게 해줘요.\n\n상기한 것들 외에도, 좋은 SFT의 예시는 Q\u0026A 모델의 훈련, 개체 인식과 같은 데이터 추출 작업, 또는 해로운 응답을 방지하는 레드팀 활동 등이 있을 수 있어요.\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 이해한 대로, SFT에는 레이블이 지정된 데이터셋이 필요합니다. 오픈 소스에는 일반 목적의 레이블이 지정된 데이터셋이 많이 있지만, 모델을 귀하의 특정 사용 사례, 산업 또는 지식 도메인에 가장 적합하게 맞추기 위해 사용자 정의 데이터셋을 수동으로 작성하는 것이 의미가 있을 수 있습니다. 최근에는 Claude 3나 GPT-4와 같은 강력한 LLM을 사용하여 이러한 데이터셋을 만드는 접근 방식이 인간 레이블링 대안으로 발전되어왔습니다.\n\n\"dolly-15k\" 데이터셋은 Databricks 직원들에 의해 수동으로 만들어진 인기 있는 일반 목적의 오픈 소스 명령어 미세 조정 데이터셋입니다. 이 데이터셋에는 명령문과 문맥이 원하는 응답과 함께 레이블이 지정된 대략 15,000개의 예시가 포함되어 있습니다. 이 데이터셋은 우리의 BioLLaMA2 모델을 지시어를 따르는 작업, 예를 들어 닫힌 Q\u0026A 작업에 맞게 조정하는 데 사용될 수 있습니다. 지시어를 따르는 SFT를 위해, 데이터셋의 각 항목을 전체 텍스트 프롬프트로 변환하고, 모델을 맞추고자하는 작업을 나타내는 프롬프트 구조에 포함하여 진행할 것입니다. 이는 다음과 같이 보일 수 있습니다:\n\n```js\n### 명령:\n{item.instruction}\n### 문맥:\n{item.context}\n### 응답:\n{item.response}\n```\n\n프롬프트 템플릿은 모델 패밀리에 따라 다양할 수 있으며, 일부 모델은 해시태그보다 HTML 태그나 다른 특수 문자를 선호할 수 있습니다. 이 절차는 데이터셋의 각 항목에 대해 적용되며, 모든 항목이 큰 텍스트 조각으로 연결되기 전에 수행됩니다. 최종적으로 위에서 설명한 NLP 특정 전 처리 후, 이 파일은 다음 토큰 예측을 활용하여 모델로 훈련될 수 있으며, CLM 기반의 훈련 목표를 사용할 것입니다. 이 특정 프롬프트 구조에 지속적으로 노출되므로 모델은 그에 따라 작동하고 해당 방식으로 행동하는 것을 배울 것입니다 — 이 경우에는 지시어를 따릅니다. BioLLaMA2를 dolly-15k 데이터셋에 맞춘 후, BioLLaMA2-instruct 모델은 프롬프트를 통해 제출된 지시를 철저히 따를 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 파인튜닝 변형 — 인간 선호도 조정 기술 (RLHF/PPO, DPO, KTO, ORPO)\n\nBioLLaMA2와 함께, 우리는 생물 기술 연구 영역에 적합한 모델을 가지고 있으며, 사용자가 기대하는 대로 편리하게 우리의 지시에 따릅니다. 하지만 기다려봐 — 모델이 실제 사용자와 일치하는 것일까? 지금까지 논의된 파인튜닝 접근 방식의 핵심 문제가 돋보입니다. 사용한 데이터셋은 우리가 사용자가 좋아하거나 필요로 한다고 생각하는 것의 대리인입니다: 선별된 연구 논문에서의 내용, 언어, 머리글, 그리고 일부 Databricks 직원들이 제작한 dolly-15k의 원하는 지시 행동과 같은 것들입니다. 이는 사용자 중심의 제품 개발 개념과 대비됩니다. 이는 기민한 제품 개발의 핵심적이고 잘 수립된 원리 중 하나입니다. 실제 대상 사용자로부터 반복적으로 피드백을 받는 것이 훌륭한 제품을 개발할 때 매우 성공적이라는 것이 입증되었습니다. 사실, 사용자를 위한 훌륭한 경험을 구축하려는 경우 우리가 해야 할 일이지 않을까요!\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_11.png)\n\n이를 염두에 두고, 연구자들은 인간 피드백을 성능 향상에 통합하는 방법을 찾는 데 상당한 노력을 기울였습니다. 이에따라, 그들은 (심층적인) 강화 학습 (RL) 과 상당한 중첩이 있다는 것을 깨달았는데, 이것은 환경 내에서 행동을 수행하는 자율 에이전트들에 대한 것으로, 다음 상태를 생성하며 항상 보상과 결합됩니다. 에이전트들은 훈련 단계에서 보상을 극대화하기 위해 점진적으로 최적화된 정책이나 가치지도에 따라 행동합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_12.png)\n\n이 개념은 LLM의 세계로 프로젝션될 때 LLM 자체가 에이전트로 작용하는 것으로 이어집니다. 추론 과정에서 자동 회귀 토큰 예측의 각 단계마다 모델의 어휘가 작업 공간이고 환경은 가능한 토큰 조합이 되는 행동을 수행합니다. 새로운 추론 주기마다 새로운 상태가 형성되며, 이는 이상적으로 어떤 인간 피드백과 관련된 보상과 함께 함께합니다.\n\n이 아이디어에 기반하여 여러 인간의 선호 정렬 접근 방식이 제안되고 시험되었습니다. 다음에서는 가장 중요한 몇 가지 접근 방법에 대해 살펴보겠습니다:\n\n## 인간 피드백에 대한 강화학습 (RLHF) 및 근접 정책 최적화 (PPO)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![여기에 이미지가 있습니다](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_13.png)\n\n인간 피드백을 통한 강화 학습은 초기 창조적 AI 혹평 중요한 기술적 기반 중 하나였으며, Anthropi Claude나 GPT-3.5와 같은 대형 디코더 모델들의 위험의 동반자가 된 획기적인 성과를 거두고 사용자 맞춤 방향으로 추가적인 동기 부여를 제공했습니다. RLHF는 두 단계로 작동하며 Figures 13과 14에서 설명되어 있습니다.\n\n1단계 (Figure 13): 먼저, 실제 RL을 통한 교육 접근에서 나중에 사용할 보상 모델을 훈련해야 합니다. 따라서, 목적과 일치하는 프롬프트 데이터 집합(우리의 BioLLaMA2-instruct 모델의 경우, 지시와 문맥의 쌍일 것입니다)이 모델에 공급되어 세부 조정되며, 단 하나가 아닌 두 개 이상의 추론 결과를 요청합니다. 이러한 결과는 최적화 목표에 따라 인간 레이블러에게 제공되고, 1등, 2등, 3등 등과 같이 점수 매기기를 기반으로 합니다. \"Anthropic/hh-rlhf\"와 같은 몇 가지 오픈 소스 기호 순위 데이터 집합도 있으며, 이것은 red-teaming 및 정직성 및 무해성 목표에 맞게 설계되어 있습니다. 정규화 단계와 보상 값으로의 변환 후, 보상 모델이 단일 샘플-보상 쌍을 기반으로 훈련되고, 여기서 샘플은 단일 모델 응답입니다. 보상 모델 아키텍처는 일반적으로 세부 값으로 잠재 공간을 투사하는 정보 대신 토큰에 대한 확률 분포가 됩니다. 그러나 이 모델의 매개 변수의 이상적인 크기는 여전히 연구 대상이며, 지난 시간에 모델 제공 업체에 의해 다양한 접근 방식이 선택되어 왔습니다.\n\n![여기에 이미지가 있습니다](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_14.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 2 (그림 14): 새 보상 모델을 사용하여 실제 모델을 학습하는 단계로 넘어갑니다. 따라서 해당 모델을 조정하기 위해 다른 일련의 프롬프트가 모델을 통해 공급됩니다 (도면의 회색 상자), 결과적으로 각각 하나의 응답이 생성됩니다. 이후 이러한 응답은 각각 개별 보상을 검색하기 위해 보상 모델에 공급됩니다. 그런 다음, 보상이 할당된 모델의 답변을 최대화하기 위해 Proximal Policy Optimization (PPO)이라는 정책 기반 강화 학습 알고리즘이 사용되어 모델의 가중치를 점진적으로 조정합니다. CLM과는 달리, 그라디언트 하강 대신에 이 접근 방식은 이제 목표 (보상)를 최대화하려 하기 때문에 그라디언트 상승(또는 그라디언트 하강 오버 1 − 보상)을 활용합니다. 학습 중에 모델 동작의 지나친 드리프트를 방지하기 위한 알고리즘적 안정성을 높이기 위해서 PPO와 같은 강화 학습 기반 알고리즘에 의해 야기될 수 있는 너무 큰 드리프트를 예방하기 위해 예측 변화 벌점이 보상 항에 추가되어, 초기 언어 모델의 입력 프롬프트에 대한 예측 확률 분포에서 너무 많이 벗어나는 답변을 벌한다.\n\nPPO와 함께 RLHF 이상의 다른 접근 방법이 개발되어 왔으며, 현재는 선호 정렬을 위해 가장 널리 사용되고 검증된 접근 방법입니다. 다음 몇 섹션에서는 조금 더 고급 수준으로 이러한 접근법 중 일부에 대해 자세히 살펴볼 것입니다. 이는 고급 독자를 대상으로 한 내용이므로 딥 러닝 및 강화 학습에 대한 경험 수준에 따라 직접 다음 섹션인 \"의사결정 흐름도 - 선택할 모델, 선택할 파인튜닝 경로\"로 건너뛰고자 할 수도 있습니다.\n\n## 직접 정책 최적화 (DPO)\n\n직접 정책 최적화(Direct Policy Optimization, DPO)은 RLHF에서 유래한 선호 정렬 접근 방식으로, RLHF의 두 가지 주요 약점을 해결하는 것이 목표입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 먼저 보상 모델을 훈련하는 것은 추가적인 자원 투자를 필요로 하며, 보상 모델의 크기에 따라 상당히 중요할 수 있습니다.\n- PPO를 사용한 RLHF의 훈련 단계는 대규모 연산 클러스터가 필요합니다. 초기 LM, 조정된 LM, 보상 모델의 3개 복제본이 저지연 설정에서 동시에 호스팅 및 조정되어야 합니다.\n- RLHF는 불안정한 절차가 될 수 있습니다 (→ 예측 변동 페널티가 이를 완화하려고 시도합니다)\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_15.png)\n\nDPO는 대안적인 선호 정렬 접근법으로 Rafailov 등(2023)에 의해 제안되었습니다. DPO의 주요 아이디어는 보상 모델 훈련을 건너뛰고 최종 선호 정렬 LLM을 선호 데이터에 직접 조정하는 것입니다. 이는 보상 모델의 매개변수화(보상 항목)를 손실 함수(figure 16)로 변환하는 몇 가지 수학적 수정을 적용하고 실제 보상 값들을 선호 데이터에 대한 확률 값으로 대체함으로써 달성됩니다.\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이는 선호도에 맞는 모델로 나아가면서 계산 및 알고리즘 복잡성을 줄입니다. 논문은 RLHF와 비교하여 성능 향상도 보여주고 있지만, 이 방법은 최근에 제안된 것이기 때문에 결과는 실질적인 증명을 필요로 합니다.\n\n## 칸먼-트벽시 최적화 (KTO)\n\nRLHF와 DPO와 같은 인간 피드백과 언어 모델을 조정하는 기존 방법들은 입력에 대해 하나가 다른 것을 선호하는 출력 쌍처럼 선호 데이터가 필요합니다. 그러나 규모별로 고품질의 선호 데이터를 수집하는 것은 현실 세계에서 어렵고 비싸며 많은 노이즈, 불일치 및 비교 불가능성으로 인해, 서로 다른 인간 평가자들이 어떤 출력이 더 나은지에 대해 상이한 견해를 가질 수 있습니다. KTO는 Ethayarajh 등에 의해 (2024) 제안된 대안적 방법으로, 간단하고 풍부한 신호와 함께 작동할 수 있는 접근법으로 소개되었습니다 — 주어진 출력이 특정 입력에 대해 바람직한지 또는 바람직하지 않은지만 알고 있으면 되며, 출력 간의 상대적인 선호를 알 필요가 없습니다.\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_17.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKTO(카네만-트버스키 최적화)는 세대의 상대적 \"좋음\"을 포착하는 보상 함수를 정의한 후, 모델을 최적화하여 카네만-트버스키 가치 함수하에서 이 보상의 기대값을 최대화하는 방식으로 작동합니다. 카네만과 트버스키의 전망 이론은 인간이 편향되었지만 명확히 정의된 방식으로 불확실한 결과에 대한 결정을 내리는 방법을 설명합니다. 이 이론은 이득에서 오목하고 손실에서 오목한 가치 함수에 의존하며, 수익과 손실을 분리하는 기준점이 존재합니다(17번 그림 참조). KTO는 선호도의 가능성을 극대화하는 것이 아니라 사람의 유틸리티 개념을 직접 최적화합니다.\n\nKTO의 주요 혁신점은 선호 가능 여부에 대한 이진 신호만 필요하다는 점입니다. 이는 전체 선호도 쌍 대신 더 데이터 효율적으로 사용할 수 있도록 합니다. 바이너리 피드백 신호는 훨씬 풍부하고 수집 비용이 저렴하기 때문에 선호도 기반 방법보다 데이터 효율적입니다(18번 그림 참조).\n\nKTO는 선호도 데이터가 부족하거나 비용이 많이 드는 상황에서 특히 유용하지만, 모델 출력의 품질에 대한 바이너리 피드백의 대량을 사용할 수 있는 경우입니다. 논문에 따르면, 모델 규모가 클 때 선호도 기반 방법인 DPO와 같은 성능을 극대화하거나 심지어 능가할 수 있습니다. 그러나 실제 환경에서 규모별로 확인되어야 합니다. KTO는 선호도 기능성을 극대화하는 것이 목표일 때 선호될 수 있습니다. 그러나 선호도 데이터가 노이즈가 적거나 추이가 없는 고 품질일 때라면 선호도 기반 방법이 여전히 더 나은 선택일 수 있습니다. KTO는 일부 경우에는 극도의 데이터 불균형을 처리하고 감독된 세밀 조정을 회피하는 측면에서 이론적 장점을 가지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Odds Ration Preference Optimization (ORPO)\n\nORPO의 주된 동기는 기존의 선호 정렬 방법인 RLHF 및 DPO와 같은 제한 사항을 해결하는 데에 있습니다. 이러한 방법들은 종종 별도의 지도적 세세조정(SFT) 단계, 참조 모델 또는 보상 모델이 필요합니다. 홍 등(2024)의 논문은 SFT만 사용하면 원하지 않는 스타일로 토큰을 생성할 가능성이 높아진다고 주장합니다. 교차 엔트로피 손실이 비선호하는 응답에 대해 직접적인 처벌을 제공하지 않기 때문입니다. 동시에 그들은 SFT가 강력한 선호 정렬 모델에 수렴하는 데 중요하다고 주장합니다. 이로 인해 자원을 많이 소모하는 두 단계 정렬 프로세스가 발생합니다. 이러한 단계를 결합하여 ORPO는 SFT의 도메인 적응 이점을 유지하면서 동시에 선호 정렬 접근 방식에 의해 목표로 하는 원치 않는 생성 스타일을 파악하고 완화하려고 합니다. (그림 19 참조)\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_19.png)\n\nORPO는 기존의 인과 언어 모델링에 연결된 손실(예: 교차 엔트로피 손실)에 오즈 비율이 기반된 처벌을 통합하는 혁신적인 선호 정렬 알고리즘을 소개합니다. ORPO의 목적 함수는 두 구성 요소로 이루어집니다: SFT 손실과 상대 비율 손실 (LOR). LOR 항목은 선호하는 응답과 비선호하는 응답을 생성하는 우도 사이의 오즈 비율을 최대화하여 모델에게 거부 응답에 높은 확률을 할당하는 것에 대한 처벌을 효과적으로 적용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_20.png\" /\u003e\n\nORPO는 미리 학습된 언어 모델을 세부 도메인이나 작업에 적응시키면서 모델의 출력이 인간의 선호와 일치하도록 보장하고 싶을 때 특히 유용합니다. 이는 UltraFeedback 또는 HH-RLHF 데이터셋과 같이 pairwise preference 데이터셋에 액세스할 수 있는 시나리오에서 적용할 수 있습니다. 이를 염두에 둔 ORPO는 별도의 참조 모델, 보상 모델 또는 두 단계의 미세 조정 접근 방식이 필요하지 않아 RLHF 및 DPO에 대한 더 효율적이고 효과적인 대안으로 설계되었습니다.\n\n# 의사결정 플로우 차트 - 어떤 모델을 선택하고 어떤 미세 조정 경로를 선택할 것인가\n\n여러 미세 조정 접근 방식을 심층적으로 살펴본 후에 특정 요구사항에 따라 시작할 모델과 선택할 접근 방법에 대한 명확한 질문이 제기됩니다. 미세 조정 목적에 적합한 올바른 모델을 선택하기 위한 접근 방법은 두 단계 접근 방식입니다. 첫 번째 단계는 어떤 미세 조정 의도도 없는 기본 모델을 선택하는 과정과 유사하며, 다음 차원을 고려하여 보다 적합한 모델을 선택합니다(완전하지 않음):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 사용할 플랫폼: 각 플랫폼은 해당 플랫폼을 통해 액세스할 수 있는 일련의 모델을 제공합니다. 이를 고려해야 합니다. 지역별 모델 가용성에 대한 차이가 있을 수 있음을 유의하십시오. 이에 대한 자세한 정보는 해당 플랫폼의 문서를 확인하시기 바랍니다.\n- 성능: 조직은 특정 작업에 최소한의 모델을 사용해야 합니다. 이에 대해 일반적인 지침은 제공되지 않지만 세밀한 조정은 모델의 성능을 크게 향상시킬 수 있습니다 (더 작은 세밀하게 조정된 모델이 더 큰 일반적인 모델보다 성능을 능가할 수 있음). 기본 모델의 평가 결과를 활용하는 것은 중요한 지표가 될 수 있습니다.\n- 예산 (TCO): 일반적으로 더 큰 모델은 더 많은 컴퓨팅 및 잠재적으로 멀티 GPU 인스턴스를 필요로 하며, 복수의 가속기에 걸쳐 교육 및 제공을 위해 이에 직접적인 영향을 미칩니다. 이는 교육 및 추론 비용, 교육 및 추론의 복잡성, 필요한 자원 및 기술 등, 모델의 전체 수명 주기 중 TCO의 일부로 고려되어야 합니다. 이는 단기 및 장기 예산 할당과 일치해야합니다.\n- 라이선스 모델: 모델마다 도메인 및 상업적 이용에 따라 라이선스 제약사항이 있습니다. 이를 고려해야 합니다.\n- 지배 체제, 윤리, 책임 있는 인공 지능: 모든 조직은 이러한 차원과 함께 규정 가이드라인을 가지고 있습니다. 모델 선택 시 반드시 고려되어야 합니다.\n\n예시: 조직은 기본 모델의 평가 결과를 바탕으로 LLaMA 2 모델을 고려하고, 기본 모델을 기반으로 한 Anthropic Claude 또는 AI21Labs Jurassic과 같은 프로프라이어터리 모델의 사용을 배제하기로 결정할 수 있습니다. 더 나아가, 그들은 이 모델의 7B-파라미터 버전만 사용하기로 결정하여 단일 GPU 인스턴스에서 교육 및 제공할 수 있도록 합니다.\n\n두 번째 단계는 실험 단계를 위해 고려해야 할 1-몇 개 모델로 초기 모델 선택을 좁히는데 관련되어 있습니다. 구체적인 접근 방식을 선택할 최종 결정은 아래 그림에서 언어 모델의 미세 조정 수명주기에 대한 원하는 진입점에 따라 달라집니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_21.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 다음 차원을 고려해야 합니다:\n\n- 수행할 작업: 각각의 사용 사례는 특정 모델 동작을 요구합니다. 어떤 사용 사례에는 간단한 텍스트 완성 모델 (다음 토큰 예측)이 충분할 수 있지만, 대부분의 사용 사례에서는 수다성, 지시사항 준수 또는 다른 작업 특정 동작이 필요합니다. 이 요구 사항을 충족하기 위해 우리는 수행할 작업을 역발상하는 접근 방식을 취할 수 있습니다. 즉, 원하는 작업을 수행할 모델에 맞게 정의해야 합니다. 이는 일러스트를 기준으로 모델이 원하는 모델 동작과 일치하여 파란색, 주황색 또는 녹색 원 안에서 끝나야 함을 의미합니다. 동시에 세부 튜닝 여정을 흘러가는 흐름도의 가능한 경로와 함께 정의해야 합니다.\n- 올바른 시작점 선택 (합리적인 범위 내에서): 세부 튜닝 여정이 끝나야 할 위치에 대해 명확해져야 하지만, 해당 흐름도에서 어디서든 출발할 수 있습니다. 그러나 이는 합리적이어야 합니다 — 수백만 개의 게시된 모델이 있는 모델 허브 시대에는 다른 사람이 이미 세부 튜닝 단계를 수행했고 결과 모델을 공유한 경우 체크하는 것이 의미있을 수 있습니다.\n- 세부 튜닝은 반복적이고 재귀적인 프로세스입니다: 원하는 모델까지 여러 번의 연속적인 세부 튜닝 작업을 수행할 수 있습니다. 그러나 모델의 가중치에 무한한 양의 정보를 인코드할 수 없으므로 추월하는 것을 염두에 둬야 합니다. 이를 해소하기 위해 이 논문과 블로그에서 보여주는 LoRA와 같은 매개변수 효율 세부 튜닝 방법을 활용할 수 있습니다.\n- 특정 작업 성능 향상에 초점을 맞춤: 세부 튜닝은 특정 작업에서 모델의 성능을 향상시키기 위해 수행됩니다. 언어 패턴(도메인별 언어, 약어 등)이나 귀하의 훈련 데이터에 암시적으로 포함된 정보에서 성능 향상을 원한다면 계속해서 사전 훈련을 진행해야 합니다. 특정 작업으로의 성능 향상을 원한다면 지도 세부 튜닝을 선택해야 합니다. 모델 동작을 실제 사용자와 일치시키고자 하는 경우 인간의 선호에 맞게 조정하는 것이 올바릅니다.\n- 데이터 가용성: 훈련 데이터도 우리가 선택하는 경로에 영향을 미칠 것입니다. 일반적으로 조직은 레이블이 지정되지 않은 텍스트 데이터를 보유하고 있는 경우가 많습니다. 레이블이 지정된 데이터를 획득하는 것은 비용이 많이 들 수 있습니다. 흐름도를 통해 탐색할 때 이 차원을 고려해야 합니다.\n\n이와 같은 역발상 방식을 흘러가는 접근과 상기된 흐름도를 통해 시작할 모델과 세부 튜닝 흐름도를 따라갈 경로를 식별할 수 있습니다.\n\n좀 더 명확히 이해하실 수 있도록 두 가지 예제를 제공해 드리겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_22.png)\n\n예시 1: 위의 튜닝 섹션에 설명된 예시를 따라가면, 특정 사용 사례에 맞는 가르침 모델을 형성할 수 있습니다. 그러면 실제 사용자의 선호도에 부합되며, 바이오테크 분야의 성능을 향상시키고 싶습니다. 연구 논문 형태의 레이블되지 않은 데이터가 제공됩니다. 우리는 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 LLaMA-2-7b 가르침 모델을 발표하지 않았으므로, 텍스트 완성 모델인 LLaMA-2-7b-base에서 출발합니다. 그런 다음, 연구 논문 코퍼스에서 계속된 사전 훈련을 수행한 후, dolly-15k 데이터셋과 같은 오픈 소스 가르침 데이터셋에서 지도 학습 미세 조정을 수행합니다. 이로써 LLaMA-2-7B-base의 가르침으로 조정된 바이오테크 버전인 BioLLaMA-2-7b-instruct가 생성됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 선호도 데이터셋을 수집하고 보상 모델을 훈련한 후, PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_23.png)\n\n예시 2: 이 예시에서는 채팅 모델을 사용하려고 합니다. 그러나 실제 사용자의 선호도에 부합되도록 조정하고 싶습니다. 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 제공하는 사용자 지정 채팅 모델인 LLaMA-2-7b-chat을 시작점으로 사용할 수 있음을 알게 됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 사용자로부터 선호도 데이터셋을 수집하고, 보상 모델을 훈련한 후 PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n생성 모델 인공지능은 기업과 조직에 많은 흥미로운 사용 사례를 제공합니다. 그러나 이러한 응용 프로그램은 대개 개별 소비자용으로 레시피나 연설을 생성하는 것과 같은 것보다 훨씬 복잡합니다. 기업에서는 인공지능이 조직의 특정 도메인 지식, 프로세스 및 데이터를 이해해야 합니다. 기존 기업 시스템 및 애플리케이션과 통합되어야 합니다. 또한 다양한 직원과 역할에 맞는 맞춤형 경험을 제공하면서 안전하게 작동해야 합니다. 기업 환경에서 생성 모델 인공지능을 성공적으로 구현하려면 기술이 조직의 고유한 요구 사항에 맞게 신중하게 설계되고 맞춤화되어야 합니다. 일반적으로 공개적으로 학습된 모델을 사용하는 것만으로 충분하지 않을 것입니다.\n\n이 블로그 글에서는 도메인 적응이 모델이 \"편안한 영역\"을 벗어난 작업에 직면했을 때 이러한 갭을 메우는 데 도움이 되는 방법으로 어떻게 도메인 적응이 도움이 될 수 있는지에 대해 설명했습니다. 문맥 내 학습과 세밀한 조정을 통해 도메인 적응을 위한 두 가지 강력한 접근법에 대해 깊이 파고들었습니다. 마지막으로, 이러한 접근법 사이에서 선택할 때 고려해야 할 절충안에 대해 논의했습니다. \n\n강력한 인공지능 능력과 실제 비즈니스 요구 사항 사이의 이 갭을 성공적으로 메우는 것은 기업을 위해 생성 모델 인공지능의 궁극적인 잠재력을 발휘하는 데 중요합니다.","ogImage":{"url":"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png"},"coverImage":"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png","tag":["Tech"],"readingTime":21},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e\"컴포트 존\"을 벗어나는 것 - LLM에 대한 도메인 적응 접근 방식에 대한 심층 탐구 3/3\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e특정 도메인이나 사용 사례에 대한 대형 언어 모델 (LLMs)의 도메인 적응을 탐색하고 계십니까? 이 3부작 블로그 시리즈에서는 도메인 적응의 동기에 대해 설명하고 이를 수행하는 다양한 옵션에 대해 깊이 파고들었습니다. 또한 인기있는 트레이드오프를 다루는 도메인 적응 여정을 마스터할 수 있는 상세 안내서가 제공됩니다.\u003c/p\u003e\n\u003cp\u003e부분 1: 도메인 적응 소개 - 동기, 옵션, 트레이드오프\n부분 2: 컨텍스트 학습에 대한 심층 탐구\n부분 3: 파인튜닝에 대한 심층 탐구 - 여기 있습니다!\u003c/p\u003e\n\u003cp\u003e참고: 모든 이미지는 별도로 언급되지 않는 한 저자가 제공했습니다.\u003c/p\u003e\n\u003ch1\u003e복습\u003c/h1\u003e\n\u003cp\u003e이 블로그 시리즈의 이전 부분에서는 인컨텍스트 학습의 개념을 탐색하여 대형 언어 모델(Large Language Models, LLMs)의 \"쾌적 영역\" 제한을 극복하는 강력한 방법으로 다뤘습니다. 이러한 기술이 어떻게 사용될 수 있는지, 작업을 변환하고 모델의 전문 분야로 이동하여 성능 향상 및 Helpfulness, Honesty, Harmlessness와 같은 주요 설계 원칙과의 일치를 도모하는 방법에 대해 논의했습니다. 이번 세 번째 부분에서는 두 번째 도메인 적응 접근 방법인 파인튜닝으로 주목을 기울일 것입니다. 파인튜닝의 세부 사항을 탐구하면서, 모델의 \"쾌적 영역\"을 확장하고 특정 도메인 및 작업에 적응시켜 성능을 향상시킬 수 있는 방법에 대해 알아볼 것입니다. 프롬프트 엔지니어링과 파인튜닝 사이의 트레이드오프에 대해 논의하고, 데이터 속도, 작업 모호성 및 기타 고려 사항과 같은 요소에 기반하여 언제 어느 접근 방식을 선택해야 하는지에 대한 안내를 제공할 것입니다.\u003c/p\u003e\n\u003ch1\u003e트랜스포머 101\u003c/h1\u003e\n\u003cp\u003e최첨단 LLM은 대부분 트랜스포머 아키텍처에 기반을 두고 있어요. 이 아키텍처는 2017년 Vaswani 등이 제안한 이후, 자연어 처리 분야를 혁명시킨 깊은 신경망 아키텍처의 가족입니다. 이 아키텍처 패밀리의 핵심 차별점은 콘텍스트에서 사용된 단어나 자연어의 더 큰 조각의 의미를 포착하는 데 뛰어난 \"어텐션\" 개념입니다.\u003c/p\u003e\n\u003cp\u003e트랜스포머 아키텍처는 두 가지 근본적으로 다른 구성 요소로 구성되어 있어요. 한쪽에는 \"인코더\" 블록이 있어 자연어의 의미를 이른바 문맥화된 임베딩으로 번역하는 데 집중합니다. 이는 벡터 공간에서의 수학적 표현입니다. 인코더 모델은 이러한 벡터 표현을 하위 결정론적 또는 확률적 작업인 분류 문제, NER 또는 의미 검색과 같은 곳에서 활용하는 데 특히 유용합니다. 반면, 디코더 블록은 다음 토큰 예측에 훈련되어 있어, 재귀적으로 사용될 경우 텍스트를 생성할 수 있는 능력을 가지고 있습니다. 이 블록은 텍스트 생성에 의존하는 모든 작업에 사용될 수 있어요. 이러한 구성 요소는 서로 독립적으로 사용할 수 있지만, 조합해서 사용할 수도 있습니다. 오늘날 생성적 AI 분야에서 언급되는 대부분의 모델이 디코더 전용 모델입니다. 그래서 이 블로그 글은 이 유형의 모델에 초점을 맞출 거예요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_1.png\" alt=\"ADeepDiveintoFine-Tuning\"\u003e\u003c/p\u003e\n\u003ch1\u003eE2E 파인 튜닝 파이프라인\u003c/h1\u003e\n\u003cp\u003eFine-tuning은 LLaMA2와 같은 기본 모델에 특정 분야 전문성을 효율적으로 주입하기 위해 전이 학습을 활용합니다. 이 과정은 도메인별 데이터에 대한 모델의 가중치를 업데이트하면서 전체 네트워크 구조를 변경하지 않는 방식으로 이루어집니다. 대규모 데이터셋과 컴퓨팅 파워가 필요한 전체 사전 학습과는 달리, fine-tuning은 매우 샘플 및 컴퓨팅 효율적입니다. 전체적으로, 이 과정은 다음 단계로 분해될 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_2.png\" alt=\"Fine-Tuning Phases\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e데이터 수집과 선택: 모델에 투입될 전용 데이터 집합을 신중하게 선택해야 합니다. 더불어, fine-tuning 목적에 따라 데이터가 아직 사용 가능하지 않을 수 있으며 목적에 맞게 수집되어야 합니다. 데이터의 양적 또는 질적 특성이 다른 경우도 있습니다. 또한 데이터 품질과 함께 데이터 소스, 기밀성 및 지적 재산권, 라이선스, 저작권, 개인 식별 정보 등의 측면을 고려해야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLLM 사전 학습은 일반적으로 웹 스크랩 및 정돈된 말뭉치를 활용합니다. fine-tuning은 도메인 적응 방식으로 데이터셋이 주로 조직, 지식 또는 작업 특정 도메인의 레이블 또는 미레이블 데이터로 구성되어 있다는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e이 데이터는 다양한 곳에서 수집될 수 있습니다(문서 저장소, 사람이 작성한 콘텐츠 등). 미세 조정을 위해서는 품질에 대해 신중하게 선택하는 것이 중요하지만 위에서 언급한 것처럼 기밀 유지 및 지적 재산권(IP), 라이센스, 저작권, 개인 식별 정보(PII) 등과 같은 주제를 고려해야 합니다.\u003c/p\u003e\n\u003cp\u003e또 다른 중요한 차원은 훈련 데이터 집합을 라벨이 지정된(labeled) 및 지정되지 않은(unlabeled) 부분(선호도 포함)으로 분류하는 것입니다. 도메인 적응 미세 조정은 라벨이 지정되지 않은 텍스트 데이터가 필요합니다(그림 4를 참조하십시오). 다시 말해, 관련 콘텐츠와 충분한 품질로 간주되는 어떤 자연어의 전문 문서를 사용할 수 있습니다. 실제 사용 사례에 따라 사용자 매뉴얼, 내부 문서 또는 심지어 법적 계약서 등이 될 수 있습니다.\u003c/p\u003e\n\u003cp\u003e한편, 명시적으로 레이블이 지정된 데이터셋인 지시-맥락-응답 데이터셋과 같은 것들은 지도 미세 조정 방법에 사용될 수 있습니다. 최근에는 모델을 실제 사용자 피드백에 맞추기 위한 강화 학습 방법이 큰 성과를 보여주며, 이는 인간 또는 기계가 생성한 선호 데이터를 활용합니다. 예를 들어, 이진 인간 피드백(좋아요/싫어요)이나 다중 응답 순위와 같은 것들이 있습니다.\u003c/p\u003e\n\u003cp\u003e레이블이 지정되지 않은 데이터와는 달리, 레이블이 지정된 데이터셋은 특히 규모와 충분한 도메인 전문 지식을 갖추기 위해서 수집하기 어렵고 비용이 많이 듭니다. HuggingFace Datasets와 같은 오픈 소스 데이터 허브는 레이블이 지정된 데이터셋에 대한 좋은 소스일 수 있습니다, 특히 적절한 인간 인구 그룹의 광범위한 부분이 동의한 지역에서(예: 레드팀에 대한 독성 데이터셋)이고, 오픈 소스 데이터셋을 사용하여 모델의 실제 사용자 선호도를 대변할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그럼에도 불구하고, 많은 사용 사례는 보다 구체적이며 오픈 소스 프록시 데이터셋만으로 충분하지 않습니다. 실제 인간들이 레이블을 지정한 데이터셋이 필요한 경우도 있습니다. Amazon SageMaker Ground Truth와 같은 도구를 사용하면, 완전히 관리되는 사용자 인터페이스 및 워크플로 또는 전체 직원을 제공함으로써 데이터 수집을 돕을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e최근에는 합성 데이터 수집이 미세 조정 분야에서 점점 더 중요해지고 있습니다. 강력한 LLM(Large Language Model)을 사용하여 레이블이 지정된 데이터셋을 합성적으로 생성하는 것이 실제로 이루어지고 있습니다. 이는 SFT 또는 선호 정렬을 위한 것일 수 있습니다. 이 방법은 이미 유망한 결과를 보여주었지만, 현재로선 추가적인 연구가 더 필요하며, 실무에서 규모에 맞게 유용함을 입증해야 합니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e데이터 전처리: 선택한 데이터는 하류 학습 알고리즘에게 \"잘 소화될 수 있도록\" 전처리 되어야 합니다. 인기 있는 전처리 단계는 다음과 같습니다:\n\u003cul\u003e\n\u003cli\u003e품질 관련 전처리, 예를 들어 포맷 지정, 중복 제거, PII 필터링\u003c/li\u003e\n\u003cli\u003e섬세한 조정 접근 방식 관련 전처리: 예를 들어 감독형 섬세 조정을 위한 프롬프트 템플릿으로 랜더링\u003c/li\u003e\n\u003cli\u003eNLP 관련 전처리, 예를 들어 토큰화, 임베딩, 쪼개기 (문맥 창에 따라)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e모델 학습: 선택한 섬세 조정 방식에 따라 딥 뉴럴 네트워크를 학습합니다. 아래에서 자세히 논의할 인기 있는 섬세 조정 방식은 다음과 같습니다:\n\u003cul\u003e\n\u003cli\u003e계속된 사전 훈련, 즉 도메인 적응 섬세 조정: 풀 텍스트 데이터에 대한 훈련, 다음 토큰 예측 작업과 연동된 정렬\u003c/li\u003e\n\u003cli\u003e감독형 섬세 조정: 레이블된 데이터를 활용한 섬세 조정 접근 방식, 타깃 레이블로 정렬\u003c/li\u003e\n\u003cli\u003e선호도 정렬 접근 방식: 선호 데이터를 활용한 섬세 조정 접근 방식, 모델/시스템의 실제 사용자가 정의한 원하는 동작과 일치시킴\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이어서 단계별로 자세히 살펴보겠습니다. 학습 접근 방식과 다양한 섬세 조정 접근 방식 소개부터 데이터셋 및 데이터 처리 요구사항으로 이동하기 전에 시작해보겠습니다.\u003c/p\u003e\n\u003ch1\u003e학습\u003c/h1\u003e\n\u003cp\u003e이 섹션에서는 디코더 트랜스포머 모델을 학습하는 방법을 탐구합니다. 이는 사전 학습과 섬세 조정 모두에 적용됩니다.\n레이블이 지정되지 않은 데이터로 비지도 학습이나 레이블이 지정된 데이터로 지도 학습과 같은 전통적인 ML 학습 방식과 달리, 트랜스포머 모델의 학습은 자가지도 학습이라고 불리는 혼합 접근 방식을 활용합니다. 이는 레이블이 지정되지 않은 텍스트 데이터가 공급되더라도, 알고리즘이 실제로는 특정 입력 토큰을 가림으로써 내재적으로 자기 감독하고 있기 때문입니다. 아래의 입력 토큰 시퀀스 \"Berlin is the capital of Germany.\"을 고려하면, 이는 y가 가려진 토큰이고 X가 나머지인 지도 학습 예제로 이어집니다.\u003c/p\u003e\n\u003cp\u003e위에서 언급한 자가 지도 학습 접근 방식은 모델 가중치를 언어 모델링(LM) 특정 손실 함수로 최적화합니다. 인코더 모델 훈련은 무작위로 토큰을 마스킹하여 양방향 컨텍스트를 활용하는 마스킹 언어 모델링(MLM)을 사용하며, 디코더 전용 모델은 시퀀스의 가장 오른쪽 토큰을 항상 마스킹하여 단방향 컨텍스트를 가지는 인과 언어 모델링(CLM) 방식으로 매핑됩니다. 간단히 말하면, 이는 이전의 의미적 문맥을 바탕으로 이후 토큰을 예측하는 방식으로 자가 회귀적으로 훈련된다는 것을 의미합니다. 이외에도 순열 언어 모델링(PLM)과 같은 다른 LM 접근 방식이 있으며, 해당 방식은 모델을 사용하여 무작위로 섞인 토큰 시퀀스를 정렬된 순서로 되돌리도록 조건부화하는 것입니다.\u003c/p\u003e\n\u003cp\u003eCLM 작업을 대리로 사용하여 예측과 실제 값이 생성되며, 이를 사용하여 예측 손실을 계산할 수 있습니다. 따라서 모델 어휘 전체의 토큰에 대한 예측 확률 분포는 그라운드 트루스인 실제 값과 비교되며, 실제 값을 나타내는 토큰에 대한 확률이 1.0인 희소 벡터입니다. 사용한 실제 손실 함수는 특정 모델 아키텍처에 따라 다르지만, 토큰 예측과 같은 범주형 문제 공간에서 잘 작동하는 교차 엔트로피나 헷갈림 손실과 같은 손실 함수가 일반적으로 사용됩니다. 손실 함수는 깊은 신경망 역전파에 의한 경사 하강을 수행하여 매 반복할 때마다 손실을 점진적으로 최소화하고 모델 가중치를 우리의 교육 목표로 최적화하는 데 활용됩니다.\u003c/p\u003e\n\u003ch1\u003e세밀한 조정 변형 - 시나리오\u003c/h1\u003e\n\u003cp\u003e이론에는 식상하니, 실전으로 넘어가 봐요. 생물기술 분야의 기관으로 가정해보겠습니다. COVID-19 백신 연구를 중심으로 다양한 NLP 사용 사례를 위한 기반 모델로 LLM을 활용하려고 하는 중입니다. 불행하게도 이 도메인은 표준 제공형 사전 훈련된 LLM의 \"편안한 영역\"에 포함되지 않아 예상 수준 이하의 성능을 낸다고 하네요. 다음 섹션에서 우리가 상상한 시나리오에서 LLaMA2의 성능을 기대 이상으로 끌어올리는 데 도움을 줄 수 있는 다양한 세밀한 조정 방법에 대해 논의해볼 거예요.\u003c/p\u003e\n\u003ch1\u003e세밀한 조정 변형 - 계속된 사전 훈련인 도메인 적응 세밀한 조정\u003c/h1\u003e\n\u003cp\u003e제목이 나타내는 대로, 이 분야가 \"계속된 사전 훈련\"이라는 용어로 수렴하려고 하는 가운데, 이 섹션에서 논의되는 세밀한 조정 방법에 대한 명확한 용어는 아직 커뮤니티에서 합의되지 않은 상태입니다. 그렇다면 이 세밀한 조정 방법이 정확히 어떤 것을 의미하는 걸까요?\u003c/p\u003e\n\u003cp\u003e바이오테크 분야의 연구 논문들은 글쓰기 스타일이 꽤 독특하며, 도메인 특화 지식과 산업 또는 기관 별 약어가 가득합니다 (예: Polack et al, 2020; Figure 7 참조).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_7.png\" alt=\"Figure 7\"\u003e\u003c/p\u003e\n\u003cp\u003e반면에, 메타 LLaMA 모델의 사전 훈련 데이터 혼합물의 자세한 조사(Touvron et al., 2023; Figure 8)와 TII Falcon 모델 패밀리(Almazrouei et al., 2023; Figure 9)는 2.5% 및 2%만큼 일반용도의 LLMs가 연구 또는 심지어 바이오테크 도메인의 데이터를 매우 적게 포함하고 있음을 나타냅니다 (LLaMA 3 패밀리의 사전 훈련 데이터 혼합물은 블로그 게시 시점에 공개되지 않았습니다).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_8.png\" alt=\"Figure 8\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_9.png\" alt=\"2024-06-19-ADeepDiveintoFine-Tuning_9.png\"\u003e\u003c/p\u003e\n\u003cp\u003e따라서, 이와 같은 차이를 줄이기 위해 파인 튜닝을 활용하여 모델의 \"편안 영역\"을 확대하여 구체적인 작업에 대한 성능을 향상시켜야 합니다. 지속적인 사전 훈련은 정확히 위에서 언급한 차원에서 뛰어난 성과를 보여줍니다. 이것은 특정 데이터 집합에 대해 미리 훈련된 LLM의 매개변수 지식에 모델의 반응을 조정하기 위해 도메인별 정보 (도메인별 언어, 약어 등) 또는 원시 전문 텍스트에 암시적으로 포함된 정보와 같은 도메인별 정보를 주입하기 위한 기술로, 평문 텍스트 데이터로 구성된 특정 데이터 집합에 대해 미리 훈련된 LLM을 조정하는 과정을 포함합니다. 이 접근 방식에서 사전 훈련된 디코더 모델은 레이블이 없는 텍스트 데이터를 사용하여 다음 토큰 예측을 위해 파인 튜닝됩니다. 이로써 미지의 텍스트 데이터를 사용한 지속적인 사전 훈련은 사전 훈련에 가장 유사한 파인 튜닝 접근 방식이 됩니다.\u003c/p\u003e\n\u003cp\u003e우리의 예시에서는, 언급된 논문의 내용과 유사한 분야의 관련 문헌을 결합하여 연결된 텍스트 파일로 변환할 수 있습니다. 튜닝 목표 및 기타 요구 사항에 따라 필요없는 콘텐츠(예: 저자, 목차 등)의 제거, 중복 데이터 제거, 또는 PII 감소와 같은 데이터 정리 단계를 적용할 수 있습니다. 마지막으로, 데이터 집합은 모델 훈련에 사용되기 전에 일부 NLP-특정 전 처리(예: 토큰 분리, 컨텍스트 창에 따른 청킹 등 -위 참조-)을 거칩니다. 훈련 자체는 이전 장에서 논의된 것과 같이 전통적인 CLM 기반 훈련입니다. 바이오테크 도메인의 연구 논문 집합에 계속된 사전 훈련을 통해 LLaMA2를 조정한 후, 이제 해당 도메인에서 이를 \"BioLLaMA2\" 텍스트 완성 모델로 활용할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e파인 튜닝 변형 — 감독된 파인 튜닝 (SFT)\u003c/h1\u003e\n\u003cp\u003e안타깝게도, 우리 사람들은 해결하고 싶은 문제를 순수한 텍스트 완성/토큰 예측 형태로 제시하는 것을 선호하지 않아요. 대화를 선호하며, 특히 일을 처리하고자 할 때에는 수다스럽거나 지시적인 행동을 하는 경향이 있는 사람들이랍니다.\u003c/p\u003e\n\u003cp\u003e그래서 모델이 간단한 다음 토큰 예측을 넘어서도 미래를 예측할 때에는 약간의 세련된 요소가 필요해요. 여기서 감독된 세밀 조정 방식이 등장해요. 감독된 세밀 조정(SFT)은 사전 훈련된 LLM을 특정 데이터셋과 레이블이 지정된 예제들과 일치시키는 과정을 말해요. 이 기술은 모델의 응답을 특정 도메인이나 작업(예: 상기한 대화적 성격 또는 지시 따름)에 맞게 조정하는 데 중요하답니다. 대상 응용 프로그램을 잘 대표하는 데이터셋으로 학습함으로써, SFT는 LLM이 심층적인 이해를 개발하고 전문적인 요구 사항과 행동에 부합하는 더 정확한 출력물을 생산할 수 있게 해줘요.\u003c/p\u003e\n\u003cp\u003e상기한 것들 외에도, 좋은 SFT의 예시는 Q\u0026#x26;A 모델의 훈련, 개체 인식과 같은 데이터 추출 작업, 또는 해로운 응답을 방지하는 레드팀 활동 등이 있을 수 있어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_10.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e위에서 이해한 대로, SFT에는 레이블이 지정된 데이터셋이 필요합니다. 오픈 소스에는 일반 목적의 레이블이 지정된 데이터셋이 많이 있지만, 모델을 귀하의 특정 사용 사례, 산업 또는 지식 도메인에 가장 적합하게 맞추기 위해 사용자 정의 데이터셋을 수동으로 작성하는 것이 의미가 있을 수 있습니다. 최근에는 Claude 3나 GPT-4와 같은 강력한 LLM을 사용하여 이러한 데이터셋을 만드는 접근 방식이 인간 레이블링 대안으로 발전되어왔습니다.\u003c/p\u003e\n\u003cp\u003e\"dolly-15k\" 데이터셋은 Databricks 직원들에 의해 수동으로 만들어진 인기 있는 일반 목적의 오픈 소스 명령어 미세 조정 데이터셋입니다. 이 데이터셋에는 명령문과 문맥이 원하는 응답과 함께 레이블이 지정된 대략 15,000개의 예시가 포함되어 있습니다. 이 데이터셋은 우리의 BioLLaMA2 모델을 지시어를 따르는 작업, 예를 들어 닫힌 Q\u0026#x26;A 작업에 맞게 조정하는 데 사용될 수 있습니다. 지시어를 따르는 SFT를 위해, 데이터셋의 각 항목을 전체 텍스트 프롬프트로 변환하고, 모델을 맞추고자하는 작업을 나타내는 프롬프트 구조에 포함하여 진행할 것입니다. 이는 다음과 같이 보일 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e### 명령:\n{item.\u003cspan class=\"hljs-property\"\u003einstruction\u003c/span\u003e}\n### 문맥:\n{item.\u003cspan class=\"hljs-property\"\u003econtext\u003c/span\u003e}\n### 응답:\n{item.\u003cspan class=\"hljs-property\"\u003eresponse\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e프롬프트 템플릿은 모델 패밀리에 따라 다양할 수 있으며, 일부 모델은 해시태그보다 HTML 태그나 다른 특수 문자를 선호할 수 있습니다. 이 절차는 데이터셋의 각 항목에 대해 적용되며, 모든 항목이 큰 텍스트 조각으로 연결되기 전에 수행됩니다. 최종적으로 위에서 설명한 NLP 특정 전 처리 후, 이 파일은 다음 토큰 예측을 활용하여 모델로 훈련될 수 있으며, CLM 기반의 훈련 목표를 사용할 것입니다. 이 특정 프롬프트 구조에 지속적으로 노출되므로 모델은 그에 따라 작동하고 해당 방식으로 행동하는 것을 배울 것입니다 — 이 경우에는 지시어를 따릅니다. BioLLaMA2를 dolly-15k 데이터셋에 맞춘 후, BioLLaMA2-instruct 모델은 프롬프트를 통해 제출된 지시를 철저히 따를 것입니다.\u003c/p\u003e\n\u003ch1\u003e파인튜닝 변형 — 인간 선호도 조정 기술 (RLHF/PPO, DPO, KTO, ORPO)\u003c/h1\u003e\n\u003cp\u003eBioLLaMA2와 함께, 우리는 생물 기술 연구 영역에 적합한 모델을 가지고 있으며, 사용자가 기대하는 대로 편리하게 우리의 지시에 따릅니다. 하지만 기다려봐 — 모델이 실제 사용자와 일치하는 것일까? 지금까지 논의된 파인튜닝 접근 방식의 핵심 문제가 돋보입니다. 사용한 데이터셋은 우리가 사용자가 좋아하거나 필요로 한다고 생각하는 것의 대리인입니다: 선별된 연구 논문에서의 내용, 언어, 머리글, 그리고 일부 Databricks 직원들이 제작한 dolly-15k의 원하는 지시 행동과 같은 것들입니다. 이는 사용자 중심의 제품 개발 개념과 대비됩니다. 이는 기민한 제품 개발의 핵심적이고 잘 수립된 원리 중 하나입니다. 실제 대상 사용자로부터 반복적으로 피드백을 받는 것이 훌륭한 제품을 개발할 때 매우 성공적이라는 것이 입증되었습니다. 사실, 사용자를 위한 훌륭한 경험을 구축하려는 경우 우리가 해야 할 일이지 않을까요!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_11.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이를 염두에 두고, 연구자들은 인간 피드백을 성능 향상에 통합하는 방법을 찾는 데 상당한 노력을 기울였습니다. 이에따라, 그들은 (심층적인) 강화 학습 (RL) 과 상당한 중첩이 있다는 것을 깨달았는데, 이것은 환경 내에서 행동을 수행하는 자율 에이전트들에 대한 것으로, 다음 상태를 생성하며 항상 보상과 결합됩니다. 에이전트들은 훈련 단계에서 보상을 극대화하기 위해 점진적으로 최적화된 정책이나 가치지도에 따라 행동합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_12.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 개념은 LLM의 세계로 프로젝션될 때 LLM 자체가 에이전트로 작용하는 것으로 이어집니다. 추론 과정에서 자동 회귀 토큰 예측의 각 단계마다 모델의 어휘가 작업 공간이고 환경은 가능한 토큰 조합이 되는 행동을 수행합니다. 새로운 추론 주기마다 새로운 상태가 형성되며, 이는 이상적으로 어떤 인간 피드백과 관련된 보상과 함께 함께합니다.\u003c/p\u003e\n\u003cp\u003e이 아이디어에 기반하여 여러 인간의 선호 정렬 접근 방식이 제안되고 시험되었습니다. 다음에서는 가장 중요한 몇 가지 접근 방법에 대해 살펴보겠습니다:\u003c/p\u003e\n\u003ch2\u003e인간 피드백에 대한 강화학습 (RLHF) 및 근접 정책 최적화 (PPO)\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_13.png\" alt=\"여기에 이미지가 있습니다\"\u003e\u003c/p\u003e\n\u003cp\u003e인간 피드백을 통한 강화 학습은 초기 창조적 AI 혹평 중요한 기술적 기반 중 하나였으며, Anthropi Claude나 GPT-3.5와 같은 대형 디코더 모델들의 위험의 동반자가 된 획기적인 성과를 거두고 사용자 맞춤 방향으로 추가적인 동기 부여를 제공했습니다. RLHF는 두 단계로 작동하며 Figures 13과 14에서 설명되어 있습니다.\u003c/p\u003e\n\u003cp\u003e1단계 (Figure 13): 먼저, 실제 RL을 통한 교육 접근에서 나중에 사용할 보상 모델을 훈련해야 합니다. 따라서, 목적과 일치하는 프롬프트 데이터 집합(우리의 BioLLaMA2-instruct 모델의 경우, 지시와 문맥의 쌍일 것입니다)이 모델에 공급되어 세부 조정되며, 단 하나가 아닌 두 개 이상의 추론 결과를 요청합니다. 이러한 결과는 최적화 목표에 따라 인간 레이블러에게 제공되고, 1등, 2등, 3등 등과 같이 점수 매기기를 기반으로 합니다. \"Anthropic/hh-rlhf\"와 같은 몇 가지 오픈 소스 기호 순위 데이터 집합도 있으며, 이것은 red-teaming 및 정직성 및 무해성 목표에 맞게 설계되어 있습니다. 정규화 단계와 보상 값으로의 변환 후, 보상 모델이 단일 샘플-보상 쌍을 기반으로 훈련되고, 여기서 샘플은 단일 모델 응답입니다. 보상 모델 아키텍처는 일반적으로 세부 값으로 잠재 공간을 투사하는 정보 대신 토큰에 대한 확률 분포가 됩니다. 그러나 이 모델의 매개 변수의 이상적인 크기는 여전히 연구 대상이며, 지난 시간에 모델 제공 업체에 의해 다양한 접근 방식이 선택되어 왔습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_14.png\" alt=\"여기에 이미지가 있습니다\"\u003e\u003c/p\u003e\n\u003cp\u003e단계 2 (그림 14): 새 보상 모델을 사용하여 실제 모델을 학습하는 단계로 넘어갑니다. 따라서 해당 모델을 조정하기 위해 다른 일련의 프롬프트가 모델을 통해 공급됩니다 (도면의 회색 상자), 결과적으로 각각 하나의 응답이 생성됩니다. 이후 이러한 응답은 각각 개별 보상을 검색하기 위해 보상 모델에 공급됩니다. 그런 다음, 보상이 할당된 모델의 답변을 최대화하기 위해 Proximal Policy Optimization (PPO)이라는 정책 기반 강화 학습 알고리즘이 사용되어 모델의 가중치를 점진적으로 조정합니다. CLM과는 달리, 그라디언트 하강 대신에 이 접근 방식은 이제 목표 (보상)를 최대화하려 하기 때문에 그라디언트 상승(또는 그라디언트 하강 오버 1 − 보상)을 활용합니다. 학습 중에 모델 동작의 지나친 드리프트를 방지하기 위한 알고리즘적 안정성을 높이기 위해서 PPO와 같은 강화 학습 기반 알고리즘에 의해 야기될 수 있는 너무 큰 드리프트를 예방하기 위해 예측 변화 벌점이 보상 항에 추가되어, 초기 언어 모델의 입력 프롬프트에 대한 예측 확률 분포에서 너무 많이 벗어나는 답변을 벌한다.\u003c/p\u003e\n\u003cp\u003ePPO와 함께 RLHF 이상의 다른 접근 방법이 개발되어 왔으며, 현재는 선호 정렬을 위해 가장 널리 사용되고 검증된 접근 방법입니다. 다음 몇 섹션에서는 조금 더 고급 수준으로 이러한 접근법 중 일부에 대해 자세히 살펴볼 것입니다. 이는 고급 독자를 대상으로 한 내용이므로 딥 러닝 및 강화 학습에 대한 경험 수준에 따라 직접 다음 섹션인 \"의사결정 흐름도 - 선택할 모델, 선택할 파인튜닝 경로\"로 건너뛰고자 할 수도 있습니다.\u003c/p\u003e\n\u003ch2\u003e직접 정책 최적화 (DPO)\u003c/h2\u003e\n\u003cp\u003e직접 정책 최적화(Direct Policy Optimization, DPO)은 RLHF에서 유래한 선호 정렬 접근 방식으로, RLHF의 두 가지 주요 약점을 해결하는 것이 목표입니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e먼저 보상 모델을 훈련하는 것은 추가적인 자원 투자를 필요로 하며, 보상 모델의 크기에 따라 상당히 중요할 수 있습니다.\u003c/li\u003e\n\u003cli\u003ePPO를 사용한 RLHF의 훈련 단계는 대규모 연산 클러스터가 필요합니다. 초기 LM, 조정된 LM, 보상 모델의 3개 복제본이 저지연 설정에서 동시에 호스팅 및 조정되어야 합니다.\u003c/li\u003e\n\u003cli\u003eRLHF는 불안정한 절차가 될 수 있습니다 (→ 예측 변동 페널티가 이를 완화하려고 시도합니다)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_15.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eDPO는 대안적인 선호 정렬 접근법으로 Rafailov 등(2023)에 의해 제안되었습니다. DPO의 주요 아이디어는 보상 모델 훈련을 건너뛰고 최종 선호 정렬 LLM을 선호 데이터에 직접 조정하는 것입니다. 이는 보상 모델의 매개변수화(보상 항목)를 손실 함수(figure 16)로 변환하는 몇 가지 수학적 수정을 적용하고 실제 보상 값들을 선호 데이터에 대한 확률 값으로 대체함으로써 달성됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_16.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이는 선호도에 맞는 모델로 나아가면서 계산 및 알고리즘 복잡성을 줄입니다. 논문은 RLHF와 비교하여 성능 향상도 보여주고 있지만, 이 방법은 최근에 제안된 것이기 때문에 결과는 실질적인 증명을 필요로 합니다.\u003c/p\u003e\n\u003ch2\u003e칸먼-트벽시 최적화 (KTO)\u003c/h2\u003e\n\u003cp\u003eRLHF와 DPO와 같은 인간 피드백과 언어 모델을 조정하는 기존 방법들은 입력에 대해 하나가 다른 것을 선호하는 출력 쌍처럼 선호 데이터가 필요합니다. 그러나 규모별로 고품질의 선호 데이터를 수집하는 것은 현실 세계에서 어렵고 비싸며 많은 노이즈, 불일치 및 비교 불가능성으로 인해, 서로 다른 인간 평가자들이 어떤 출력이 더 나은지에 대해 상이한 견해를 가질 수 있습니다. KTO는 Ethayarajh 등에 의해 (2024) 제안된 대안적 방법으로, 간단하고 풍부한 신호와 함께 작동할 수 있는 접근법으로 소개되었습니다 — 주어진 출력이 특정 입력에 대해 바람직한지 또는 바람직하지 않은지만 알고 있으면 되며, 출력 간의 상대적인 선호를 알 필요가 없습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_17.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eKTO(카네만-트버스키 최적화)는 세대의 상대적 \"좋음\"을 포착하는 보상 함수를 정의한 후, 모델을 최적화하여 카네만-트버스키 가치 함수하에서 이 보상의 기대값을 최대화하는 방식으로 작동합니다. 카네만과 트버스키의 전망 이론은 인간이 편향되었지만 명확히 정의된 방식으로 불확실한 결과에 대한 결정을 내리는 방법을 설명합니다. 이 이론은 이득에서 오목하고 손실에서 오목한 가치 함수에 의존하며, 수익과 손실을 분리하는 기준점이 존재합니다(17번 그림 참조). KTO는 선호도의 가능성을 극대화하는 것이 아니라 사람의 유틸리티 개념을 직접 최적화합니다.\u003c/p\u003e\n\u003cp\u003eKTO의 주요 혁신점은 선호 가능 여부에 대한 이진 신호만 필요하다는 점입니다. 이는 전체 선호도 쌍 대신 더 데이터 효율적으로 사용할 수 있도록 합니다. 바이너리 피드백 신호는 훨씬 풍부하고 수집 비용이 저렴하기 때문에 선호도 기반 방법보다 데이터 효율적입니다(18번 그림 참조).\u003c/p\u003e\n\u003cp\u003eKTO는 선호도 데이터가 부족하거나 비용이 많이 드는 상황에서 특히 유용하지만, 모델 출력의 품질에 대한 바이너리 피드백의 대량을 사용할 수 있는 경우입니다. 논문에 따르면, 모델 규모가 클 때 선호도 기반 방법인 DPO와 같은 성능을 극대화하거나 심지어 능가할 수 있습니다. 그러나 실제 환경에서 규모별로 확인되어야 합니다. KTO는 선호도 기능성을 극대화하는 것이 목표일 때 선호될 수 있습니다. 그러나 선호도 데이터가 노이즈가 적거나 추이가 없는 고 품질일 때라면 선호도 기반 방법이 여전히 더 나은 선택일 수 있습니다. KTO는 일부 경우에는 극도의 데이터 불균형을 처리하고 감독된 세밀 조정을 회피하는 측면에서 이론적 장점을 가지고 있습니다.\u003c/p\u003e\n\u003ch2\u003eOdds Ration Preference Optimization (ORPO)\u003c/h2\u003e\n\u003cp\u003eORPO의 주된 동기는 기존의 선호 정렬 방법인 RLHF 및 DPO와 같은 제한 사항을 해결하는 데에 있습니다. 이러한 방법들은 종종 별도의 지도적 세세조정(SFT) 단계, 참조 모델 또는 보상 모델이 필요합니다. 홍 등(2024)의 논문은 SFT만 사용하면 원하지 않는 스타일로 토큰을 생성할 가능성이 높아진다고 주장합니다. 교차 엔트로피 손실이 비선호하는 응답에 대해 직접적인 처벌을 제공하지 않기 때문입니다. 동시에 그들은 SFT가 강력한 선호 정렬 모델에 수렴하는 데 중요하다고 주장합니다. 이로 인해 자원을 많이 소모하는 두 단계 정렬 프로세스가 발생합니다. 이러한 단계를 결합하여 ORPO는 SFT의 도메인 적응 이점을 유지하면서 동시에 선호 정렬 접근 방식에 의해 목표로 하는 원치 않는 생성 스타일을 파악하고 완화하려고 합니다. (그림 19 참조)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_19.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eORPO는 기존의 인과 언어 모델링에 연결된 손실(예: 교차 엔트로피 손실)에 오즈 비율이 기반된 처벌을 통합하는 혁신적인 선호 정렬 알고리즘을 소개합니다. ORPO의 목적 함수는 두 구성 요소로 이루어집니다: SFT 손실과 상대 비율 손실 (LOR). LOR 항목은 선호하는 응답과 비선호하는 응답을 생성하는 우도 사이의 오즈 비율을 최대화하여 모델에게 거부 응답에 높은 확률을 할당하는 것에 대한 처벌을 효과적으로 적용합니다.\u003c/p\u003e\n\u003cp\u003eORPO는 미리 학습된 언어 모델을 세부 도메인이나 작업에 적응시키면서 모델의 출력이 인간의 선호와 일치하도록 보장하고 싶을 때 특히 유용합니다. 이는 UltraFeedback 또는 HH-RLHF 데이터셋과 같이 pairwise preference 데이터셋에 액세스할 수 있는 시나리오에서 적용할 수 있습니다. 이를 염두에 둔 ORPO는 별도의 참조 모델, 보상 모델 또는 두 단계의 미세 조정 접근 방식이 필요하지 않아 RLHF 및 DPO에 대한 더 효율적이고 효과적인 대안으로 설계되었습니다.\u003c/p\u003e\n\u003ch1\u003e의사결정 플로우 차트 - 어떤 모델을 선택하고 어떤 미세 조정 경로를 선택할 것인가\u003c/h1\u003e\n\u003cp\u003e여러 미세 조정 접근 방식을 심층적으로 살펴본 후에 특정 요구사항에 따라 시작할 모델과 선택할 접근 방법에 대한 명확한 질문이 제기됩니다. 미세 조정 목적에 적합한 올바른 모델을 선택하기 위한 접근 방법은 두 단계 접근 방식입니다. 첫 번째 단계는 어떤 미세 조정 의도도 없는 기본 모델을 선택하는 과정과 유사하며, 다음 차원을 고려하여 보다 적합한 모델을 선택합니다(완전하지 않음):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e사용할 플랫폼: 각 플랫폼은 해당 플랫폼을 통해 액세스할 수 있는 일련의 모델을 제공합니다. 이를 고려해야 합니다. 지역별 모델 가용성에 대한 차이가 있을 수 있음을 유의하십시오. 이에 대한 자세한 정보는 해당 플랫폼의 문서를 확인하시기 바랍니다.\u003c/li\u003e\n\u003cli\u003e성능: 조직은 특정 작업에 최소한의 모델을 사용해야 합니다. 이에 대해 일반적인 지침은 제공되지 않지만 세밀한 조정은 모델의 성능을 크게 향상시킬 수 있습니다 (더 작은 세밀하게 조정된 모델이 더 큰 일반적인 모델보다 성능을 능가할 수 있음). 기본 모델의 평가 결과를 활용하는 것은 중요한 지표가 될 수 있습니다.\u003c/li\u003e\n\u003cli\u003e예산 (TCO): 일반적으로 더 큰 모델은 더 많은 컴퓨팅 및 잠재적으로 멀티 GPU 인스턴스를 필요로 하며, 복수의 가속기에 걸쳐 교육 및 제공을 위해 이에 직접적인 영향을 미칩니다. 이는 교육 및 추론 비용, 교육 및 추론의 복잡성, 필요한 자원 및 기술 등, 모델의 전체 수명 주기 중 TCO의 일부로 고려되어야 합니다. 이는 단기 및 장기 예산 할당과 일치해야합니다.\u003c/li\u003e\n\u003cli\u003e라이선스 모델: 모델마다 도메인 및 상업적 이용에 따라 라이선스 제약사항이 있습니다. 이를 고려해야 합니다.\u003c/li\u003e\n\u003cli\u003e지배 체제, 윤리, 책임 있는 인공 지능: 모든 조직은 이러한 차원과 함께 규정 가이드라인을 가지고 있습니다. 모델 선택 시 반드시 고려되어야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e예시: 조직은 기본 모델의 평가 결과를 바탕으로 LLaMA 2 모델을 고려하고, 기본 모델을 기반으로 한 Anthropic Claude 또는 AI21Labs Jurassic과 같은 프로프라이어터리 모델의 사용을 배제하기로 결정할 수 있습니다. 더 나아가, 그들은 이 모델의 7B-파라미터 버전만 사용하기로 결정하여 단일 GPU 인스턴스에서 교육 및 제공할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e두 번째 단계는 실험 단계를 위해 고려해야 할 1-몇 개 모델로 초기 모델 선택을 좁히는데 관련되어 있습니다. 구체적인 접근 방식을 선택할 최종 결정은 아래 그림에서 언어 모델의 미세 조정 수명주기에 대한 원하는 진입점에 따라 달라집니다.\u003c/p\u003e\n\u003cp\u003e따라서, 다음 차원을 고려해야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e수행할 작업: 각각의 사용 사례는 특정 모델 동작을 요구합니다. 어떤 사용 사례에는 간단한 텍스트 완성 모델 (다음 토큰 예측)이 충분할 수 있지만, 대부분의 사용 사례에서는 수다성, 지시사항 준수 또는 다른 작업 특정 동작이 필요합니다. 이 요구 사항을 충족하기 위해 우리는 수행할 작업을 역발상하는 접근 방식을 취할 수 있습니다. 즉, 원하는 작업을 수행할 모델에 맞게 정의해야 합니다. 이는 일러스트를 기준으로 모델이 원하는 모델 동작과 일치하여 파란색, 주황색 또는 녹색 원 안에서 끝나야 함을 의미합니다. 동시에 세부 튜닝 여정을 흘러가는 흐름도의 가능한 경로와 함께 정의해야 합니다.\u003c/li\u003e\n\u003cli\u003e올바른 시작점 선택 (합리적인 범위 내에서): 세부 튜닝 여정이 끝나야 할 위치에 대해 명확해져야 하지만, 해당 흐름도에서 어디서든 출발할 수 있습니다. 그러나 이는 합리적이어야 합니다 — 수백만 개의 게시된 모델이 있는 모델 허브 시대에는 다른 사람이 이미 세부 튜닝 단계를 수행했고 결과 모델을 공유한 경우 체크하는 것이 의미있을 수 있습니다.\u003c/li\u003e\n\u003cli\u003e세부 튜닝은 반복적이고 재귀적인 프로세스입니다: 원하는 모델까지 여러 번의 연속적인 세부 튜닝 작업을 수행할 수 있습니다. 그러나 모델의 가중치에 무한한 양의 정보를 인코드할 수 없으므로 추월하는 것을 염두에 둬야 합니다. 이를 해소하기 위해 이 논문과 블로그에서 보여주는 LoRA와 같은 매개변수 효율 세부 튜닝 방법을 활용할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e특정 작업 성능 향상에 초점을 맞춤: 세부 튜닝은 특정 작업에서 모델의 성능을 향상시키기 위해 수행됩니다. 언어 패턴(도메인별 언어, 약어 등)이나 귀하의 훈련 데이터에 암시적으로 포함된 정보에서 성능 향상을 원한다면 계속해서 사전 훈련을 진행해야 합니다. 특정 작업으로의 성능 향상을 원한다면 지도 세부 튜닝을 선택해야 합니다. 모델 동작을 실제 사용자와 일치시키고자 하는 경우 인간의 선호에 맞게 조정하는 것이 올바릅니다.\u003c/li\u003e\n\u003cli\u003e데이터 가용성: 훈련 데이터도 우리가 선택하는 경로에 영향을 미칠 것입니다. 일반적으로 조직은 레이블이 지정되지 않은 텍스트 데이터를 보유하고 있는 경우가 많습니다. 레이블이 지정된 데이터를 획득하는 것은 비용이 많이 들 수 있습니다. 흐름도를 통해 탐색할 때 이 차원을 고려해야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이와 같은 역발상 방식을 흘러가는 접근과 상기된 흐름도를 통해 시작할 모델과 세부 튜닝 흐름도를 따라갈 경로를 식별할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e좀 더 명확히 이해하실 수 있도록 두 가지 예제를 제공해 드리겠습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_22.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e예시 1: 위의 튜닝 섹션에 설명된 예시를 따라가면, 특정 사용 사례에 맞는 가르침 모델을 형성할 수 있습니다. 그러면 실제 사용자의 선호도에 부합되며, 바이오테크 분야의 성능을 향상시키고 싶습니다. 연구 논문 형태의 레이블되지 않은 데이터가 제공됩니다. 우리는 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 LLaMA-2-7b 가르침 모델을 발표하지 않았으므로, 텍스트 완성 모델인 LLaMA-2-7b-base에서 출발합니다. 그런 다음, 연구 논문 코퍼스에서 계속된 사전 훈련을 수행한 후, dolly-15k 데이터셋과 같은 오픈 소스 가르침 데이터셋에서 지도 학습 미세 조정을 수행합니다. 이로써 LLaMA-2-7B-base의 가르침으로 조정된 바이오테크 버전인 BioLLaMA-2-7b-instruct가 생성됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 선호도 데이터셋을 수집하고 보상 모델을 훈련한 후, PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_23.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e예시 2: 이 예시에서는 채팅 모델을 사용하려고 합니다. 그러나 실제 사용자의 선호도에 부합되도록 조정하고 싶습니다. 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 제공하는 사용자 지정 채팅 모델인 LLaMA-2-7b-chat을 시작점으로 사용할 수 있음을 알게 됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 사용자로부터 선호도 데이터셋을 수집하고, 보상 모델을 훈련한 후 PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e생성 모델 인공지능은 기업과 조직에 많은 흥미로운 사용 사례를 제공합니다. 그러나 이러한 응용 프로그램은 대개 개별 소비자용으로 레시피나 연설을 생성하는 것과 같은 것보다 훨씬 복잡합니다. 기업에서는 인공지능이 조직의 특정 도메인 지식, 프로세스 및 데이터를 이해해야 합니다. 기존 기업 시스템 및 애플리케이션과 통합되어야 합니다. 또한 다양한 직원과 역할에 맞는 맞춤형 경험을 제공하면서 안전하게 작동해야 합니다. 기업 환경에서 생성 모델 인공지능을 성공적으로 구현하려면 기술이 조직의 고유한 요구 사항에 맞게 신중하게 설계되고 맞춤화되어야 합니다. 일반적으로 공개적으로 학습된 모델을 사용하는 것만으로 충분하지 않을 것입니다.\u003c/p\u003e\n\u003cp\u003e이 블로그 글에서는 도메인 적응이 모델이 \"편안한 영역\"을 벗어난 작업에 직면했을 때 이러한 갭을 메우는 데 도움이 되는 방법으로 어떻게 도메인 적응이 도움이 될 수 있는지에 대해 설명했습니다. 문맥 내 학습과 세밀한 조정을 통해 도메인 적응을 위한 두 가지 강력한 접근법에 대해 깊이 파고들었습니다. 마지막으로, 이러한 접근법 사이에서 선택할 때 고려해야 할 절충안에 대해 논의했습니다.\u003c/p\u003e\n\u003cp\u003e강력한 인공지능 능력과 실제 비즈니스 요구 사항 사이의 이 갭을 성공적으로 메우는 것은 기업을 위해 생성 모델 인공지능의 궁극적인 잠재력을 발휘하는 데 중요합니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-ADeepDiveintoFine-Tuning"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>