<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기 | itposting" data-gatsby-head="true"/><meta property="og:title" content="LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini" data-gatsby-head="true"/><meta name="twitter:title" content="LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 19:24" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>소개</h1>
<p>리트리버는 RAG(Retrieval Augmented Generation) 파이프라인의 가장 중요한 부분입니다. 이 문서에서는 LlamaIndex를 사용하여 키워드 및 벡터 검색 리트리버를 결합한 사용자 정의 리트리버를 구현할 것입니다. Gemini LLM을 사용한 다중 문서 채팅은 우리가 이 RAG 파이프라인을 구축할 프로젝트 사례입니다. 프로젝트를 시작하기 전에 우리는 이러한 애플리케이션을 구축하기 위해 서비스 및 스토리지 콘텍스트와 같은 몇 가지 중요한 구성 요소를 먼저 이해할 것입니다.</p>
<h2>학습 목표</h2>
<ul>
<li>RAG 파이프라인에 대한 통찰력을 얻고, 리트리버와 제너레이터 구성 요소의 역할을 이해하여 맥락적으로 응답을 생성하는 방법을 알아봅니다.</li>
<li>키워드 및 벡터 검색 기술을 통합하여 검색 정확도를 향상시키는 사용자 정의 리트리버 개발법을 배웁니다.</li>
<li>LlamaIndex를 활용하여 데이터 삽입을 수행하고, LLM에 맥락을 제공하며, 사용자 지정 데이터와의 연결을 깊이 있게 이해하는 능력을 습득합니다.</li>
<li>LLM 응답에서 발생할 수 있는 환각 현상을 완화하는 데 사용자 정의 리트리버의 중요성을 이해합니다.</li>
<li>문서의 관련성을 향상시키기 위해 다시 순위 지정 및 HyDE와 같은 고급 리트리버 구현을 탐색합니다.</li>
<li>Gemini LLM 및 LlamaIndex 내에서 임베딩을 통합하여 응답 생성 및 데이터 저장을 개선하여 RAG 기능을 향상합니다.</li>
<li>맞춤형 리트리버 설정에 대한 의사 결정 능력을 발전시켜 검색 결과 최적화를 위해 AND 및 OR 연산 중 선택하는 방법을 배웁니다.</li>
</ul>
<h1>LlamaIndex이 무엇인가요?</h1>
<p>대규모 언어 모델 분야는 매일 크게 발전하고 있습니다. 많은 모델이 빠르게 출시되고 있기 때문에 이러한 모델을 사용자 정의 데이터와 통합할 필요성이 점점 커지고 있습니다. 이러한 통합은 기업, 기업, 그리고 최종 사용자에게 더 많은 유연성과 데이터와의 깊은 연결성을 제공합니다.</p>
<p>초기에 GPT-index로 알려져 있었던 LlamaIndex는 LLM 애플리케이션을 위해 설계된 데이터 프레임워크입니다. ChatGPT와 같은 사용자 지정 데이터 기반 챗봇을 만드는 인기가 계속해서 증가함에 따라 LlamaIndex와 같은 프레임워크는 점점 가치가 있는 존재가 되고 있습니다. 핵심적으로, LlamaIndex는 다양한 데이터 커넥터를 제공하여 데이터 수집을 용이하게합니다. 이 글에서는 우리가 데이터를 LLM에 context로 전달하는 방법을 알아볼 것이며, 이 개념이 의미하는 것이 Retrieval Augmented Generation, 줄여서 RAG입니다.</p>
<h1>RAG가 무엇인가요?</h1>
<p>RAG(축약어인 Retrieval Augmented Generation)에는 Retriever와 Generator 두 가지 주요 구성 요소가 있습니다.</p>
<ul>
<li>Retriever는 벡터 데이터베이스일 수 있으며, 사용자 쿼리에 관련 문서를 검색한 후 이를 문맥으로 사용자에게 전달하는 역할을 합니다.</li>
<li>Generator 모델은 대형 언어 모델로, 검색된 문서와 프롬프트를 함께 사용하여 문맥으로부터 의미 있는 응답을 생성하는 역할을 합니다.</li>
</ul>
<p>이 방식으로 RAG는 Automated Few shot prompting을 통한 문맥 학습에 대한 최적의 솔루션입니다.</p>
<h1>Retriever의 중요성</h1>
<p><img src="/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png" alt="Retriever Component"></p>
<p>RAG 파이프라인에서 Retriever 구성 요소의 중요성을 이해해봅시다.</p>
<p>사용자 정의 검색기를 개발할 때, 우리의 Bed을 가장 잘 수행하는 검색기 유형을 결정하는 것이 중요합니다. 우리의 목적에 따라 우리는 Keyword Search와 Vector Search를 통합한 Hybrid Search를 구현할 것입니다.</p>
<p>Vector Search는 유사성 또는 의미 검색을 기반으로 사용자 쿼리에 대한 관련 문서를 식별하며, Keyword Search는 용어 발생 빈도에 기반하여 문서를 찾습니다. 이 통합은 LlamaIndex를 사용하여 두 가지 방법으로 달성할 수 있습니다. Hybrid Search의 사용자 정의 검색기를 구축할 때, 중요한 결정 사항은 AND 또는 OR 연산자 중 어느 것을 사용할지 선택하는 것입니다:</p>
<ul>
<li>
<p>AND 연산: 이 방법은 모든 지정된 용어를 포함하는 문서를 검색하여 더 제한적이지만 높은 관련성을 보장합니다. 이를 키워드 검색과 벡터 검색 결과 간의 교집합으로 생각할 수 있습니다.</p>
</li>
<li>
<p>OR 연산: 이 방법은 지정된 용어 중 어떤 것이라도 포함하는 문서를 검색하며 결과의 폭을 늘릴 수 있지만 관련성을 줄일 수 있습니다. 이를 키워드 검색과 벡터 검색 결과 간의 합집합으로 생각할 수 있습니다.</p>
</li>
</ul>
<h1>LLamaIndex를 사용한 사용자 지정 검색기 만들기</h1>
<p>이제 LLamaIndex를 사용하여 사용자 지정 검색기를 만들어 보겠습니다. 이를 위해 특정 단계를 따라야 합니다.</p>
<h1>단계 1: 설치</h1>
<p>Google Colab이나 Jupyter Notebook에서 코드 구현을 시작하려면 주로 필요한 라이브러리를 설치해야 합니다. 이 경우에는 사용자 지정 검색기 생성을 위해 LlamaIndex, 임베딩 모델 및 LLM 추론을 위한 Gemini, 데이터 커넥터로 PyPDF를 사용할 것입니다.</p>
<pre><code class="hljs language-js">!pip install llama-index
!pip install llama-index-multi-modal-llms-gemini
!pip install llama-index-embeddings-gemini
</code></pre>
<h1>단계 2: Google API 키 설정</h1>
<p>이 프로젝트에서는 Google Gemini를 사용하여 대규모 언어 모델로 응답을 생성하고, LlamaIndex를 사용하여 데이터를 벡터-DB나 메모리 저장 공간에 변환 및 저장하는 임베딩 모델로 사용할 것입니다.</p>
<p>여기서 API 키를 얻으세요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> getpass <span class="hljs-keyword">import</span> getpass
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">GOOGLE_API_KEY</span> = <span class="hljs-title function_">getpass</span>(<span class="hljs-string">"Google API 키를 입력하세요:"</span>)
</code></pre>
<h1>단계 3: 데이터 로드 및 문서 노드 생성</h1>
<p>LlamaIndex에서는 SimpleDirectoryLoader를 사용하여 데이터를 로드합니다. 먼저 폴더를 만들고 이 데이터 폴더에 데이터를 어떤 형식으로든 업로드해야 합니다. 저희 예시에서는 PDF 파일을 데이터 폴더에 업로드할 것입니다. 문서가 로드된 후, 문서를 더 작은 세그먼트로 분할하기 위해 노드로 파싱됩니다. 노드는 LlamaIndex 프레임워크 내에서 정의된 데이터 스키마입니다.</p>
<p>LlamaIndex의 최신 버전은 코드 구조를 업데이트했는데요, 이제 노드 파서, 임베딩 모델 및 Settings 내의 LLM에 대한 정의가 포함되어 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SimpleDirectoryReader</span>
<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Settings</span>
</code></pre>
<pre><code class="hljs language-js">documents = <span class="hljs-title class_">SimpleDirectoryReader</span>(<span class="hljs-string">'data'</span>).<span class="hljs-title function_">load_data</span>()
nodes = <span class="hljs-title class_">Settings</span>.<span class="hljs-property">node_parser</span>.<span class="hljs-title function_">get_nodes_from_documents</span>(documents)
</code></pre>
<h1>단계 4: 임베딩 모델 및 큰 언어 모델 설정하기</h1>
<p>젬니(Gemini)는 gemini-pro, gemini-1.0-pro, gemini-1.5, 비전 모델 등 다양한 모델을 지원합니다. 이 경우에는 기본 모델을 사용하고 Google API 키를 제공할 것입니다. Gemini의 임베딩 모델로는 현재 embedding-001을 사용하고 있습니다. 유효한 API 키가 추가되었는지 확인하세요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">embeddings</span>.<span class="hljs-property">gemini</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">GeminiEmbedding</span>
<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">llms</span>.<span class="hljs-property">gemini</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Gemini</span>
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-title class_">Settings</span>.<span class="hljs-property">embed_model</span> = <span class="hljs-title class_">GeminiEmbedding</span>(
    model_name=<span class="hljs-string">"models/embedding-001"</span>, api_key=<span class="hljs-variable constant_">GOOGLE_API_KEY</span>
)
<span class="hljs-title class_">Settings</span>.<span class="hljs-property">llm</span> = <span class="hljs-title class_">Gemini</span>(api_key=<span class="hljs-variable constant_">GOOGLE_API_KEY</span>)
</code></pre>
<h1>단계5: 저장 문맥 정의 및 데이터 저장</h1>
<p>데이터가 노드로 구문 분석되면 LlamaIndex는 저장 문맥을 제공하여 데이터의 벡터 임베딩을 저장하는 기본 문서 저장소를 제공합니다. 이 저장 문맥은 데이터를 메모리에 유지하여 나중에 색인화할 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">StorageContext</span>
</code></pre>
<pre><code class="hljs language-js">storage_context = <span class="hljs-title class_">StorageContext</span>.<span class="hljs-title function_">from_defaults</span>()
storage_context.<span class="hljs-property">docstore</span>.<span class="hljs-title function_">add_documents</span>(nodes)
</code></pre>
<p>인덱스-키워드 및 인덱스 생성</p>
<p><img src="/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_1.png" alt="이미지"></p>
<p>하이브리드 검색을 수행하는 사용자 지정 검색기를 구축하려면 두 가지 인덱스를 생성해야 합니다. 벡터 검색을 수행할 수 있는 첫 번째 벡터 인덱스와 키워드 검색을 수행할 수 있는 두 번째 키워드 인덱스입니다. 인덱스를 생성하려면 저장 컨텍스트와 노드 문서뿐만 아니라 임베딩 모델과 LLM의 기본 설정도 필요합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SimpleKeywordTableIndex</span>, <span class="hljs-title class_">VectorStoreIndex</span>
</code></pre>
<pre><code class="hljs language-js">vector_index = <span class="hljs-title class_">VectorStoreIndex</span>(nodes, storage_context=storage_context)
keyword_index = <span class="hljs-title class_">SimpleKeywordTableIndex</span>(nodes, storage_context=storage_context)
</code></pre>
<h1>단계6: 사용자 지정 검색기 만들기</h1>
<p>LlamaIndex를 사용하여 하이브리드 검색을 위한 사용자 지정 검색기를 만들기 위해 먼저 스키마를 정의해야 합니다. 이는 노드를 적절하게 구성함으로써 수행됩니다. 검색기에는 벡터 인덱스 검색기와 키워드 검색기 모두가 필요합니다. 이를 통해 하이브리드 검색을 수행하고 결과를 결합하여 혼돈을 최소화할 수 있습니다. 게다가 우리는 결과를 결합할 때 사용할 모드(AND 또는 OR)를 지정해야 합니다.</p>
<p>노드가 구성되면 각 노드 ID에 대해 번들을 조회하고 벡터 및 키워드 검색기를 사용합니다. 선택한 모드에 따라 사용자 지정 검색기를 정의하고 완성합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">QueryBundle</span>
<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span>.<span class="hljs-property">schema</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">NodeWithScore</span>
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span>.<span class="hljs-property">retrievers</span> <span class="hljs-keyword">import</span> (
    <span class="hljs-title class_">BaseRetriever</span>,
    <span class="hljs-title class_">VectorIndexRetriever</span>,
    <span class="hljs-title class_">KeywordTableSimpleRetriever</span>,
)
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-title class_">List</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomRetriever</span>(<span class="hljs-title class_">BaseRetriever</span>):
    def <span class="hljs-title function_">__init__</span>(
        self,
        <span class="hljs-attr">vector_retriever</span>: <span class="hljs-title class_">VectorIndexRetriever</span>,
        <span class="hljs-attr">keyword_retriever</span>: <span class="hljs-title class_">KeywordTableSimpleRetriever</span>,
        <span class="hljs-attr">mode</span>: str = <span class="hljs-string">"AND"</span>) -> <span class="hljs-title class_">None</span>:
       
        self.<span class="hljs-property">_vector_retriever</span> = vector_retriever
        self.<span class="hljs-property">_keyword_retriever</span> = keyword_retriever
        <span class="hljs-keyword">if</span> mode not <span class="hljs-keyword">in</span> (<span class="hljs-string">"AND"</span>, <span class="hljs-string">"OR"</span>):
            raise <span class="hljs-title class_">ValueError</span>(<span class="hljs-string">"Invalid mode."</span>)
        self.<span class="hljs-property">_mode</span> = mode
        <span class="hljs-variable language_">super</span>().<span class="hljs-title function_">__init__</span>()
    def <span class="hljs-title function_">_retrieve</span>(self, <span class="hljs-attr">query_bundle</span>: <span class="hljs-title class_">QueryBundle</span>) -> <span class="hljs-title class_">List</span>[<span class="hljs-title class_">NodeWithScore</span>]:
        vector_nodes = self.<span class="hljs-property">_vector_retriever</span>.<span class="hljs-title function_">retrieve</span>(query_bundle)
        keyword_nodes = self.<span class="hljs-property">_keyword_retriever</span>.<span class="hljs-title function_">retrieve</span>(query_bundle)
        vector_ids = {n.<span class="hljs-property">node</span>.<span class="hljs-property">node_id</span> <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> vector_nodes}
        keyword_ids = {n.<span class="hljs-property">node</span>.<span class="hljs-property">node_id</span> <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> keyword_nodes}
        combined_dict = {n.<span class="hljs-property">node</span>.<span class="hljs-property">node_id</span>: n <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> vector_nodes}
        combined_dict.<span class="hljs-title function_">update</span>({n.<span class="hljs-property">node</span>.<span class="hljs-property">node_id</span>: n <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> keyword_nodes})
        <span class="hljs-keyword">if</span> self.<span class="hljs-property">_mode</span> == <span class="hljs-string">"AND"</span>:
            retrieve_ids = vector_ids.<span class="hljs-title function_">intersection</span>(keyword_ids)
        <span class="hljs-attr">else</span>:
            retrieve_ids = vector_ids.<span class="hljs-title function_">union</span>(keyword_ids)
        retrieve_nodes = [combined_dict[r_id] <span class="hljs-keyword">for</span> r_id <span class="hljs-keyword">in</span> retrieve_ids]
        <span class="hljs-keyword">return</span> retrieve_nodes
</code></pre>
<h1>Step7: Define Retrievers</h1>
<p>이제 사용자 정의 검색기 클래스가 정의되었으므로, 검색기를 인스턴스화하고 쿼리 엔진을 합성해야 합니다. 응답 씨네사이저는 사용자 쿼리와 주어진 텍스트 청크 세트를 기반으로 LLM에서 응답을 생성하는 데 사용됩니다. 응답 씨네사이저에서 출력은 응답 객체이며, 이 객체는 사용자 정의 검색기를 하나의 매개 변수로 취합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> get_response_synthesizer
<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span>.<span class="hljs-property">query_engine</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">RetrieverQueryEngine</span>
</code></pre>
<pre><code class="hljs language-js">vector_retriever = <span class="hljs-title class_">VectorIndexRetriever</span>(index=vector_index, similarity_top_k=<span class="hljs-number">2</span>)
keyword_retriever = <span class="hljs-title class_">KeywordTableSimpleRetriever</span>(index=keyword_index)
# custom retriever => combine vector and keyword retriever
custom_retriever = <span class="hljs-title class_">CustomRetriever</span>(vector_retriever, keyword_retriever)
# define response synthesizer
response_synthesizer = <span class="hljs-title function_">get_response_synthesizer</span>()
custom_query_engine = <span class="hljs-title class_">RetrieverQueryEngine</span>(
    retriever=custom_retriever,
    response_synthesizer=response_synthesizer,
)
</code></pre>
<h1>Step8: Run Custom Retriever Query Engine</h1>
<p>마침내, 현저하게 환각을 줄이는 사용자 정의 검색기를 개발했습니다. 그 효과를 테스트하기 위해, 우리는 컨텍스트 내부와 외부에서 한 가지 프롬프트를 포함한 사용자 쿼리를 실행하고 생성된 답변을 평가했습니다.</p>
<pre><code class="hljs language-js">query = <span class="hljs-string">"데이터 컨텍스트에는 무엇이 포함되어 있나요?"</span>
<span class="hljs-title function_">print</span>(custom_query_engine.<span class="hljs-title function_">query</span>(query))
<span class="hljs-title function_">print</span>(custom_query_engine.<span class="hljs-title function_">query</span>(<span class="hljs-string">"과학이란 무엇인가요?"</span>)
</code></pre>
<p><img src="/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_2.png" alt="이미지"></p>
<h1>결론</h1>
<p>우리는 LlamaIndex를 사용하여 벡터와 키워드 검색기를 결합하여 Gemini LLM 및 임베딩의 지원을 받아 하이브리드 검색을 수행하는 맞춤형 리트리버를 성공적으로 구현했습니다. 이 접근은 전형적인 RAG 파이프라인에서 LLM 환각을 어느 정도 감소시킴으로써 효과적입니다.</p>
<h2>중요 사항</h2>
<ul>
<li>벡터 및 키워드 리트리버를 통합한 사용자 지정 리트리버 개발으로 RAG의 관련 문서 식별 능력과 정확성을 향상시킴.</li>
<li>LlamaIndex 설정을 사용하여 Gemini Embedding 및 LLM을 구현하였으며, 최신 버전에서는 이전에 사용되던 Service Context보다 나은 것으로 대체되었음.</li>
<li>사용자 지정 리트리버 구축 시 AND 또는 OR 연산을 사용할 것인지 결정하는 것이 중요하며, 특정 요구 사항에 따라 키워드 및 벡터 검색 결과의 교집합과 합집합을 균형 있게 조정해야 함.</li>
<li>사용자 지정 리트리버 설정은 RAG 파이프라인 내에서 하이브리드 검색 메커니즘을 사용하여 대형 언어 모델 응답에서 환각을 크게 줄여줌.</li>
</ul>
<h1>나에 대해</h1>
<p>LinkedIn 프로필이 이곳에 있어요. 연결하고 싶으시면 클릭해주세요. 제 글을 즐겁게 읽어주셨으면 좋겠어요. 마음에 드셨다면 친구들과 공유하고 저를 팔로우해주세요. 제 글 작성을 개선할 수 있는 생각이 있다면 자유롭게 의견을 남겨주세요.
제 이전 게시된 모든 글은 여기에서 읽을 수 있어요. [<a href="https://aivichar.com/" rel="nofollow" target="_blank">https://aivichar.com/</a>]</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기","description":"","date":"2024-06-19 19:24","slug":"2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini","content":"\n\n# 소개\n\n리트리버는 RAG(Retrieval Augmented Generation) 파이프라인의 가장 중요한 부분입니다. 이 문서에서는 LlamaIndex를 사용하여 키워드 및 벡터 검색 리트리버를 결합한 사용자 정의 리트리버를 구현할 것입니다. Gemini LLM을 사용한 다중 문서 채팅은 우리가 이 RAG 파이프라인을 구축할 프로젝트 사례입니다. 프로젝트를 시작하기 전에 우리는 이러한 애플리케이션을 구축하기 위해 서비스 및 스토리지 콘텍스트와 같은 몇 가지 중요한 구성 요소를 먼저 이해할 것입니다.\n\n## 학습 목표\n\n- RAG 파이프라인에 대한 통찰력을 얻고, 리트리버와 제너레이터 구성 요소의 역할을 이해하여 맥락적으로 응답을 생성하는 방법을 알아봅니다.\n- 키워드 및 벡터 검색 기술을 통합하여 검색 정확도를 향상시키는 사용자 정의 리트리버 개발법을 배웁니다.\n- LlamaIndex를 활용하여 데이터 삽입을 수행하고, LLM에 맥락을 제공하며, 사용자 지정 데이터와의 연결을 깊이 있게 이해하는 능력을 습득합니다.\n- LLM 응답에서 발생할 수 있는 환각 현상을 완화하는 데 사용자 정의 리트리버의 중요성을 이해합니다.\n- 문서의 관련성을 향상시키기 위해 다시 순위 지정 및 HyDE와 같은 고급 리트리버 구현을 탐색합니다.\n- Gemini LLM 및 LlamaIndex 내에서 임베딩을 통합하여 응답 생성 및 데이터 저장을 개선하여 RAG 기능을 향상합니다.\n- 맞춤형 리트리버 설정에 대한 의사 결정 능력을 발전시켜 검색 결과 최적화를 위해 AND 및 OR 연산 중 선택하는 방법을 배웁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# LlamaIndex이 무엇인가요?\n\n대규모 언어 모델 분야는 매일 크게 발전하고 있습니다. 많은 모델이 빠르게 출시되고 있기 때문에 이러한 모델을 사용자 정의 데이터와 통합할 필요성이 점점 커지고 있습니다. 이러한 통합은 기업, 기업, 그리고 최종 사용자에게 더 많은 유연성과 데이터와의 깊은 연결성을 제공합니다.\n\n초기에 GPT-index로 알려져 있었던 LlamaIndex는 LLM 애플리케이션을 위해 설계된 데이터 프레임워크입니다. ChatGPT와 같은 사용자 지정 데이터 기반 챗봇을 만드는 인기가 계속해서 증가함에 따라 LlamaIndex와 같은 프레임워크는 점점 가치가 있는 존재가 되고 있습니다. 핵심적으로, LlamaIndex는 다양한 데이터 커넥터를 제공하여 데이터 수집을 용이하게합니다. 이 글에서는 우리가 데이터를 LLM에 context로 전달하는 방법을 알아볼 것이며, 이 개념이 의미하는 것이 Retrieval Augmented Generation, 줄여서 RAG입니다.\n\n# RAG가 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG(축약어인 Retrieval Augmented Generation)에는 Retriever와 Generator 두 가지 주요 구성 요소가 있습니다.\n\n- Retriever는 벡터 데이터베이스일 수 있으며, 사용자 쿼리에 관련 문서를 검색한 후 이를 문맥으로 사용자에게 전달하는 역할을 합니다.\n- Generator 모델은 대형 언어 모델로, 검색된 문서와 프롬프트를 함께 사용하여 문맥으로부터 의미 있는 응답을 생성하는 역할을 합니다.\n\n이 방식으로 RAG는 Automated Few shot prompting을 통한 문맥 학습에 대한 최적의 솔루션입니다.\n\n# Retriever의 중요성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Retriever Component](/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png)\n\nRAG 파이프라인에서 Retriever 구성 요소의 중요성을 이해해봅시다.\n\n사용자 정의 검색기를 개발할 때, 우리의 Bed을 가장 잘 수행하는 검색기 유형을 결정하는 것이 중요합니다. 우리의 목적에 따라 우리는 Keyword Search와 Vector Search를 통합한 Hybrid Search를 구현할 것입니다.\n\nVector Search는 유사성 또는 의미 검색을 기반으로 사용자 쿼리에 대한 관련 문서를 식별하며, Keyword Search는 용어 발생 빈도에 기반하여 문서를 찾습니다. 이 통합은 LlamaIndex를 사용하여 두 가지 방법으로 달성할 수 있습니다. Hybrid Search의 사용자 정의 검색기를 구축할 때, 중요한 결정 사항은 AND 또는 OR 연산자 중 어느 것을 사용할지 선택하는 것입니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- AND 연산: 이 방법은 모든 지정된 용어를 포함하는 문서를 검색하여 더 제한적이지만 높은 관련성을 보장합니다. 이를 키워드 검색과 벡터 검색 결과 간의 교집합으로 생각할 수 있습니다.\n\n- OR 연산: 이 방법은 지정된 용어 중 어떤 것이라도 포함하는 문서를 검색하며 결과의 폭을 늘릴 수 있지만 관련성을 줄일 수 있습니다. 이를 키워드 검색과 벡터 검색 결과 간의 합집합으로 생각할 수 있습니다.\n\n# LLamaIndex를 사용한 사용자 지정 검색기 만들기\n\n이제 LLamaIndex를 사용하여 사용자 지정 검색기를 만들어 보겠습니다. 이를 위해 특정 단계를 따라야 합니다.\n\n# 단계 1: 설치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle Colab이나 Jupyter Notebook에서 코드 구현을 시작하려면 주로 필요한 라이브러리를 설치해야 합니다. 이 경우에는 사용자 지정 검색기 생성을 위해 LlamaIndex, 임베딩 모델 및 LLM 추론을 위한 Gemini, 데이터 커넥터로 PyPDF를 사용할 것입니다.\n\n```js\n!pip install llama-index\n!pip install llama-index-multi-modal-llms-gemini\n!pip install llama-index-embeddings-gemini\n```\n\n# 단계 2: Google API 키 설정\n\n이 프로젝트에서는 Google Gemini를 사용하여 대규모 언어 모델로 응답을 생성하고, LlamaIndex를 사용하여 데이터를 벡터-DB나 메모리 저장 공간에 변환 및 저장하는 임베딩 모델로 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 API 키를 얻으세요.\n\n```js\nfrom getpass import getpass\n```\n\n```js\nGOOGLE_API_KEY = getpass(\"Google API 키를 입력하세요:\")\n```\n\n# 단계 3: 데이터 로드 및 문서 노드 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLlamaIndex에서는 SimpleDirectoryLoader를 사용하여 데이터를 로드합니다. 먼저 폴더를 만들고 이 데이터 폴더에 데이터를 어떤 형식으로든 업로드해야 합니다. 저희 예시에서는 PDF 파일을 데이터 폴더에 업로드할 것입니다. 문서가 로드된 후, 문서를 더 작은 세그먼트로 분할하기 위해 노드로 파싱됩니다. 노드는 LlamaIndex 프레임워크 내에서 정의된 데이터 스키마입니다.\n\nLlamaIndex의 최신 버전은 코드 구조를 업데이트했는데요, 이제 노드 파서, 임베딩 모델 및 Settings 내의 LLM에 대한 정의가 포함되어 있습니다.\n\n```js\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core import Settings\n```\n\n```js\ndocuments = SimpleDirectoryReader('data').load_data()\nnodes = Settings.node_parser.get_nodes_from_documents(documents)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 단계 4: 임베딩 모델 및 큰 언어 모델 설정하기\n\n젬니(Gemini)는 gemini-pro, gemini-1.0-pro, gemini-1.5, 비전 모델 등 다양한 모델을 지원합니다. 이 경우에는 기본 모델을 사용하고 Google API 키를 제공할 것입니다. Gemini의 임베딩 모델로는 현재 embedding-001을 사용하고 있습니다. 유효한 API 키가 추가되었는지 확인하세요.\n\n```js\nfrom llama_index.embeddings.gemini import GeminiEmbedding\nfrom llama_index.llms.gemini import Gemini\n```\n\n```js\nSettings.embed_model = GeminiEmbedding(\n    model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY\n)\nSettings.llm = Gemini(api_key=GOOGLE_API_KEY)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 단계5: 저장 문맥 정의 및 데이터 저장\n\n데이터가 노드로 구문 분석되면 LlamaIndex는 저장 문맥을 제공하여 데이터의 벡터 임베딩을 저장하는 기본 문서 저장소를 제공합니다. 이 저장 문맥은 데이터를 메모리에 유지하여 나중에 색인화할 수 있습니다.\n\n```js\nfrom llama_index.core import StorageContext\n```\n\n```js\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인덱스-키워드 및 인덱스 생성\n\n![이미지](/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_1.png)\n\n하이브리드 검색을 수행하는 사용자 지정 검색기를 구축하려면 두 가지 인덱스를 생성해야 합니다. 벡터 검색을 수행할 수 있는 첫 번째 벡터 인덱스와 키워드 검색을 수행할 수 있는 두 번째 키워드 인덱스입니다. 인덱스를 생성하려면 저장 컨텍스트와 노드 문서뿐만 아니라 임베딩 모델과 LLM의 기본 설정도 필요합니다.\n\n```js\nfrom llama_index.core import SimpleKeywordTableIndex, VectorStoreIndex\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nvector_index = VectorStoreIndex(nodes, storage_context=storage_context)\nkeyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)\n```\n\n# 단계6: 사용자 지정 검색기 만들기\n\nLlamaIndex를 사용하여 하이브리드 검색을 위한 사용자 지정 검색기를 만들기 위해 먼저 스키마를 정의해야 합니다. 이는 노드를 적절하게 구성함으로써 수행됩니다. 검색기에는 벡터 인덱스 검색기와 키워드 검색기 모두가 필요합니다. 이를 통해 하이브리드 검색을 수행하고 결과를 결합하여 혼돈을 최소화할 수 있습니다. 게다가 우리는 결과를 결합할 때 사용할 모드(AND 또는 OR)를 지정해야 합니다.\n\n노드가 구성되면 각 노드 ID에 대해 번들을 조회하고 벡터 및 키워드 검색기를 사용합니다. 선택한 모드에 따라 사용자 지정 검색기를 정의하고 완성합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom llama_index.core import QueryBundle\nfrom llama_index.core.schema import NodeWithScore\n```\n\n```js\nfrom llama_index.core.retrievers import (\n    BaseRetriever,\n    VectorIndexRetriever,\n    KeywordTableSimpleRetriever,\n)\nfrom typing import List\n\nclass CustomRetriever(BaseRetriever):\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,\n        keyword_retriever: KeywordTableSimpleRetriever,\n        mode: str = \"AND\") -\u003e None:\n       \n        self._vector_retriever = vector_retriever\n        self._keyword_retriever = keyword_retriever\n        if mode not in (\"AND\", \"OR\"):\n            raise ValueError(\"Invalid mode.\")\n        self._mode = mode\n        super().__init__()\n    def _retrieve(self, query_bundle: QueryBundle) -\u003e List[NodeWithScore]:\n        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n        vector_ids = {n.node.node_id for n in vector_nodes}\n        keyword_ids = {n.node.node_id for n in keyword_nodes}\n        combined_dict = {n.node.node_id: n for n in vector_nodes}\n        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n        if self._mode == \"AND\":\n            retrieve_ids = vector_ids.intersection(keyword_ids)\n        else:\n            retrieve_ids = vector_ids.union(keyword_ids)\n        retrieve_nodes = [combined_dict[r_id] for r_id in retrieve_ids]\n        return retrieve_nodes\n```\n\n# Step7: Define Retrievers\n\n이제 사용자 정의 검색기 클래스가 정의되었으므로, 검색기를 인스턴스화하고 쿼리 엔진을 합성해야 합니다. 응답 씨네사이저는 사용자 쿼리와 주어진 텍스트 청크 세트를 기반으로 LLM에서 응답을 생성하는 데 사용됩니다. 응답 씨네사이저에서 출력은 응답 객체이며, 이 객체는 사용자 정의 검색기를 하나의 매개 변수로 취합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom llama_index.core import get_response_synthesizer\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n```\n\n```js\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=2)\nkeyword_retriever = KeywordTableSimpleRetriever(index=keyword_index)\n# custom retriever =\u003e combine vector and keyword retriever\ncustom_retriever = CustomRetriever(vector_retriever, keyword_retriever)\n# define response synthesizer\nresponse_synthesizer = get_response_synthesizer()\ncustom_query_engine = RetrieverQueryEngine(\n    retriever=custom_retriever,\n    response_synthesizer=response_synthesizer,\n)\n```\n\n# Step8: Run Custom Retriever Query Engine\n\n마침내, 현저하게 환각을 줄이는 사용자 정의 검색기를 개발했습니다. 그 효과를 테스트하기 위해, 우리는 컨텍스트 내부와 외부에서 한 가지 프롬프트를 포함한 사용자 쿼리를 실행하고 생성된 답변을 평가했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nquery = \"데이터 컨텍스트에는 무엇이 포함되어 있나요?\"\nprint(custom_query_engine.query(query))\nprint(custom_query_engine.query(\"과학이란 무엇인가요?\")\n```\n\n![이미지](/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_2.png)\n\n# 결론\n\n우리는 LlamaIndex를 사용하여 벡터와 키워드 검색기를 결합하여 Gemini LLM 및 임베딩의 지원을 받아 하이브리드 검색을 수행하는 맞춤형 리트리버를 성공적으로 구현했습니다. 이 접근은 전형적인 RAG 파이프라인에서 LLM 환각을 어느 정도 감소시킴으로써 효과적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 중요 사항\n\n- 벡터 및 키워드 리트리버를 통합한 사용자 지정 리트리버 개발으로 RAG의 관련 문서 식별 능력과 정확성을 향상시킴.\n- LlamaIndex 설정을 사용하여 Gemini Embedding 및 LLM을 구현하였으며, 최신 버전에서는 이전에 사용되던 Service Context보다 나은 것으로 대체되었음.\n- 사용자 지정 리트리버 구축 시 AND 또는 OR 연산을 사용할 것인지 결정하는 것이 중요하며, 특정 요구 사항에 따라 키워드 및 벡터 검색 결과의 교집합과 합집합을 균형 있게 조정해야 함.\n- 사용자 지정 리트리버 설정은 RAG 파이프라인 내에서 하이브리드 검색 메커니즘을 사용하여 대형 언어 모델 응답에서 환각을 크게 줄여줌.\n\n# 나에 대해\n\nLinkedIn 프로필이 이곳에 있어요. 연결하고 싶으시면 클릭해주세요. 제 글을 즐겁게 읽어주셨으면 좋겠어요. 마음에 드셨다면 친구들과 공유하고 저를 팔로우해주세요. 제 글 작성을 개선할 수 있는 생각이 있다면 자유롭게 의견을 남겨주세요.\n제 이전 게시된 모든 글은 여기에서 읽을 수 있어요. [https://aivichar.com/]","ogImage":{"url":"/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png"},"coverImage":"/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e리트리버는 RAG(Retrieval Augmented Generation) 파이프라인의 가장 중요한 부분입니다. 이 문서에서는 LlamaIndex를 사용하여 키워드 및 벡터 검색 리트리버를 결합한 사용자 정의 리트리버를 구현할 것입니다. Gemini LLM을 사용한 다중 문서 채팅은 우리가 이 RAG 파이프라인을 구축할 프로젝트 사례입니다. 프로젝트를 시작하기 전에 우리는 이러한 애플리케이션을 구축하기 위해 서비스 및 스토리지 콘텍스트와 같은 몇 가지 중요한 구성 요소를 먼저 이해할 것입니다.\u003c/p\u003e\n\u003ch2\u003e학습 목표\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRAG 파이프라인에 대한 통찰력을 얻고, 리트리버와 제너레이터 구성 요소의 역할을 이해하여 맥락적으로 응답을 생성하는 방법을 알아봅니다.\u003c/li\u003e\n\u003cli\u003e키워드 및 벡터 검색 기술을 통합하여 검색 정확도를 향상시키는 사용자 정의 리트리버 개발법을 배웁니다.\u003c/li\u003e\n\u003cli\u003eLlamaIndex를 활용하여 데이터 삽입을 수행하고, LLM에 맥락을 제공하며, 사용자 지정 데이터와의 연결을 깊이 있게 이해하는 능력을 습득합니다.\u003c/li\u003e\n\u003cli\u003eLLM 응답에서 발생할 수 있는 환각 현상을 완화하는 데 사용자 정의 리트리버의 중요성을 이해합니다.\u003c/li\u003e\n\u003cli\u003e문서의 관련성을 향상시키기 위해 다시 순위 지정 및 HyDE와 같은 고급 리트리버 구현을 탐색합니다.\u003c/li\u003e\n\u003cli\u003eGemini LLM 및 LlamaIndex 내에서 임베딩을 통합하여 응답 생성 및 데이터 저장을 개선하여 RAG 기능을 향상합니다.\u003c/li\u003e\n\u003cli\u003e맞춤형 리트리버 설정에 대한 의사 결정 능력을 발전시켜 검색 결과 최적화를 위해 AND 및 OR 연산 중 선택하는 방법을 배웁니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eLlamaIndex이 무엇인가요?\u003c/h1\u003e\n\u003cp\u003e대규모 언어 모델 분야는 매일 크게 발전하고 있습니다. 많은 모델이 빠르게 출시되고 있기 때문에 이러한 모델을 사용자 정의 데이터와 통합할 필요성이 점점 커지고 있습니다. 이러한 통합은 기업, 기업, 그리고 최종 사용자에게 더 많은 유연성과 데이터와의 깊은 연결성을 제공합니다.\u003c/p\u003e\n\u003cp\u003e초기에 GPT-index로 알려져 있었던 LlamaIndex는 LLM 애플리케이션을 위해 설계된 데이터 프레임워크입니다. ChatGPT와 같은 사용자 지정 데이터 기반 챗봇을 만드는 인기가 계속해서 증가함에 따라 LlamaIndex와 같은 프레임워크는 점점 가치가 있는 존재가 되고 있습니다. 핵심적으로, LlamaIndex는 다양한 데이터 커넥터를 제공하여 데이터 수집을 용이하게합니다. 이 글에서는 우리가 데이터를 LLM에 context로 전달하는 방법을 알아볼 것이며, 이 개념이 의미하는 것이 Retrieval Augmented Generation, 줄여서 RAG입니다.\u003c/p\u003e\n\u003ch1\u003eRAG가 무엇인가요?\u003c/h1\u003e\n\u003cp\u003eRAG(축약어인 Retrieval Augmented Generation)에는 Retriever와 Generator 두 가지 주요 구성 요소가 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRetriever는 벡터 데이터베이스일 수 있으며, 사용자 쿼리에 관련 문서를 검색한 후 이를 문맥으로 사용자에게 전달하는 역할을 합니다.\u003c/li\u003e\n\u003cli\u003eGenerator 모델은 대형 언어 모델로, 검색된 문서와 프롬프트를 함께 사용하여 문맥으로부터 의미 있는 응답을 생성하는 역할을 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 방식으로 RAG는 Automated Few shot prompting을 통한 문맥 학습에 대한 최적의 솔루션입니다.\u003c/p\u003e\n\u003ch1\u003eRetriever의 중요성\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png\" alt=\"Retriever Component\"\u003e\u003c/p\u003e\n\u003cp\u003eRAG 파이프라인에서 Retriever 구성 요소의 중요성을 이해해봅시다.\u003c/p\u003e\n\u003cp\u003e사용자 정의 검색기를 개발할 때, 우리의 Bed을 가장 잘 수행하는 검색기 유형을 결정하는 것이 중요합니다. 우리의 목적에 따라 우리는 Keyword Search와 Vector Search를 통합한 Hybrid Search를 구현할 것입니다.\u003c/p\u003e\n\u003cp\u003eVector Search는 유사성 또는 의미 검색을 기반으로 사용자 쿼리에 대한 관련 문서를 식별하며, Keyword Search는 용어 발생 빈도에 기반하여 문서를 찾습니다. 이 통합은 LlamaIndex를 사용하여 두 가지 방법으로 달성할 수 있습니다. Hybrid Search의 사용자 정의 검색기를 구축할 때, 중요한 결정 사항은 AND 또는 OR 연산자 중 어느 것을 사용할지 선택하는 것입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAND 연산: 이 방법은 모든 지정된 용어를 포함하는 문서를 검색하여 더 제한적이지만 높은 관련성을 보장합니다. 이를 키워드 검색과 벡터 검색 결과 간의 교집합으로 생각할 수 있습니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOR 연산: 이 방법은 지정된 용어 중 어떤 것이라도 포함하는 문서를 검색하며 결과의 폭을 늘릴 수 있지만 관련성을 줄일 수 있습니다. 이를 키워드 검색과 벡터 검색 결과 간의 합집합으로 생각할 수 있습니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eLLamaIndex를 사용한 사용자 지정 검색기 만들기\u003c/h1\u003e\n\u003cp\u003e이제 LLamaIndex를 사용하여 사용자 지정 검색기를 만들어 보겠습니다. 이를 위해 특정 단계를 따라야 합니다.\u003c/p\u003e\n\u003ch1\u003e단계 1: 설치\u003c/h1\u003e\n\u003cp\u003eGoogle Colab이나 Jupyter Notebook에서 코드 구현을 시작하려면 주로 필요한 라이브러리를 설치해야 합니다. 이 경우에는 사용자 지정 검색기 생성을 위해 LlamaIndex, 임베딩 모델 및 LLM 추론을 위한 Gemini, 데이터 커넥터로 PyPDF를 사용할 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!pip install llama-index\n!pip install llama-index-multi-modal-llms-gemini\n!pip install llama-index-embeddings-gemini\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e단계 2: Google API 키 설정\u003c/h1\u003e\n\u003cp\u003e이 프로젝트에서는 Google Gemini를 사용하여 대규모 언어 모델로 응답을 생성하고, LlamaIndex를 사용하여 데이터를 벡터-DB나 메모리 저장 공간에 변환 및 저장하는 임베딩 모델로 사용할 것입니다.\u003c/p\u003e\n\u003cp\u003e여기서 API 키를 얻으세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e getpass \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e getpass\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eGOOGLE_API_KEY\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003egetpass\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Google API 키를 입력하세요:\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e단계 3: 데이터 로드 및 문서 노드 생성\u003c/h1\u003e\n\u003cp\u003eLlamaIndex에서는 SimpleDirectoryLoader를 사용하여 데이터를 로드합니다. 먼저 폴더를 만들고 이 데이터 폴더에 데이터를 어떤 형식으로든 업로드해야 합니다. 저희 예시에서는 PDF 파일을 데이터 폴더에 업로드할 것입니다. 문서가 로드된 후, 문서를 더 작은 세그먼트로 분할하기 위해 노드로 파싱됩니다. 노드는 LlamaIndex 프레임워크 내에서 정의된 데이터 스키마입니다.\u003c/p\u003e\n\u003cp\u003eLlamaIndex의 최신 버전은 코드 구조를 업데이트했는데요, 이제 노드 파서, 임베딩 모델 및 Settings 내의 LLM에 대한 정의가 포함되어 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSimpleDirectoryReader\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSettings\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edocuments = \u003cspan class=\"hljs-title class_\"\u003eSimpleDirectoryReader\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'data'\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eload_data\u003c/span\u003e()\nnodes = \u003cspan class=\"hljs-title class_\"\u003eSettings\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003enode_parser\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eget_nodes_from_documents\u003c/span\u003e(documents)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e단계 4: 임베딩 모델 및 큰 언어 모델 설정하기\u003c/h1\u003e\n\u003cp\u003e젬니(Gemini)는 gemini-pro, gemini-1.0-pro, gemini-1.5, 비전 모델 등 다양한 모델을 지원합니다. 이 경우에는 기본 모델을 사용하고 Google API 키를 제공할 것입니다. Gemini의 임베딩 모델로는 현재 embedding-001을 사용하고 있습니다. 유효한 API 키가 추가되었는지 확인하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003eembeddings\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003egemini\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGeminiEmbedding\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ellms\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003egemini\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGemini\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eSettings\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eembed_model\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eGeminiEmbedding\u003c/span\u003e(\n    model_name=\u003cspan class=\"hljs-string\"\u003e\"models/embedding-001\"\u003c/span\u003e, api_key=\u003cspan class=\"hljs-variable constant_\"\u003eGOOGLE_API_KEY\u003c/span\u003e\n)\n\u003cspan class=\"hljs-title class_\"\u003eSettings\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ellm\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eGemini\u003c/span\u003e(api_key=\u003cspan class=\"hljs-variable constant_\"\u003eGOOGLE_API_KEY\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e단계5: 저장 문맥 정의 및 데이터 저장\u003c/h1\u003e\n\u003cp\u003e데이터가 노드로 구문 분석되면 LlamaIndex는 저장 문맥을 제공하여 데이터의 벡터 임베딩을 저장하는 기본 문서 저장소를 제공합니다. 이 저장 문맥은 데이터를 메모리에 유지하여 나중에 색인화할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eStorageContext\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003estorage_context = \u003cspan class=\"hljs-title class_\"\u003eStorageContext\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_defaults\u003c/span\u003e()\nstorage_context.\u003cspan class=\"hljs-property\"\u003edocstore\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eadd_documents\u003c/span\u003e(nodes)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e인덱스-키워드 및 인덱스 생성\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e하이브리드 검색을 수행하는 사용자 지정 검색기를 구축하려면 두 가지 인덱스를 생성해야 합니다. 벡터 검색을 수행할 수 있는 첫 번째 벡터 인덱스와 키워드 검색을 수행할 수 있는 두 번째 키워드 인덱스입니다. 인덱스를 생성하려면 저장 컨텍스트와 노드 문서뿐만 아니라 임베딩 모델과 LLM의 기본 설정도 필요합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSimpleKeywordTableIndex\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eVectorStoreIndex\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003evector_index = \u003cspan class=\"hljs-title class_\"\u003eVectorStoreIndex\u003c/span\u003e(nodes, storage_context=storage_context)\nkeyword_index = \u003cspan class=\"hljs-title class_\"\u003eSimpleKeywordTableIndex\u003c/span\u003e(nodes, storage_context=storage_context)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e단계6: 사용자 지정 검색기 만들기\u003c/h1\u003e\n\u003cp\u003eLlamaIndex를 사용하여 하이브리드 검색을 위한 사용자 지정 검색기를 만들기 위해 먼저 스키마를 정의해야 합니다. 이는 노드를 적절하게 구성함으로써 수행됩니다. 검색기에는 벡터 인덱스 검색기와 키워드 검색기 모두가 필요합니다. 이를 통해 하이브리드 검색을 수행하고 결과를 결합하여 혼돈을 최소화할 수 있습니다. 게다가 우리는 결과를 결합할 때 사용할 모드(AND 또는 OR)를 지정해야 합니다.\u003c/p\u003e\n\u003cp\u003e노드가 구성되면 각 노드 ID에 대해 번들을 조회하고 벡터 및 키워드 검색기를 사용합니다. 선택한 모드에 따라 사용자 지정 검색기를 정의하고 완성합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eQueryBundle\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eschema\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNodeWithScore\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eretrievers\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    \u003cspan class=\"hljs-title class_\"\u003eBaseRetriever\u003c/span\u003e,\n    \u003cspan class=\"hljs-title class_\"\u003eVectorIndexRetriever\u003c/span\u003e,\n    \u003cspan class=\"hljs-title class_\"\u003eKeywordTableSimpleRetriever\u003c/span\u003e,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e typing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCustomRetriever\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eBaseRetriever\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\n        self,\n        \u003cspan class=\"hljs-attr\"\u003evector_retriever\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eVectorIndexRetriever\u003c/span\u003e,\n        \u003cspan class=\"hljs-attr\"\u003ekeyword_retriever\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eKeywordTableSimpleRetriever\u003c/span\u003e,\n        \u003cspan class=\"hljs-attr\"\u003emode\u003c/span\u003e: str = \u003cspan class=\"hljs-string\"\u003e\"AND\"\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e:\n       \n        self.\u003cspan class=\"hljs-property\"\u003e_vector_retriever\u003c/span\u003e = vector_retriever\n        self.\u003cspan class=\"hljs-property\"\u003e_keyword_retriever\u003c/span\u003e = keyword_retriever\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e mode not \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e (\u003cspan class=\"hljs-string\"\u003e\"AND\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"OR\"\u003c/span\u003e):\n            raise \u003cspan class=\"hljs-title class_\"\u003eValueError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Invalid mode.\"\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003e_mode\u003c/span\u003e = mode\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n    def \u003cspan class=\"hljs-title function_\"\u003e_retrieve\u003c/span\u003e(self, \u003cspan class=\"hljs-attr\"\u003equery_bundle\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eQueryBundle\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eNodeWithScore\u003c/span\u003e]:\n        vector_nodes = self.\u003cspan class=\"hljs-property\"\u003e_vector_retriever\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eretrieve\u003c/span\u003e(query_bundle)\n        keyword_nodes = self.\u003cspan class=\"hljs-property\"\u003e_keyword_retriever\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eretrieve\u003c/span\u003e(query_bundle)\n        vector_ids = {n.\u003cspan class=\"hljs-property\"\u003enode\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003enode_id\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e n \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e vector_nodes}\n        keyword_ids = {n.\u003cspan class=\"hljs-property\"\u003enode\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003enode_id\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e n \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e keyword_nodes}\n        combined_dict = {n.\u003cspan class=\"hljs-property\"\u003enode\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003enode_id\u003c/span\u003e: n \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e n \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e vector_nodes}\n        combined_dict.\u003cspan class=\"hljs-title function_\"\u003eupdate\u003c/span\u003e({n.\u003cspan class=\"hljs-property\"\u003enode\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003enode_id\u003c/span\u003e: n \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e n \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e keyword_nodes})\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003e_mode\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"AND\"\u003c/span\u003e:\n            retrieve_ids = vector_ids.\u003cspan class=\"hljs-title function_\"\u003eintersection\u003c/span\u003e(keyword_ids)\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            retrieve_ids = vector_ids.\u003cspan class=\"hljs-title function_\"\u003eunion\u003c/span\u003e(keyword_ids)\n        retrieve_nodes = [combined_dict[r_id] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e r_id \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e retrieve_ids]\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e retrieve_nodes\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eStep7: Define Retrievers\u003c/h1\u003e\n\u003cp\u003e이제 사용자 정의 검색기 클래스가 정의되었으므로, 검색기를 인스턴스화하고 쿼리 엔진을 합성해야 합니다. 응답 씨네사이저는 사용자 쿼리와 주어진 텍스트 청크 세트를 기반으로 LLM에서 응답을 생성하는 데 사용됩니다. 응답 씨네사이저에서 출력은 응답 객체이며, 이 객체는 사용자 정의 검색기를 하나의 매개 변수로 취합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e get_response_synthesizer\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equery_engine\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRetrieverQueryEngine\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003evector_retriever = \u003cspan class=\"hljs-title class_\"\u003eVectorIndexRetriever\u003c/span\u003e(index=vector_index, similarity_top_k=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\nkeyword_retriever = \u003cspan class=\"hljs-title class_\"\u003eKeywordTableSimpleRetriever\u003c/span\u003e(index=keyword_index)\n# custom retriever =\u003e combine vector and keyword retriever\ncustom_retriever = \u003cspan class=\"hljs-title class_\"\u003eCustomRetriever\u003c/span\u003e(vector_retriever, keyword_retriever)\n# define response synthesizer\nresponse_synthesizer = \u003cspan class=\"hljs-title function_\"\u003eget_response_synthesizer\u003c/span\u003e()\ncustom_query_engine = \u003cspan class=\"hljs-title class_\"\u003eRetrieverQueryEngine\u003c/span\u003e(\n    retriever=custom_retriever,\n    response_synthesizer=response_synthesizer,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eStep8: Run Custom Retriever Query Engine\u003c/h1\u003e\n\u003cp\u003e마침내, 현저하게 환각을 줄이는 사용자 정의 검색기를 개발했습니다. 그 효과를 테스트하기 위해, 우리는 컨텍스트 내부와 외부에서 한 가지 프롬프트를 포함한 사용자 쿼리를 실행하고 생성된 답변을 평가했습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003equery = \u003cspan class=\"hljs-string\"\u003e\"데이터 컨텍스트에는 무엇이 포함되어 있나요?\"\u003c/span\u003e\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(custom_query_engine.\u003cspan class=\"hljs-title function_\"\u003equery\u003c/span\u003e(query))\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(custom_query_engine.\u003cspan class=\"hljs-title function_\"\u003equery\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"과학이란 무엇인가요?\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e우리는 LlamaIndex를 사용하여 벡터와 키워드 검색기를 결합하여 Gemini LLM 및 임베딩의 지원을 받아 하이브리드 검색을 수행하는 맞춤형 리트리버를 성공적으로 구현했습니다. 이 접근은 전형적인 RAG 파이프라인에서 LLM 환각을 어느 정도 감소시킴으로써 효과적입니다.\u003c/p\u003e\n\u003ch2\u003e중요 사항\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e벡터 및 키워드 리트리버를 통합한 사용자 지정 리트리버 개발으로 RAG의 관련 문서 식별 능력과 정확성을 향상시킴.\u003c/li\u003e\n\u003cli\u003eLlamaIndex 설정을 사용하여 Gemini Embedding 및 LLM을 구현하였으며, 최신 버전에서는 이전에 사용되던 Service Context보다 나은 것으로 대체되었음.\u003c/li\u003e\n\u003cli\u003e사용자 지정 리트리버 구축 시 AND 또는 OR 연산을 사용할 것인지 결정하는 것이 중요하며, 특정 요구 사항에 따라 키워드 및 벡터 검색 결과의 교집합과 합집합을 균형 있게 조정해야 함.\u003c/li\u003e\n\u003cli\u003e사용자 지정 리트리버 설정은 RAG 파이프라인 내에서 하이브리드 검색 메커니즘을 사용하여 대형 언어 모델 응답에서 환각을 크게 줄여줌.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e나에 대해\u003c/h1\u003e\n\u003cp\u003eLinkedIn 프로필이 이곳에 있어요. 연결하고 싶으시면 클릭해주세요. 제 글을 즐겁게 읽어주셨으면 좋겠어요. 마음에 드셨다면 친구들과 공유하고 저를 팔로우해주세요. 제 글 작성을 개선할 수 있는 생각이 있다면 자유롭게 의견을 남겨주세요.\n제 이전 게시된 모든 글은 여기에서 읽을 수 있어요. [\u003ca href=\"https://aivichar.com/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://aivichar.com/\u003c/a\u003e]\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>