<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>강화 학습 소개 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-AnIntroductiontoReinforcementLearning" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="강화 학습 소개 | itposting" data-gatsby-head="true"/><meta property="og:title" content="강화 학습 소개 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-AnIntroductiontoReinforcementLearning" data-gatsby-head="true"/><meta name="twitter:title" content="강화 학습 소개 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 06:29" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">강화 학습 소개</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="강화 학습 소개" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">28<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-AnIntroductiontoReinforcementLearning&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>강화 학습의 기초에 대한 심층 탐구, 모델 기반 및 모델 없는 방법 포함</h2>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png" alt="Image"></p>
<h2>강화 학습이란?</h2>
<p>공학 지능의 한 경로는 생물학적 생물체를 모방하는 것에 있습니다.</p>
<div class="content-ad"></div>
<p>생물학적 생명체들은 환경으로부터 정보를 전도하고, 이를 처리(인지과학이 연구하는 바)하며, 생존에 유리한 행동을 출력합니다. 이러한 행동들은 가장 기본적인 수준에서 음식 수확, 번식, 위험 회피와 관련됩니다. 또한, 이는 놀이, 창의성, 문제 해결, 설계 및 공학, 사교, 로맨스, 지성 생활과 같은 다양한 인간 활동도 포함합니다.</p>
<p>그렇다면, 위의 모든 것을 수행할 수 있는 시스템을 어떻게 설계할까요?</p>
<p>만약 우리가 간단한 생물체를 어떤 환경의 함수로 모델링한다면, 우리는 에이전트, 환경의 모델, 그 에이전트를 현재 상태에서 원하는 상태로 이동시키는 함수가 필요할 것입니다.</p>
<p>심리학에서, 두 가지 주요 학파인 행동주의와 인지과학은 인간 행동을 설명하기 위해 양립하고 있습니다. 행동주의자는 학습 메커니즘의 함수로써 행동을 이해하며, 학습은 행동적 출력에 귀속될 수 있다고 합니다. 반면에, 인지과학은 환경과의 상호작용을 정보 처리 접근법을 통해 모델링합니다. 이 접근법에서, 에이전트는 외부 자극을 처음에는 감각을 통해 내부 표현으로 변환하고, 그 후 사고와 추론 능력에 이르기까지 변형 및 통합 과정을 거쳐 행동적 출력을 반환합니다. 전자 접근법에서는, 학습은 주로 환경적 조건부로서 이해됩니다. 반면에 후자에서는, 정신적 표현이 행동을 예측하는 데 필수적이라고 여겨집니다. 강화 학습은 주로 행동주의 접근법에서 영향을 받아, 환경적 보상이 에이전트의 탐색 공간 내에서 진화를 결정한다고 합니다.</p>
<div class="content-ad"></div>
<p>작업자 조건화, 1950년대-60년대에 유행했던 행동주의 심리학 학파로서, 학습을 보상과 처벌이라는 환경 메커니즘의 결과물로 정의했습니다. 조작적 조건화의 전제 조건으로는 에드워드 손다이크가 제안한 효과의 법칙이 포함되어 있습니다. 이 법칙은 만족스러운 효과를 일으키는 행동은 재발을 더 많이 유발하며, 불만족스러운 효과를 일으키는 행동은 재발을 덜 유발한다는 것을 제안합니다. B.F. 스키너는 효과를 강화와 처벌의 용어로 운용했습니다. 강화는 행동의 재발 발생 가능성을 증가시키며, 이것은 접근 또는 억제 요인의 제거를 말할 수 있습니다. 접근은 긍정적 강화, 회피의 역전인 부정적 강화로 표현됩니다. 긍정적 강화의 예로는 스포츠에서 뛰어나고 자주 이기는 것이 포함됩니다. 부정적 강화의 예는 억제적 자극을 제거하는 것인데, 이를 예로 들 수 있는 것은 경기 도중 당신을 조롱하는 학교 폭력가입니다. 작업자 조건화는 가장 큰 보상을 받는 행동을 반복할 가능성이 높다고 예측합니다. 반면 처벌은 행동 효과를 제어하기 위해 부정적 결과를 추가하거나 행동과 관련된 보상을 제거함으로써 구성됩니다. 파울링으로 게임에서 퇴장당했을 때의 경우는 긍정적 처벌을 보여줍니다. 성적이 좋지 않고 게임에서 패배한 경우는 부정적 처벌을 나타내며, 이는 미래에 더 이상 게임을 하지 않을 수 있습니다.</p>
<p>인간 사회의 삶의 게임은 행동을 구성하는 보조적 강화나 사회적으로 구성된 보상과 처벌로 가득 차 있습니다. 이는 돈, 학점, 대학 입학 기준, 게임에서 이기고 지는 규칙과 같이 사회적 보상과 처벌을 포함합니다. 이러한 것들은 음식, 번식, 사회적 승인과 같은 생물학적 요구에 더 가까운 자연적 강화요인을 보완합니다.</p>
<p>기억은 이전 경험을 유지할 수 있도록 하는 학습에서 중요한 역할을 합니다. 증거에 따르면 기억은 경험의 콘텐츠보다는 보상과 처벌을 부호화합니다. 실험 대상은 보상을 받는 경험을 기억할 가능성이 더 높아지며, 따라서 이를 반복하는 경향이 있습니다. 부정적인 경험은 불리하게 기억될 가능성이 더 높아지며, 이를 피하려고 합니다. 기억 메커니즘은 복잡하고 다양하며, 실험 대상들이 기억을 회상함으로써 기억을 다시 구성함에 있어서 적극적인 역할을 하는 것으로 나타납니다. 이 사실은 행동주의에 대한 예측을 어렵게 만들며, 단독으로 조건화 원리에 근거한 예측을 하기 어렵게 만든다. 게다가 보상과 처벌은 긍정적과 부정적 영향의 풍경을 단순화하며, 이것은 복합한 골짜기와 웅덩이들, 중첩된 의존성으로 이루어진 복잡한 지형이며, 이는 이진 공간보다는 연속적 스펙트럼으로 더 잘 모델링됩니다.</p>
<p>불구하고, 강화 학습은 인공지능을 모델링하기 위해 에이전트, 환경 및 보상의 행동 온톨로지를 적응하는 다양한 수학적 기법으로 이루어져 있습니다. 아래에서 보게 되겠지만, 강화 학습의 측면 중 일부는 통제 이론에서 비롯되어 물리학과 공학으로 확장되는 전제 조건으로부터 나오며, 다른 측면은 심리학과 생물학으로부터 직접적으로 나오는 것입니다. 통제 이론의 대상과 생명체는 열역학적 균형으로부터 멀리 떨어진 최적 범위 내에 남아야 하는 동력학 시스템으로 구성되기 때문에, 기본 원리는 강화 학습과 인공지능의 목표에 부합됩니다.</p>
<div class="content-ad"></div>
<p>다이내믹 프로그래밍은 주로 제어 이론에서 시작되어, 더 큰 문제를 하위 문제로 재귀적으로 분해하여 해결하는 수학적 최적화 방법으로 발전했습니다. 보통 재귀는 함수가 직접 또는 간접적으로 자기 자신을 파라미터로 전달하는 것을 말합니다.</p>
<p>본 글에서는 주로 동적 프로그래밍의 요소에 초점을 맞추며, 이를 이산적이고 유한한 게임에 초점을 맞출 것입니다. 그러나 동적 프로그래밍은 모델 없이 강화학습 접근 방식과 결합하여 해결되는 한계를 가지고 있으며, 이를 보완하기 위해 동적 프로그래밍과 인공 신경망을 결합한 방법이 있습니다. 이는 한 때 신경동적 프로그래밍이라 불렸습니다. 보다 넓게는 강화학습과 인공 신경망의 결합을 딥 강화학습이라고 합니다. 이러한 모델은 강화학습 기법 내에서 딥 러닝의 강점을 통합하고 있습니다. 이러한 알고리즘 가운데 가장 인기 있는 것은 2013년 DeepMind에 의해 소개된 딥 Q-네트워크(DQN)입니다. 이 알고리즘 계열은 Q 함수를 근사화하기 위해 딥 러닝을 활용합니다. Q 함수의 근사화가 강화학습의 한 약점 중 하나이므로, 이러한 알고리즘들은 강화학습 패러다임의 주요 개선점을 제공합니다.</p>
<p>DQN이 해결한 다른 약점에는 비선형 동역학을 캡처하는 유연성 부여, 차원의 저주로 인해 계산적으로 처리하기 어려워지는 일반적 범위의 차원을 수용하는 능력, 그리고 환경에 대한 보다 큰 일반화 능력이 포함됩니다.</p>
<p>신경동적 프로그래밍은 순수히 행동주의 접근 방식의 약점을 해결하기 위해 심리학의 인지 패러다임을 활용하는 방향으로 발전하고 있습니다. 그러나 하위 수준 지각 정보의 계층적 구조와 처리에 대한 과학적 진전이 이루어지는 반면, 그 정보를 생각과 의식에 연결시키는 데는 더 많은 노력이 필요하며, 이는 과학적으로 약간 불가능한 것으로 남아 있습니다. 이러한 이유로 인공 신경망(ANNs)은 아직까지 사람의 지능의 복잡한 일반화 능력을 갖추고 있지 않습니다. 이는 ANNs보다 지수적으로 적은 샘플로 학습하는 인간의 지능과 대조적입니다. 본 글의 마지막 섹션에서 강화학습의 원칙을 인공 일반 지능(AGI) 쪽으로 채택하는 시사점을 논의하겠습니다.</p>
<div class="content-ad"></div>
<h2>의사 결정 이론 및 제어 이론</h2>
<p>동적 프로그래밍과 강화 학습의 수학적 요소에 깊이 파고들기 전에, 철학적이고 수학적인 의사 결정 이론과 강화 학습 간의 관계를 명확히해야 합니다. 의사 결정 이론은 주로 합리적 선택 이론의 수학적 형식화로 구성되어 있지만, 강화 학습의 목표와 겹치는 부분이 있습니다. 강화 학습은 복잡한 환경과 정보 환경과 상호작용할 수 있는 성공적인 인공 에이전트로의 모델을 구축하려고 합니다.</p>
<p>의사 결정 이론, 또는 선택 이론으로도 알려진 이론은 20세기에 이상적인 이유의 형식화가 짙어진 가운데 발전하였습니다. 구체적으로, 에이전트의 행위 확률을 그들의 선호도를 고려하여 양적화하기 위해 확률 이론을 사용합니다. 이 형식화 노력의 꼭대기는 폰 노이만-모건슈턴 유틸리티 절차였습니다. 요약하자면, 이 절차는 에이전트가 유틸리티 기대치에 따라 최대 이익을 가져다주는 행동을 선택하는 경향이 있음을 설명합니다.</p>
<p>제어 이론은 기계 및 전기 공학 분야에서 나타나며, 동적 시스템의 상태 및 성능을 원하는 매개변수에 대해 최적화하는 데 관심이 있습니다. 중요한 메커니즘은 희망 변수를 측정하고 설정점과 비교한 후 그 차이를 수정을 위한 피드백으로 전달하는 컨트롤러로 이루어져 있습니다. 제어 이론의 큰 그림은 생명체의 대사 과정과 유사하며, 외부 변수 조건에 대비해 내부 온도의 설정 점을 유지하는 생물들의 과정을 반영합니다. 제어 이론과 의사 결정 이론의 연결은 명백합니다: 둘 다 시스템의 상태를 최적화하거나 발전시키기 위해 환경으로부터의 피드백에 의존합니다.</p>
<div class="content-ad"></div>
<p>수학적으로, 제어 및 의사 결정 문제의 부분 집합은 모두 동적 프로그래밍을 통해 해결할 수 있는 최적화 문제로 축소될 수 있습니다. 동적 프로그래밍은 상태 변수의 수가 지수적으로 증가함에 따라 계산 요구 사항이 지수적으로 증가하는 차원의 저주에 시달린 일반적인 확률적 최적 제어 문제를 해결하기 위해 그것을 더 작은 하위 문제로 분해하고 가치 함수를 계산함으로써 해결합니다. 저희는 강화 학습의 기본 원칙을 시연하면서, 동적 프로그래밍의 핵심인 에이전트의 상태 및 가치 함수 사이의 재귀적 관계에 대해 깊이 파헤쳐볼 것입니다.</p>
<p>강화 학습과 의사 결정 이론은 보상 또는 유틸리티를 극대화하기 위한 절차를 정의하는 부분에서 겹칩니다. 그러나 의사 결정 이론에서는 유틸리티가 명시적으로 정의되지만, 경제 행동을 모델링하려는 것인 반면, 강화 학습에서는 유틸리티가 누적 보상으로 대체됩니다. 서로 다른 작업 목표에 대한 다른 정책을 적용하여 누적 보상을 극대화할 수 있으며, 탐구와 개발의 극성 방향 간의 상호 관계에 따라 달라집니다. 우리가 볼 것처럼, 탐색과 개발의 상호 관계를 탐색하는 것으로 표현되는 탐사-개발 딜레마에 따라 누적 보상을 극대화하는 것이 달라집니다.</p>
<p>강화 모델의 기반이 되는 온톨로지를 개요화하는 것으로 시작해 봅시다.</p>
<h2>상태, 동작 및 보상</h2>
<div class="content-ad"></div>
<p>강화 학습은 의사 결정 이론의 이론적 장치를 활용하여 에이전트, 환경 및 동적 진화 규칙을 포함하는 모델을 구성합니다. 진화 규칙은 에이전트가 환경 내에서 보상을 추구할 수 있게 허용하며, 이를 관찰이라고도 합니다.</p>
<p>에이전트는 환경으로부터 결정까지의 출력으로 정의됩니다. 특정 결정을 행동이라고 합니다. 네트워크의 현재 상태에서 행동으로의 매핑을 정책이라고 합니다. 정책은 상태에서 결과로의 매핑으로서 행동을 안내합니다.</p>
<p>따라서 형식적으로 정책은 상태를 행동으로 매핑하는 함수입니다. 현재 상태가 주어졌을 때 행동의 조건부 확률로 나타낼 수 있으며, 여기서 그리스 문자 𝛑은 정책을 나타냅니다:</p>
<p><img src="https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_1.png" alt="정책"></p>
<div class="content-ad"></div>
<p>전이 역학은 모든 가능한 상태 및 보상 값에 대한 확률 분포로 주어진 입력 보상에 따라 다음 상태를 정의합니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_2.png" alt="이미지"></p>
<p>위의 공식은 다음 상태와 보상 쌍의 확률을 현재 상태 s와 행동 a가 주어졌을 때 다음 상태 s'와 보상 r의 조건부 확률과 같다고 정의합니다.</p>
<p>행동은 보상을 누적하여 환경을 변경합니다. 그 결과로 보상은 에이전트 상태나 관측을 변경합니다. 보상 입력은 정책에 기반하여 미래의 행동 출력을 결정합니다.</p>
<div class="content-ad"></div>
<p>일반적으로, 정책에는 두 가지 유형이 있습니다:</p>
<p>보상은 일반적으로 스칼라 값 x로 형식화됩니다.</p>
<p>특정 보상이 주어지면, 에이전트는 최적화 딜레마에 직면합니다: 에이전트는 단기 보상을 극대화해야 하는지, 아니면 완전한 생생력 기록을 통해 누적 보상을 극대화해야 하는지를 결정해야 합니다.</p>
<p>이것은 탐색-이용 딜레마로 알려져 있습니다. 다시 말해, 전이 함수는 환경을 탐색하고 누적한 지식을 활용하여 최대 보상을 얻으며, 그 둘 사이의 균형을 최적화해야 합니다.</p>
<div class="content-ad"></div>
<p>탐색-활용 딜레마에 대한 최적의 해결책은 모델이 학습해야 하는 작업의 유형에 따라 달라집니다. 이 작업은 유한에서 무한(연속적 또는 이산적으로)로 범위가 있을 수 있습니다. 예를 들어 체스 게임은 에피소드 작업으로 형식화될 수 있습니다. 왜냐하면 유한한 구성 공간과 승, 패, 무승부 세 가지 가능한 결과를 가진 미리 정의된 종료 상태가 있기 때문입니다. 이는 현재 상태를 기준으로 최적의 후속 상태를 결정할 수 있는 것을 의미하며, 결정론적 전이 동역학을 통해 계산됩니다. 따라서 각 상태에 대해 단일 최적의 행동이 존재합니다.</p>
<p>그러나 대부분의 작업은 유한한 구성 공간이나 미리 정의된 종료 상태를 가지고 있지 않습니다. 우리는 이러한 것들을 연속적인 작업으로 분류하고, 모델이 없는 방법을 통해 최적화합니다. 모델 없는 방법론에서는 전환 동역학을 계산하는 대신 모델이 환경에서 샘플링하여 최적의 후속 상태를 계산합니다. 다르게 말하면, 선견지명을 통해 행동을 계획하는 대신 환경에 대해 배우기 위해 시행착오를 사용합니다.</p>
<p>모델 없이 강화 학습하는 두 가지 접근법이 일반적으로 있습니다: 몬테 카를로 접근법과 시간차 학습. 충분한 샘플의 평균이 기대값으로 수렴하기 때문에, 모델 없는 방법은 샘플 평균을 통해 예상값을 추정합니다. 몬테 카를로 방법은 충분히 큰 상태-행동 쌍의 샘플로 예상 누적 반환을 추정하여 가치 함수를 계산합니다. 일부 몬테 카를로 방법은 에피소드 작업의 끝에서만 가치 함수를 평가합니다. 연속적인 작업에서는 에피소드의 정의가 다양하게 변할 수 있고, 디자이너에 따라 시간 간격에 따라 설정할 수 있습니다.</p>
<p>몬테 카를로 탐색과 반대로 시간차 학습은 시간 단계 간의 차이를 활용하여 가치 함수를 증분적으로 추정합니다. 시간차 방법의 접근 방식을 두면 몬테 카를로 방법에 비해 실제 예상 값과의 분산을 낮추는 특성이 있습니다.</p>
<div class="content-ad"></div>
<p>요약하자면: 에이전트는 현재 상태와 액션 공간 쌍에서 상태 공간으로의 매핑을 통해 환경을 탐색합니다. 전이 동적은 미리 정의된 종단 상태를 가진 유한한 구성 공간에 대한 모든 가능한 매핑을 계산합니다. 미리 정의된 종단 상태와 유한한 상태 공간 대신에, 모델 무작위 접근법은 최상의 정책을 찾기 위해 환경에서 계속 샘플링합니다.</p>
<p>동적 프로그래밍은 모든 상태-액션 쌍에서 상태 전이 확률과 예상 보상을 계산합니다. 이 프로세스가 어떻게 작동하는지 이해하기 위해서는, 마르코프 프로세스를 이해해야 합니다.</p>
<p>다음에는 에이전트가 최적 후속 상태를 계산할 수 있도록 하는 수학적 모델을 배우게 됩니다. 앞서 논의한 대로, 최적성은 탐사-이용 딜레마로 이어지며, 이는 모델링하려는 작업 유형에 따라 다릅니다. 보상 구조를 자세히 살펴봄으로써 이를 더 잘 이해할 수 있을 겁니다.</p>
<h2>보상 평가</h2>
<div class="content-ad"></div>
<p>강화 학습에서 보상을 측정하는 방법은 에이전트가 행동을 취함으로써 환경으로부터 얻는 스칼라 값으로 계량화됩니다. 이 보상의 가치는 행동의 즉각적인 선호도를 나타냅니다.</p>
<p>반면에 누적 보상 또는 반환은 해당 시점까지 환경으로부터 누적된 모든 보상의 합을 나타냅니다. 에이전트의 목표는 단순히 즉각적 보상을 최적화하는 것이 아니라 누적 보상을 최적화하는 것입니다. 전자는 근시적 에이전트를 나타내며, 후자는 장기간 수익을 극대화하려는 장기 노력 에이전트를 나타냅니다.</p>
<p>대부분의 경우 에이전트가 가장 높은 보상을 최대한 빨리 극대화하길 원하기 때문에 할인은 현재 최대 보상을 나중에 최대 보상보다 우선시하는 방식으로 도입됩니다.</p>
<p>할인을 적용한 누적 보상 G는 아래 식으로 표현됩니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_3.png" alt="Reinforcement Learning Introduction"></p>
<p>여기서 누적 보상 G는 보상과 해당 할인 계수 감마 𝜸의 곱의 합과 같습니다. 감마는 항상 0과 1 사이의 값인 '0,1'입니다. 감마는 각 시간 단계마다 지수적으로 증가되므로 무한한 시간 단계를 통해 감마가 0에 접근합니다.</p>
<p>감마가 0에 접근할수록 단기 이익을 장려하고, 감마가 1에 가까워지면 무한한 반복을 통해 보상 합이 자체적으로 무한에 접근하므로 장기 이익을 장려합니다.</p>
<p>대부분의 작업은 시간에 제한이 있기 때문에, 감마 할인은 값이 1 미만일 때 보상에 상한선을 부과합니다.</p>
<div class="content-ad"></div>
<p>할인을 고려한 누적 보상의 압축된 방정식은 아래와 같습니다. 여기서 G는 보상 R의 예상 합을 나타내며, 이는 할인 요소 감마로 곱해집니다. 따라서 누적 보상은 보상과 할인 요소의 합으로 계산됩니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_4.png" alt="cumulative reward equation"></p>
<h2>마르코프 의사결정 과정 (MDP)</h2>
<p>지금까지 정책을 상태에서 행동으로 매핑하는 확률적 정의, 보상이 주어졌을 때 한 상태에서 다른 상태로 움직일 확률인 전이 역학, 그리고 보상이 어떻게 계산되는지에 대한 공식에 대해 논의해 왔습니다.</p>
<div class="content-ad"></div>
<p>자, 이제 조금 물러나서 확률적 전이 체인을 정의하는 보충 이론을 제공하겠습니다. 먼저 마르코프 과정이라고 하는 것부터 시작해봅시다. 마르코프 과정은 마르코프 성질을 만족하는 확률 과정입니다. 확률 과정은 무작위로 변하는 과정입니다. 마르코프 성질은 모든 상태에 대해 후속 상태가 현재 상태에만 의존된다는 것을 말합니다.</p>
<p>과거 상태가 미래 상태에 영향을 미치지 않기 때문에 마르코프 성질을 만족하는 과정을 메모리리스라고 합니다. 집을 나가서 일하러 나가 다시 집으로 돌아오는 매일 재발되는 일정된 목적지 집합을 상상해보세요. 즉, 시작과 끝이 있는 순환 과정이 있습니다. 이제 더 나아가서 다음 목적지로 움직일 결정이 현재 목적지에만 의존한다고 상상해보세요. 처음에는 각 연결된 목적지가 동일한 확률 분포를 갖게 될 것입니다. 예를 들어, 집을 나가면 운전하거나 지하철을 탈 수 있는 선택지가 있다면, 두 가능한 미래 상태에 대한 초기 확률을 각각 0.5로 정할 수 있습니다. 모든 가능한 경로의 반복을 통해 이러한 확률은 어떤 경로가 다른 경로보다 선호되는 빈도 분포로 안정화될 수 있습니다. (이 유형의 확률을 경험적 확률이라고 부르며, 가능한 사건에 대한 결과를 한정된 테스트 수에 대해 평균화합니다) 그 분포 평형은 마르코프 체인 또는 과정이 될 것입니다.</p>
<p>이제 아마도 생각 중일 것입니다: 어떻게 사건과 상태를 정의하나요? 고정된 가능한 상태와 안정한 확률 분포에 대해 얘기하려면 세상이 너무 복잡하지 않나요?</p>
<p>매우 그렇습니다. 그러나 우리는 환경 속 요소들의 수학적 형식론을 원하기 때문에 모델링하려는 작업 또는 환경 유형을 구별해야 합니다. 이를 위해 시간 단계와 상태 공간의 표현, 즉 모든 가능한 상태의 분포를 명시해야 합니다. 아래의 정사각 행렬은 상태 공간과 시간의 축을 기준으로 마르코프 체인의 정의를 제공합니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_5.png" alt="image"></p>
<p>상태 공간은 셀 수 있는/유한한 상태거나 연속적일 수 있습니다. 유한 상태 공간은 시스템의 모든 가능한 구성을 조합 이론을 통해 설명하고, 연속 상태 공간은 연속 함수를 통해 모든 가능한 구성을 설명합니다.</p>
<p>유한 및 가산 무한 공간은 측정 가능한 공간으로 정수 또는 유리수를 취하며, 연속 공간은 실수를 취합니다.</p>
<p>마찬가지로 시간 축은 이산 또는 연속적으로 정의될 수 있습니다.</p>
<div class="content-ad"></div>
<p>이산시간 프로세스는 이산적으로 상태 전이를 계산하지만, 가산 또는 비가산 상태 공간에서 모델링할 수 있습니다. 여기서 비가산이라 함은 실수의 무한한 10진 확장을 의미합니다. 실제로 컴퓨터가 시간을 세는 방식도 이와 같습니다. 이를 이산 단계로 처리합니다. 단계 사이의 간격은 아키텍처에 따라 다르지만, 주기는 보통 레지스터 상태를 변경하는 데 필요한 시간 단계의 길이로 측정됩니다.</p>
<p>연속시간 체인은 연속으로 상태 전이를 계산하며, 가산 또는 비가산 상태 공간에 모델링될 수 있습니다.</p>
<p>마르코프 프로세스라는 용어는 일반적으로 연속시간 프로세스에 사용되며, 마르코프 체인이라는 용어는 이 중 일부인 것을 나타냅니다: 이산시간, 확률적 제어 프로세스입니다. 이 기사에서는 이산시간, 유한 상태 공간에 초점을 맞출 것입니다.</p>
<p>지금까지 우리의 마르코프 체인은 상태간 전이를 고정된 확률로 설명하는 매우 단순한 모델입니다. 행동과 보상이라는 모델링에 중요한 두 가지 요소가 빠져 있습니다.</p>
<div class="content-ad"></div>
<p>보상을 전이 확률로 할당하는 것이 마르코프 보상 과정입니다. 마르코프 보상 과정은 각 전이 상태에 보상을 할당합니다(양수 또는 음수 정수로 정의됨)으로써 시스템을 원하는 상태로 이끕니다. 누적 보상 공식을 상기해 보겠습니다. 기대 보상의 합에 일정한 할인 계수가 곱해진 값입니다. 마르코프 보상 과정을 사용하면 초기 상태 S가 주어졌을 때 상태 v(s)의 값과 누적 보상 G의 확률을 계산할 수 있습니다(여기서 G는 많은 반복 시행에서 평균화된 값입니다):</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_6.png" alt="이미지"></p>
<p>마르코프 결정 과정으로 나아가기 위해 필요한 마지막 변수는 행동입니다. 에이전트는 가능한 행동 집합에 대해 동등하게 분포된 확률로 시작하고 이후에 전이 함수를 업데이트하여 현재 상태와 행동을 다음 상태와 보상으로 매핑합니다. 이렇게 하면 앞서 설명한 전이 동학에 다시 도달하게 됩니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_7.png" alt="이미지"></p>
<div class="content-ad"></div>
<h2>동적 계획법 및 벨만 최적성</h2>
<p>이것은 벨만(1957)에 의해 개발된 동적 프로그래밍의 개념으로 이어집니다.</p>
<p>동적 프로그래밍을 이해하면, 동적 프로그래밍과 같은 완벽한 환경 지식이 필요하지 않는 근사 방법인 몬테카를로 탐색 및 시간차이 메소드도 이해할 수 있습니다. 이러한 모델-프리 방법은 완벽한 정보 대신 동적 프로그래밍의 결정적 정책을 근사화합니다. 따라서, 실제 세계 학습을 근사화하는 강력한 메커니즘이 제공됩니다.</p>
<p>동적 프로그래밍이 최적의 에이전트 상태를 검색하고 찾는 핵심 아이디어는 상태 가치 함수와 행동 가치 함수 사이의 관계에 있습니다. 이들은 재귀적으로 관련되어 있습니다.</p>
<div class="content-ad"></div>
<p>이러한 아이디어를 관련성 있는 예시로 확장해 봅시다. 예를 들어, 당신이 삶에서 최적 상태가 아니고 이를 바꾸고 싶다고 가정해 봅시다. 미래에 이루고 싶은 구체적인 목표나 위치가 있다고 해 봅시다. 이 큰 목표에 도달하기 위해 (더 좋은 직장을 얻는다던가, 가족을 꾸린다던가 등을 대체할 수 있습니다), 당신은 원하는 결과에 도움이 되는 일련의 작은 단계나 행동을 취해야 할 것입니다. 강화 학습의 언어로 번역하면, 현재 상태에는 특정 가치가 할당될 것입니다. 현재 상태와 가치를 고려하여 당신은 행동을 취할 것입니다. 이러한 행동은 전체 목표와 현재 상태에 따라 평가될 것입니다. 좋은 행동은 나쁜 행동보다 높은 가치를 받을 것입니다. 환경으로부터의 피드백은 행동의 가치를 결정할 것입니다 (이 값들이 어떻게 결정되는지는 작업마다 다릅니다). 상태의 평가는 사용 가능한 행동과 후속 상태의 가치에 영향을 미칠 것입니다. 그리고 행동의 평가는 현재 상태의 가치를 재귀적으로 영향을 줄 것입니다. 다시 말해, 행동과 상태는 재귀적으로 연결되어 있습니다.</p>
<p>이제 현실에서, 당신의 목표와 그 목표에 이르는 행동 단계들은 이산 시간 단계 및 이산 상태 공간을 갖는 결정론적 시스템으로 명시할 수 없습니다 (비록 이 방식으로 근사화할 수도 있습니다). 대신, 동적 프로그래밍은 체스와 같은 게임처럼 정의 가능한 환경을 가정합니다. 여기서 시간 단계와 행동 공간이 이산적이고 유한하게 추상화됩니다. 현실과의 중요한 점은 더 큰 목표가 해당 큰 목표에 유리한 작은 부목표를 최적화함으로써 다가올 것이라는 점입니다.</p>
<p>따라서 동적 프로그래밍은 다음 값들을 가정할 것입니다: (Ω, A, 𝒫), 여기서 Ω는 모든 가능한 상태의 합을 나타냅니다, A는 유한 샘플 공간의 부분집합인 행동 이벤트를 나타내며, P는 일정 정책 함수 𝝅에 의해 각 행동 이벤트에 할당된 확률을 나타냅니다.</p>
<p>이제 우리가 결정론적 전이 역학에 대해 생각해 보면, 상태, 행동 및 보상의 집합이 유한하기 때문에, 특정 상태와 보상 쌍은 일정한 상태 및 행동 쌍이 주어졌을 때 그 값들이 발생할 확률을 갖게 될 것입니다. 이러한 확률은 상태 공간이 이산적이기 때문에 이산 확률 분포로 명시됩니다. 우리는 상태, 행동 및 보상으로 구성된 일련의 순서가 누적 보상을 최대화하려는 마르코프 결정 과정(MDPs)이라고 했습니다. 이때 보상을 스칼라 값으로 표현하며, 시간이 흐름에 따라 예상되는 누적 보상을 최대화하려고 합니다.</p>
<div class="content-ad"></div>
<p>이제 다루어야 할 질문은 우리가 지정한 가정에 따라 마르코프 의사결정 프로세스가 누적 보상을 최대화하는 방법입니다. 이 답은 벨만 최적 방정식에 의해 제공되며 두 함수인 상태 가치 함수와 행동 가치 함수 사이의 관계를 설명합니다.</p>
<h2>상태 가치 함수</h2>
<p>상태 가치 함수는 에이전트가 정책 𝝅에 따라 취할 수 있는 모든 가능한 조치의 확률의 합으로 정의될 수 있습니다. 각 조치에 대해 가능한 후속 상태의 가중치 값의 합으로 그 가치가 결정됩니다.</p>
<p>보다 간단하게 말하자면, 상태 가치 함수는 특정 상태(s)에서 정책 𝝅을 따라 시작하여 에이전트가 얻을 수 있는 예상 누적 보상을 정의합니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_8.png" alt="equation_8"></p>
<p>위의 방정식은 두 항으로 구성되어 있습니다: a) 정책 (𝝅)을 따라 상태 (s)에서 에이전트가 취할 수 있는 모든 가능한 조치들의 확률의 합, 그리고 b) 각 가능한 조치마다 가능한 후속 상태의 가중치 값을 계산하는 내부 합계입니다. 대괄호 내의 항은 각 조치의 가능한 상태의 기여도를 즉각적 보상 R(s, a, s’)의 합과 감마 요소 𝛾에 의한 할인된 보상의 합으로 계산합니다.</p>
<p>상태-가치 함수를 표현하는 다른 방법은 다음과 같습니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_9.png" alt="equation_9"></p>
<div class="content-ad"></div>
<p>위의 공식은 다음 상태의 가치를 조건부 확률로 계산된 예상 반환 E𝝅로 정의합니다. 시간 t에서 상태 s가 주어졌을 때 시간 t에서 보상 R을 받을 조건부 확률로 계산됩니다. 보상 R은 후속 상태의 예상 반환의 곱의 합과 감마 감쇠를 고려하여 계산됩니다.</p>
<p>더 잘 이해하기 위해 3 x 3 그리드 월드의 에이전트를 상상해보세요. 각 시간 단계마다 상, 하, 오른쪽, 왼쪽 네 가지의 가능한 조치가 사용 가능합니다.</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_10.png" alt="grid"></p>
<p>우리는 상태 가치를 0으로 초기화하고, 상태 가치 함수에 대한 벨만 방정식을 사용하여 그리드 내의 보상 분포가 주어졌을 때 상태 가치를 최적화합니다. 우리는 (행, 열) 색인을 사용하여 그리드의 각 위치를 식별합니다.</p>
<div class="content-ad"></div>
<p>이 테이블 태그를 마크다운 형식으로 변경해주세요.</p>
<div class="content-ad"></div>
<h2>Action-Value Function</h2>
<p>우리는 행동-가치 함수가 상태-가치 함수의 두 번째 항목으로 내장되어 있다는 것을 보았습니다. 이는 행동-가치 함수가 상태 (s)에서 가능한 모든 행동의 가치를 계산한다는 것을 의미합니다. (s)에서 (s')로의 전이로부터 얻은 즉각적인 보상의 합과 다음 상태 (s')에서의 예상 누적 보상을 고려하여 주어진 작업으로부터 계산됩니다.</p>
<p>다시 말해, 행동 가치 함수는 상태 (s)에서 작업 (a)를 수행하는 것에 대한 누적 보상을 계산합니다. 여기서 기대 수익은 즉각적인 상태 전이 — R(s, a, s')로 표시됨 — 및 다음 상태 s'의 누적 보상의 할인 가치 —𝛾∑𝝅(a'|s')Q(s',a')​​로 표시됩니다.</p>
<div class="content-ad"></div>
<p>행동 가치 함수를 정하는 또 다른 방법은 최적 정책 𝝅을 따라 상태와 행동 쌍 (s, a)이 주어졌을 때 기대 반환값 E로 나타내는 것입니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_14.png" alt="image"></p>
<p>상태 가치 함수와 행동 가치 함수는 상태 가치 함수가 정책과 행동 가치 함수 Q(s, a)로 구할 수 있다는 관점에서 관련이 있습니다.</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_15.png" alt="image"></p>
<div class="content-ad"></div>
<p>따라서 행동-가치 함수와 상태-가치 함수는 재귀적으로 관련이 있습니다: 행동-상태 쌍의 가치가 상태의 가치를 결정하며, 상태는 반대로 행동의 가치를 결정합니다.</p>
<p>상태-가치 함수는 상태를 우선으로 하고 기대값 E를 출력합니다. 행동 가치 함수는 상태와 행동 쌍을 우선으로 하여 보상을 계산하고 기대 누적 반환 E를 얻습니다.</p>
<p>따라서 벨만 최적 방정식은 상태-가치와 행동-가치 함수의 재귀적 반복을 나타내며, 최적 값에 수렴할 때까지 반복됩니다. 상태-가치 함수를 위한 벨만 방정식은 아래와 같이 표현됩니다:</p>
<p><img src="https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_16.png" alt="image"></p>
<div class="content-ad"></div>
<p>현재 상태의 값은 가능한 모든 행동의 최대 보상으로 정의되며, 이는 (s) 상태에서 행동 a를 취할 때 얻는 보상과 다음 행동 s'의 값 및 할인 계수 감마의 곱으로 계산됩니다.</p>
<p>벨만 방정식은 현재 상태에서 모든 가능한 행동을 평균화하고 발생 확률에 따라 가중치를 부여합니다.</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_17.png" alt="image"></p>
<h2>모델 프리 메소드: 몬테카를로 &#x26; 시간차학습</h2>
<div class="content-ad"></div>
<p>위의 예시는 전환 동역학이 알려져 있어 따라서 완벽하게 계산될 수 있는 결정론적 모델을 설명합니다. 이것은 환경에 대한 완전한 지식을 가지고 있기 때문입니다.</p>
<p>그러나 대부분의 작업에서는 환경에 대해 완전한 지식을 갖고 있지 않습니다. 이 정보 대신에 우리는 동적 프로그래밍 방정식을 해결할 수 없기 때문에 정확한 결정론적 전환 동역학으로 진행할 수 없습니다. 이 문제를 극복하기 위해 통계에서 빌려온 기술을 사용하여 환경의 상태를 샘플에서 추론할 수 있습니다.</p>
<p>Monte Carlo 방법론에서는 예상 수익을 샘플 수익의 평균으로 근사화합니다. 샘플이 무한대로 접근함에 따라 평균 수익이 예상 수익의 실제 값으로 수렴합니다. 에이전트가 종료될 때까지 전체 에피소드를 실행한 다음 가치 함수를 계산하는 방식으로 이를 수행합니다. 그런 다음 N개의 에피소드 샘플을 취하여 평균을 사용하여 대상 상태의 예상 가치를 근사화합니다. 지금까지 궁금해 하고 계실 수 있듯이, 에피소드가 어떻게 정의되는지는 작업과 모델의 목적에 따라 달라집니다. 예를 들어, 체스 게임에서는 전체 게임을 실행하거나 임의의 단계 시리즈를 에피소드로 정의할 수 있습니다.</p>
<p>MC 업데이트 규칙을 다음과 같이 작성할 수 있습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_18.png" alt="image"></p>
<p>V(s) n+1은 다음 에피소드의 가치를 나타내며, S(s)n은 상태의 누적 가치를 나타내고 G는 보상의 가치를 나타냅니다. 누적 보상 G를 상태 값에 추가하고 에피소드 또는 샘플의 수로 나눕니다.</p>
<p>우리는 MC 업데이트 규칙을 대수적으로 재배치할 수 있습니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_19.png" alt="image"></p>
<div class="content-ad"></div>
<p>Monte Carlo 방법과는 달리 Temporal Difference (TD)에서는 각 에피소드가 끝난 후가 아니라 각 시간 단계나 증분마다 상태 가치 함수를 평가합니다. 환경에 대한 정보가 없는 초기 상태에서는 상태의 가치 V(s)를 0이나 다른 값으로 초기화해야 하며, 이후 매 시간 단계마다 업데이트됩니다.</p>
<p>TD에서는 상태의 가치를 계산하는 데 두 단계가 필요합니다. 먼저 한 단계의 오차를 계산한 다음 업데이트 규칙을 적용하여 상태의 가치를 변경합니다. 오차는 다음과 같은 차이 공식으로 주어집니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_20.png" alt="image"></p>
<p>여기서 𝜹t는 오차를 나타내며, R(t+1)은 행동에서 얻는 보상, V(S t+1)은 다음 상태의 추정 가치, V(S)는 현재 상태의 가치를 의미합니다. TD가 다음 상태의 추정 가치를 사용하여 현재 상태를 평가하는 것을 부트스트랩이라고 합니다. 이를 통해 현재 상태의 값에서 행동의 보상과 할인 계수와 다음 상태의 가치의 곱을 더하고 빼는 것으로 상태의 값이 즉시 시간 단계마다 업데이트됩니다.</p>
<div class="content-ad"></div>
<p>예상 보상과 실제 관측 값 사이의 차이를 𝜹(탐색-백업 간격)을 𝛼(학습률)에 곱해주면 관측과 기대 사이의 차이를 줄일 수 있어요.</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_21.png" alt="image"></p>
<p>𝛼의 역할은 TD 알고리즘의 학습 정도를 결정하는데, 𝛼는 실수형양수값이에요. 일반적으로 𝛼는 0.1, 0.01, 0.001 같은 값으로 설정돼요. 높은 𝛼 값은 업데이트를 더 적극적으로 진행하도록 해주고, 낮은 값은 보수적인 업데이트를 보장해요. 𝛼의 값은 탐험-활용 균형에 영향을 미치는데, 더 높은 𝛼는 탐험을 선호하고, 낮은 𝛼는 활용을 선호하게 됩니다.</p>
<p>MC와 TD 방법은 환경에 대한 사전 지식 없이 진행되지만, Temporal Difference의 장점은 매 시간 단계에서 온라인 업데이트를 계산한다는 것이고, 몬테카를로 방법의 장점은 값 추정을 위해 샘플링에만 의존하여 편향이 없다는 것이에요. TD 방법의 단점은 높은 편향이고, MC 방법의 단점은 중요한 업데이트를 간과하여 높은 분산을 유발한다는 점이에요. 이것은 두 학습 전략 사이의 최적점이 어딘가에서 존재해야 한다는 것을 시사합니다.</p>
<div class="content-ad"></div>
<p>TD 방법론은 단계 평가 전략을 n단계로 변경함으로써 최적화할 수 있습니다. 이렇게 함으로써, TD와 MC 사이에서 타협할 수 있는 기회를 얻게 됩니다. 우리가 n단계마다 상태 가치를 평가할 때, 우리는 매 단계 후가 아닌 미래 n단계를 추정함으로써 이를 수행합니다.</p>
<p>n단계 TD에 대한 수정된 접근 방식은 TD(𝝀)입니다. TD(𝝀) 방법은 지나간 상태-액션 쌍에 대한 신용을 할당하기 위해 '적격성 흔적(eligibility traces)'이라 불리는 매개변수를 사용합니다. 미래 n단계를 추정하는 대신, 적격성 흔적은 여러 TD 단계에 걸쳐 상태-액션 쌍에 대한 신용을 할당합니다. 적격성 흔적은 지난 상태-액션 쌍이 관찰된 보상 전환에 기여한 정도에 대해 신용을 부여합니다. 적격성 흔적은 각 상태-액션 쌍에 연관된 벡터나 행렬로 표현됩니다. 시간 단계에 대한 적격성 흔적은 다음과 같이 재귀적으로 계산됩니다:</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_22.png" alt="Eligibility Trace Formula"></p>
<p>여기서 람다(𝝀) 매개변수는 부트스트래핑의 정도를 제어합니다. 𝝀 = 1일 때, 부트스트래핑이 없어지고 업데이트 규칙은 Monte Carlo로 축소됩니다. 𝝀 = 0일 때는 부트스트래핑이 있는 TD(0)로 축소됩니다. TD(𝝀)는 TD와 MC를 연속체로 일반화한 것으로, 여기서 TD(0)는 단일 단계 TD를 의미하며, TD(1)은 TD를 ∞ 단계까지 확장한 극한값인 MC를 의미합니다. 수식에서 보듯이, 적격성 흔적 매개변수는 재귀적으로 계산됩니다. 다음 시간 단계의 적격성 흔적값은 이전 단계의 적격성 흔적 값을 입력으로 취합니다. E(s) = 0일 때, 부트스트랩이 없어집니다. TD(𝝀) 업데이트 규칙은 아래와 같이 TD 및 MC 업데이트 규칙과 동일하게 계산되지만, 애러에 적격성 흔적을 곱하는 것이 차이입니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_23.png" alt="Image"></p>
<h2>ANNs를 활용한 강화 학습 확장</h2>
<p>모델 기반 또는 모델 무관 RL 알고리즘은 차원의 저주로 인한 스케일링 문제에 직면하며, 다양한 유형의 환경 간 일반화에 어려움을 겪으며, 샘플 효율성에 대한 어려움이 있습니다.</p>
<p>인공 신경망(ANN)은 RL 아키텍처 내재한 일부 한계를 교정하는 강력한 방법을 제공합니다. 특히, ANNs는 샘플링 효율성, 환경 일반화, 차원의 저주로 인한 스케일링 문제를 개선합니다. 데이터로부터 일반 함수를 학습하기 때문에 우수한 일반화 능력을 통해 샘플 효율성을 줄이고 환경 일반화를 향상시킵니다. 이는 숨겨진 계층의 수와 각 숨겨진 계층 당 뉴런 수를 늘릴 수 있어 더 잘 스케일링할 수 있도록 합니다. 그러나 너무 많은 숨겨진 계층과 뉴런은 계산 스케일링 문제를 야기할 수도 있습니다(특정 범위를 벗어날 경우 차원의 저주를 피할 수 없습니다). 또한 전통적으로 ANNs가 사전에 목표 상태의 비정상성 문제에 시달리며, RL 알고리즘은 정책 상이든 오프-정책 상에 상관없이 업데이트 함수를 통해 최적 상태를 찾습니다.</p>
<div class="content-ad"></div>
<p>전통적인 강화 학습 알고리즘이 확률적 전이 규칙에 의존하는 데 반해, 강화 학습에 인공 신경망(ANNs)을 적용하면 함수 근사를 사용하여 상태 및 상태-행동 값들을 계산하게 됩니다. 선형 근사 및 타일 코딩과 같은 여러 함수 근사 방법을 적용할 수 있지만, 인공 신경망은 비선형 함수 근사를 활용한 일반화 능력으로 가장 강력한 기술을 구성합니다.</p>
<p>강화 학습에 인공 신경망을 적용하는 두 가지 접근 방식인 딥 Q 학습(DQN)과 시간차 학습( Temporal Difference; TD(𝝀))에 대해 알아봅시다. 미리 목표 값들을 모르기 때문에 MC 또는 TD를 사용하여 목표 상태의 추정인 예상 반환값을 생성합니다. 그런 다음 이 값은 함수(실제로는 네트워크 매개 변수 𝜃에 대한 전체 네트워크의 오차의 편도함수인 경사)가 근사화할 목표값으로 사용됩니다. 인공 신경망은 목표값을 추정 값과 출력 사이의 오차를 계산하여 그 오차를 역전파 및 최적화 알고리즘을 통해 감소시킴으로써 목표 값을 근사화합니다. 가장 일반적인 최적화 알고리즘은 확률적 경사 하강법의 변형인 것이며, 이를테면 확률적 경사 하강법이 대표적입니다.</p>
<p><img src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_24.png" alt="강화 학습 소개"></p>
<h2>오프-폴리시 DQN</h2>
<div class="content-ad"></div>
<p>큐-러닝은 SARSA(State, Action, Reward, State', Action')의 오프-폴리시 버전으로, 다음 상태-액션 쌍 Q(s', a')은 다음 상태에서 이용 가능한 액션들 중에서 최대 예측 값으로 추정됩니다. 다시 말해, 큐-러닝은 다음 상태 s'에서 이용 가능한 액션들 사이에서 Q(s',a')의 최댓값을 선택합니다. 이는 정책(𝜋)을 사용하지 않고 Q(s',a')를 학습한다는 것을 의미합니다. 반면, SARSA는 이전 액션을 선택하고 다음 상태-액션 쌍인 Q(s',a')를 추정하는 온-폴리시 방법입니다. 이는 상태가 주어졌을 때 액션의 확률인 정책(𝜋)을 사용하여 Q-함수를 학습한다는 것을 의미합니다.</p>
<p>딥 큐-러닝에서는 액션-가치 함수 Q(a, s)가 Q(a,s, 𝜃)로 표현되며, 여기서 𝜃은 신경망 매개변수를 나타냅니다. Theta(𝜃) 매개변수는 신경망에서의 가중치 w에 해당하며, 뉴런들 간의 연결에 관련되어 있습니다. 이 가중치는 연결의 강도를 결정하고, 오차를 최소화하기 위해 역전파를 통해 후방으로 조정됩니다. DQN은 환경의 고차원 표현을 입력으로 삼고, 각 가능한 액션에 대한 액션-가치 벡터를 출력합니다. 예상 수익은 일반적으로 MC 또는 TD 접근법을 통해 근사됩니다. 이후, 역전파와 최적화 함수를 사용하여 정책 기울기를 계산하고 정책 네트워크의 매개변수(𝜃)를 조정하여 오차를 줄입니다.</p>
<p>인공신경망은 새로운 정보에 매우 민감하기 때문에, 새로운 정보가 이전에 작성된 정보를 덮어쓰는 치명적인 잊혀짐을 초래할 수 있습니다. 이러한 치명적인 잊혀짐을 다루는 방법 중 하나는 경험 재생을 적용하는 것입니다. 이 기법은 과거 경험을 저장하고 네트워크를 훈련하는 데 재사용합니다.</p>
<h2>온-폴리시 딥 TD(𝝀)</h2>
<div class="content-ad"></div>
<p>ANNs는 TD(λ) 방법에도 적용할 수 있습니다. 여기서 상태 관찰은 ANN에 입력으로 공급되고, ANN은 그것을 통해 액션-가치 함수를 출력으로 근사화합니다. TD(λ)의 온-폴리시 성격 때문에 Deep TD(λ) 방법은 상태 간의 장기 의존성이 필요한 작업에 가장 적합합니다.</p>
<p>TD(λ)와 같은 온라인 학습 방법을 훈련하는 것은 도전적일 수 있습니다. 왜냐하면 환경의 분포가 부트스트래핑으로 인해 매 단계마다 또는 n 단계마다 변경되기 때문이기 때문입니다. 이를 비정상성(nonstationarity)이라고 부르며, ANN 매개변수 𝜃가 최적으로 수렴하는 것을 방해합니다. 온라인 학습에서 연이어 발생하는 상태 간의 종속성은 치명적인 잊혀짐(catastrophic forgetting)을 발생시킬 수 있어, 업데이트가 과거 학습에 영향을 미칩니다. 더욱이, 과거 조치에 신용을 할당하는 자격 흔적 및 ANNs의 결합은 역전파 단계에서 추가적인 복잡성을 초래할 수 있습니다.</p>
<p>위기에 대처하는 한 가지 방법은 경험 재생(experience replay) 기법을 활용하는 것입니다. 경험 재생은 학습된 에이전트 에피소드를 [s, a, r, s’]의 벡터로 메모리 버퍼에 저장합니다. 훈련 중에 네트워크는 저장된 학습 벡터의 메모리 버퍼에서 샘플을 추출하여 네트워크 매개변수를 업데이트합니다. 이를 통해 네트워크는 더 큰 안정성을 제공받고, 새로운 경험으로 인한 큰 오류나 단계 간의 시간 차이로 인한 치명적인 간섭에서 덜 영향을 받습니다.</p>
<p>Deep TD(λ) 알고리즘은 상태 공간이 연속적이고 대상이 알려지지 않거나 불분명한 연속 제어 작업에서 뛰어난 성과를 보여주었습니다. 이러한 작업에는 로봇 과제, 자율 주행 자동차, 금융 시장의 연속 제어 작업 등이 포함됩니다.</p>
<div class="content-ad"></div>
<h2>강화 학습과 인공 일반 지능</h2>
<p>강화 학습이 인공 일반 지능에 미칠 영향은 무엇인가요?</p>
<p>"지능"이라는 것은 서로 다른 능력을 단일한 개념으로 결합하기 때문에 모호한 변수이지만, "일반 지능"은 생물의 진화된 능력들을 기반으로 하며, 생존과 번식을 위해 세계적인 정보를 변환하는 것이 요구된다. 심지어 인간의 맥락에서도 지능은 유기적인 생존 가능성의 윤곽에서 분리될 수 없다. 그러나 이것이 통용되는 견해는 아니다. 일반적인 지혜는 지능이 이용 가능한 정보를 기반으로 추론을 계산하는 프로그램 또는 소프트웨어와 유사하다고 주장한다.</p>
<p>후자의 개념은 경쟁한다고 여겨지는 두 가지 모델로 구성되어 있는데, 하나는 절차를 따르는 지능으로 설명되고, 다른 하나는 최적의 예측을 위해 데이터로부터 일반화하는 지능으로 설명된다. 전자는 일반적으로 더 잘 이해되지만, 후자는 예측의 강도를 신뢰성 있게 향상시키는 기법 집합으로 이루어져 있다고 할 수 있다. 동물의 지능은 대부분 후자 모델에 기반한다.</p>
<div class="content-ad"></div>
<p>두 번째 모델의 가장 성공적인 패러다임은 인공 신경망을 통한 딥 러닝입니다. 인공 신경망 구조의 주요 장점은 사전 정보나 개념 없이 데이터로부터 일반화를 가능하게 한다는 것입니다. 이는 비지도 학습과 혼동해서는 안됩니다. 인공 신경망은 먼저 훈련을 통해 모델을 구축한 다음 새로운 데이터에 대해 해당 모델을 기반으로 예측합니다. 따라서 두뇌도 (진화를 통한 사전 훈련을 고려하면) 비슷한 일을 한다고 생각됩니다. 그러나 현재 인공 신경망에서는 두 가지 약점이 있습니다. 첫 번째 약점은 목표나 결과가 인간 디자이너에 의해 설정되어야 한다는 것입니다. 인공 신경망은 스스로 목표를 설정할 수 없습니다. 또한, 스스로 진실과 거짓을 구별할 수 없습니다. 모델이 그 결과를 근사화하는 방법을 배우기 위해 참된 결과를 제공해야 합니다. 두 번째 약점은 강화 학습 없이 인공 신경망이 자체 상태를 최적화하기 위해 환경을 탐색할 수 없다는 것입니다. 이러한 이유로 인공 신경망의 일반화와 예측 능력을 강화 학습의 결정 최적화 능력과 결합하는 것이 엄청난 섞임을 만들어냅니다.</p>
<p>이 기반 위에서 강화 학습이 인공 일반 지능으로 가는 가장 명확한 길을 대표한다고 주장한 사람들도 있습니다(Sutton, 2014). 이에 대한 직관은 분명합니다. 강화 학습은 생명체를 모델링하는 데 가장 가깝습니다. 이것이 다른 성공적인 구조들과 결합되면 (예를 들어 변환기와 같은), 모든 인간 능력을 복제하고 능가하는 AI 모델로 이어질 수 있습니다.</p>
<p>그러나 만약 인간이 일반 지능의 기초라면, 일반 지능의 개념은 인지능력을 생존 제약과 어떤 형태의 구현체와 이혼시키지 않는 것일 수 없습니다. 반면에, 일반 지능을 생명 형태에 언급하지 않고 정의할 수 있다면, 그것이 어떤 모습인지는 분명하지 않습니다. 즉 순수한 추상 모델은 Marcus Hutter의 AIXI와 같은 시도에도 불구하고 만족스러운 형식화를 피해갑니다. 추상적으로는 논리적 추론과 계산 능력만으로 문제를 해결하는 완벽하게 합리적인 에이전트로 생각할 수 있습니다. 정보와 구현체 간의 격차는 이 기사의 범위를 넘어서는 더 큰 토론을 유발하는 것이며, 관심이 있다면 이 논문이 좋은 시작점을 제공합니다.</p>
<p>그러나 강화 학습만으로는 인공 일반 지능을 충분히 표현할 수 있는지에 대한 의문이 있습니다. 이에 대한 이유로는 일반 지능의 정의에서 비롯한 것들이며 현재의 대부분의 AI 연구자들이 명시적인 내부 표현을 필수적인 요소로 간주하지 않고 있기 때문입니다. 이에 대한 상당한 이유가 있습니다. 딥 러닝의 성공 이전에 인공 일반지능의 희망을 걸고 있던 상징적 AI는 실패로 이어졌습니다. 상징적 AI는 주로 명시적으로 코딩된 논리 규칙과 최적 추론 생성을 위한 지식 저장소에 기초한 인공 지능 접근 방식을 나타냅니다.</p>
<div class="content-ad"></div>
<p>상징적 인공지능과 신경망 사이의 긴장은 그러나 근거 없을 수도 있습니다. 많은 연구자들은 인공 일반 지능을 얻기 위한 탐구가 이러한 접근 방식을 적절히 결합하는 데 있다고 믿습니다. 신경망이 뇌의 본질적 오네톨로지를 근사화한다고 생각하는 이유는 수학적 논리가 뇌가 추론하는 방식이 아니기 때문입니다. 즉, 그것은 필요충분 조건을 계산하지 않거나 정확한 멤버십을 계산하는 것보다는 점수 있는 멤버십에 중점을 두며, 이는 퍼지 논리와 같은 방식에 의해 근사화되며 ANN(인공신경망)이 뛰어납니다.</p>
<p>신경망은 원하는 출력을 달성하기 위해 파라미터화된 은닉층의 계층적 아키텍처로 구성되어 있으며, 고도로 보정된 동적 학습률, 활성화 함수, 연결 가중치 및 최적화 알고리즘을 통해 오차를 최소화하기 위해 보정된 학습율을 통해 원하는 출력을 달성합니다. 위와 같이 고도로 보정된 하이퍼파라미터를 넘어서 사람이 이해하지 못하는 정보가 은닉층에서 처리된다는 가정입니다. 정보가 이산적인 표현 단위(아날로그나 이미지 등)의 조합으로 저장되지 않으며 수십억 개의 뉴런들로 이루어진 분산 아키텍처로 저장된다는 것이 뇌의 경우와 같다는 가정입니다. 언어적으로 구조화된 생각이 고정적인 뉴런 조합으로 내부적으로 표현되는 것이 아닌데, 예를 들어 '존재 자체가 다른 존재를 위한 존재임을 하자’ 라는 문장 또는 단어를 나타내는 뉴런의 특정 조합이 없습니다.</p>
<p>언어적 능력은 대신 경험에 의해 강화된 의미 연결과 재생규칙으로 내장된 거대한 네트워크에 포함되어 있습니다. 다시 말해, 우리가 반영적으로 문장을 작성하고 말할 때 언어와 생각이 뇌의 본질적 오네톨로지와 문법 간 이쾌적 매핑이 아닌, 연결의 정도와 연결 강도에 의해 특징을 가진 신경 접속의 분산 네트워크에 내재되어 있다는것을 보여줍니다.</p>
<div class="content-ad"></div>
<p>현재 AI는 세계에서 자체적으로 추진되는 자율 시스템을 근사하지 않습니다. 또한 인간과 다른 동물들이 하는 방식으로 자체 환경적 환경을 생성하거나 자체 검색 공간을 재구성하지도 않습니다. 현재 이 제약이 없기 때문에 인간 디자이너는 AI의 정보적 중요성을 설정할 수 있습니다. 예를 들어 텍스트 생성, 환경 감지 등이 있습니다. 아키텍처가 진정한 문제 해결 기계로 진화하더라도 반성적 인식 능력이 없다면 일반적 지능을 보유하고 있다고 할 수 없습니다. 전통적으로 일반 지능의 정의에서는 인간 지능의 상징인 전체 인식의 변수를 생략합니다. 전통적인 지능 정의는 반사적이고 전체적 인식은 역공학 및 구성 요소의 분석에 대한 강한 저항을 가지고 있기 때문입니다. 그 이유로 반성적 인식은 지능의 구성 요소로서 배제됩니다. 그러나 현재의 과학적 설명에 대한 저항을 인정한다고 해도, 물리주의를 배제하거나 비자연론의 지지를 함축하지 않습니다. 오히려 이해력의 부재를 인정하는 신호일 뿐입니다. 이해력의 공방 속에, 반성적인 인식이 생명 유기체의 기본 속성인 감각의 확장임을 가설합니다. 이를 주장함으로써, 자연선택 이외의 방법을 통해 자율 시스템을 설계할 수 없다는 의미는 아니지만, 그들이 가까운 미래에도 과학적 분석에는 어려움을 줄 수 있다는 가능성은 여전히 열려 있습니다. 강화 학습이 일반적 지능으로 이어지길 바란다면, 에이전트는 세계의 복잡한 표현을 보유할 뿐만 아니라 그 표현의 내부에서 전반적인 관점을 유지할 수 있는 강력한 아키텍처를 사전으로 가져야 합니다. 이는 모델-세계 상호작용이 과제에 반드시 필수적이지만, 원조 아키텍처는 다중 모달 정보 처리와 통합 능력을 갖춘 복합적인 계층적 내부 구조를 요구할 것입니다.</p>
<h2>선택된 참고 자료</h2>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., &#x26; Hassabis, D. (2015). Deep Reinforcement Learning을 통한 Human-level Control. Nature, 518(7540), 529–533. <a href="https://doi.org/10.1038/nature14236" rel="nofollow" target="_blank">https://doi.org/10.1038/nature14236</a></p>
<p>Neftci, E. O., &#x26; Averbeck, B. B. (2019, March 4). 인공 및 생물학적 시스템에서 강화 학습. Nature News. <a href="https://www.nature.com/articles/s42256-019-0025-4" rel="nofollow" target="_blank">https://www.nature.com/articles/s42256-019-0025-4</a></p>
<div class="content-ad"></div>
<p>Sharma, S. (2024, March 7). Learning to Mix 𝑛-Step Returns: Generalizing 𝜆-Returns for Deep Reinforcement Learning. Ar5iv. <a href="https://ar5iv.labs.arxiv.org/html/1705.07445" rel="nofollow" target="_blank">Link</a></p>
<p>Sanghi, Nimish. Deep Reinforcement Learning with Python: With PYTORCH, Tensorflow, and Openai Gym. Apress, 2021.</p>
<p>Silver, D., Singh, S., Precup, D., &#x26; Sutton, R. S. (2021). Reward is enough. Artificial Intelligence, 299, 103535. <a href="https://doi.org/10.1016/j.artint.2021.103535" rel="nofollow" target="_blank">Link</a></p>
<p>Spens, E., &#x26; Burgess, N. (2024, January 19). A generative model of memory construction and consolidation. Nature News. <a href="https://www.nature.com/articles/s41562-023-01799-z" rel="nofollow" target="_blank">Link</a></p>
<div class="content-ad"></div>
<p>Sutton, Richard S. Introduction to Reinforcement Learning. MIT Press.</p>
<p>Tyng, C. M., Amin, H. U., Saad, M. N. M., &#x26; Malik, A. S. (2017, August 24). The influences of emotion on learning and memory. Frontiers in psychology. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/" rel="nofollow" target="_blank">Link</a></p>
<p>White, A., Modayil, J., &#x26; Sutton, R. (2014). Surprise and Curiosity for Big Data Robotics. Association for the Advancement of Artificial Intelligence, 19–22.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"강화 학습 소개","description":"","date":"2024-06-19 06:29","slug":"2024-06-19-AnIntroductiontoReinforcementLearning","content":"\n\n## 강화 학습의 기초에 대한 심층 탐구, 모델 기반 및 모델 없는 방법 포함\n\n![Image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png)\n\n## 강화 학습이란?\n\n공학 지능의 한 경로는 생물학적 생물체를 모방하는 것에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생물학적 생명체들은 환경으로부터 정보를 전도하고, 이를 처리(인지과학이 연구하는 바)하며, 생존에 유리한 행동을 출력합니다. 이러한 행동들은 가장 기본적인 수준에서 음식 수확, 번식, 위험 회피와 관련됩니다. 또한, 이는 놀이, 창의성, 문제 해결, 설계 및 공학, 사교, 로맨스, 지성 생활과 같은 다양한 인간 활동도 포함합니다.\n\n그렇다면, 위의 모든 것을 수행할 수 있는 시스템을 어떻게 설계할까요?\n\n만약 우리가 간단한 생물체를 어떤 환경의 함수로 모델링한다면, 우리는 에이전트, 환경의 모델, 그 에이전트를 현재 상태에서 원하는 상태로 이동시키는 함수가 필요할 것입니다.\n\n심리학에서, 두 가지 주요 학파인 행동주의와 인지과학은 인간 행동을 설명하기 위해 양립하고 있습니다. 행동주의자는 학습 메커니즘의 함수로써 행동을 이해하며, 학습은 행동적 출력에 귀속될 수 있다고 합니다. 반면에, 인지과학은 환경과의 상호작용을 정보 처리 접근법을 통해 모델링합니다. 이 접근법에서, 에이전트는 외부 자극을 처음에는 감각을 통해 내부 표현으로 변환하고, 그 후 사고와 추론 능력에 이르기까지 변형 및 통합 과정을 거쳐 행동적 출력을 반환합니다. 전자 접근법에서는, 학습은 주로 환경적 조건부로서 이해됩니다. 반면에 후자에서는, 정신적 표현이 행동을 예측하는 데 필수적이라고 여겨집니다. 강화 학습은 주로 행동주의 접근법에서 영향을 받아, 환경적 보상이 에이전트의 탐색 공간 내에서 진화를 결정한다고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업자 조건화, 1950년대-60년대에 유행했던 행동주의 심리학 학파로서, 학습을 보상과 처벌이라는 환경 메커니즘의 결과물로 정의했습니다. 조작적 조건화의 전제 조건으로는 에드워드 손다이크가 제안한 효과의 법칙이 포함되어 있습니다. 이 법칙은 만족스러운 효과를 일으키는 행동은 재발을 더 많이 유발하며, 불만족스러운 효과를 일으키는 행동은 재발을 덜 유발한다는 것을 제안합니다. B.F. 스키너는 효과를 강화와 처벌의 용어로 운용했습니다. 강화는 행동의 재발 발생 가능성을 증가시키며, 이것은 접근 또는 억제 요인의 제거를 말할 수 있습니다. 접근은 긍정적 강화, 회피의 역전인 부정적 강화로 표현됩니다. 긍정적 강화의 예로는 스포츠에서 뛰어나고 자주 이기는 것이 포함됩니다. 부정적 강화의 예는 억제적 자극을 제거하는 것인데, 이를 예로 들 수 있는 것은 경기 도중 당신을 조롱하는 학교 폭력가입니다. 작업자 조건화는 가장 큰 보상을 받는 행동을 반복할 가능성이 높다고 예측합니다. 반면 처벌은 행동 효과를 제어하기 위해 부정적 결과를 추가하거나 행동과 관련된 보상을 제거함으로써 구성됩니다. 파울링으로 게임에서 퇴장당했을 때의 경우는 긍정적 처벌을 보여줍니다. 성적이 좋지 않고 게임에서 패배한 경우는 부정적 처벌을 나타내며, 이는 미래에 더 이상 게임을 하지 않을 수 있습니다.\n\n인간 사회의 삶의 게임은 행동을 구성하는 보조적 강화나 사회적으로 구성된 보상과 처벌로 가득 차 있습니다. 이는 돈, 학점, 대학 입학 기준, 게임에서 이기고 지는 규칙과 같이 사회적 보상과 처벌을 포함합니다. 이러한 것들은 음식, 번식, 사회적 승인과 같은 생물학적 요구에 더 가까운 자연적 강화요인을 보완합니다.\n\n기억은 이전 경험을 유지할 수 있도록 하는 학습에서 중요한 역할을 합니다. 증거에 따르면 기억은 경험의 콘텐츠보다는 보상과 처벌을 부호화합니다. 실험 대상은 보상을 받는 경험을 기억할 가능성이 더 높아지며, 따라서 이를 반복하는 경향이 있습니다. 부정적인 경험은 불리하게 기억될 가능성이 더 높아지며, 이를 피하려고 합니다. 기억 메커니즘은 복잡하고 다양하며, 실험 대상들이 기억을 회상함으로써 기억을 다시 구성함에 있어서 적극적인 역할을 하는 것으로 나타납니다. 이 사실은 행동주의에 대한 예측을 어렵게 만들며, 단독으로 조건화 원리에 근거한 예측을 하기 어렵게 만든다. 게다가 보상과 처벌은 긍정적과 부정적 영향의 풍경을 단순화하며, 이것은 복합한 골짜기와 웅덩이들, 중첩된 의존성으로 이루어진 복잡한 지형이며, 이는 이진 공간보다는 연속적 스펙트럼으로 더 잘 모델링됩니다.\n\n불구하고, 강화 학습은 인공지능을 모델링하기 위해 에이전트, 환경 및 보상의 행동 온톨로지를 적응하는 다양한 수학적 기법으로 이루어져 있습니다. 아래에서 보게 되겠지만, 강화 학습의 측면 중 일부는 통제 이론에서 비롯되어 물리학과 공학으로 확장되는 전제 조건으로부터 나오며, 다른 측면은 심리학과 생물학으로부터 직접적으로 나오는 것입니다. 통제 이론의 대상과 생명체는 열역학적 균형으로부터 멀리 떨어진 최적 범위 내에 남아야 하는 동력학 시스템으로 구성되기 때문에, 기본 원리는 강화 학습과 인공지능의 목표에 부합됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다이내믹 프로그래밍은 주로 제어 이론에서 시작되어, 더 큰 문제를 하위 문제로 재귀적으로 분해하여 해결하는 수학적 최적화 방법으로 발전했습니다. 보통 재귀는 함수가 직접 또는 간접적으로 자기 자신을 파라미터로 전달하는 것을 말합니다.\n\n본 글에서는 주로 동적 프로그래밍의 요소에 초점을 맞추며, 이를 이산적이고 유한한 게임에 초점을 맞출 것입니다. 그러나 동적 프로그래밍은 모델 없이 강화학습 접근 방식과 결합하여 해결되는 한계를 가지고 있으며, 이를 보완하기 위해 동적 프로그래밍과 인공 신경망을 결합한 방법이 있습니다. 이는 한 때 신경동적 프로그래밍이라 불렸습니다. 보다 넓게는 강화학습과 인공 신경망의 결합을 딥 강화학습이라고 합니다. 이러한 모델은 강화학습 기법 내에서 딥 러닝의 강점을 통합하고 있습니다. 이러한 알고리즘 가운데 가장 인기 있는 것은 2013년 DeepMind에 의해 소개된 딥 Q-네트워크(DQN)입니다. 이 알고리즘 계열은 Q 함수를 근사화하기 위해 딥 러닝을 활용합니다. Q 함수의 근사화가 강화학습의 한 약점 중 하나이므로, 이러한 알고리즘들은 강화학습 패러다임의 주요 개선점을 제공합니다.\n\nDQN이 해결한 다른 약점에는 비선형 동역학을 캡처하는 유연성 부여, 차원의 저주로 인해 계산적으로 처리하기 어려워지는 일반적 범위의 차원을 수용하는 능력, 그리고 환경에 대한 보다 큰 일반화 능력이 포함됩니다.\n\n신경동적 프로그래밍은 순수히 행동주의 접근 방식의 약점을 해결하기 위해 심리학의 인지 패러다임을 활용하는 방향으로 발전하고 있습니다. 그러나 하위 수준 지각 정보의 계층적 구조와 처리에 대한 과학적 진전이 이루어지는 반면, 그 정보를 생각과 의식에 연결시키는 데는 더 많은 노력이 필요하며, 이는 과학적으로 약간 불가능한 것으로 남아 있습니다. 이러한 이유로 인공 신경망(ANNs)은 아직까지 사람의 지능의 복잡한 일반화 능력을 갖추고 있지 않습니다. 이는 ANNs보다 지수적으로 적은 샘플로 학습하는 인간의 지능과 대조적입니다. 본 글의 마지막 섹션에서 강화학습의 원칙을 인공 일반 지능(AGI) 쪽으로 채택하는 시사점을 논의하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 의사 결정 이론 및 제어 이론\n\n동적 프로그래밍과 강화 학습의 수학적 요소에 깊이 파고들기 전에, 철학적이고 수학적인 의사 결정 이론과 강화 학습 간의 관계를 명확히해야 합니다. 의사 결정 이론은 주로 합리적 선택 이론의 수학적 형식화로 구성되어 있지만, 강화 학습의 목표와 겹치는 부분이 있습니다. 강화 학습은 복잡한 환경과 정보 환경과 상호작용할 수 있는 성공적인 인공 에이전트로의 모델을 구축하려고 합니다.\n\n의사 결정 이론, 또는 선택 이론으로도 알려진 이론은 20세기에 이상적인 이유의 형식화가 짙어진 가운데 발전하였습니다. 구체적으로, 에이전트의 행위 확률을 그들의 선호도를 고려하여 양적화하기 위해 확률 이론을 사용합니다. 이 형식화 노력의 꼭대기는 폰 노이만-모건슈턴 유틸리티 절차였습니다. 요약하자면, 이 절차는 에이전트가 유틸리티 기대치에 따라 최대 이익을 가져다주는 행동을 선택하는 경향이 있음을 설명합니다.\n\n제어 이론은 기계 및 전기 공학 분야에서 나타나며, 동적 시스템의 상태 및 성능을 원하는 매개변수에 대해 최적화하는 데 관심이 있습니다. 중요한 메커니즘은 희망 변수를 측정하고 설정점과 비교한 후 그 차이를 수정을 위한 피드백으로 전달하는 컨트롤러로 이루어져 있습니다. 제어 이론의 큰 그림은 생명체의 대사 과정과 유사하며, 외부 변수 조건에 대비해 내부 온도의 설정 점을 유지하는 생물들의 과정을 반영합니다. 제어 이론과 의사 결정 이론의 연결은 명백합니다: 둘 다 시스템의 상태를 최적화하거나 발전시키기 위해 환경으로부터의 피드백에 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학적으로, 제어 및 의사 결정 문제의 부분 집합은 모두 동적 프로그래밍을 통해 해결할 수 있는 최적화 문제로 축소될 수 있습니다. 동적 프로그래밍은 상태 변수의 수가 지수적으로 증가함에 따라 계산 요구 사항이 지수적으로 증가하는 차원의 저주에 시달린 일반적인 확률적 최적 제어 문제를 해결하기 위해 그것을 더 작은 하위 문제로 분해하고 가치 함수를 계산함으로써 해결합니다. 저희는 강화 학습의 기본 원칙을 시연하면서, 동적 프로그래밍의 핵심인 에이전트의 상태 및 가치 함수 사이의 재귀적 관계에 대해 깊이 파헤쳐볼 것입니다.\n\n강화 학습과 의사 결정 이론은 보상 또는 유틸리티를 극대화하기 위한 절차를 정의하는 부분에서 겹칩니다. 그러나 의사 결정 이론에서는 유틸리티가 명시적으로 정의되지만, 경제 행동을 모델링하려는 것인 반면, 강화 학습에서는 유틸리티가 누적 보상으로 대체됩니다. 서로 다른 작업 목표에 대한 다른 정책을 적용하여 누적 보상을 극대화할 수 있으며, 탐구와 개발의 극성 방향 간의 상호 관계에 따라 달라집니다. 우리가 볼 것처럼, 탐색과 개발의 상호 관계를 탐색하는 것으로 표현되는 탐사-개발 딜레마에 따라 누적 보상을 극대화하는 것이 달라집니다.\n\n강화 모델의 기반이 되는 온톨로지를 개요화하는 것으로 시작해 봅시다.\n\n## 상태, 동작 및 보상\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n강화 학습은 의사 결정 이론의 이론적 장치를 활용하여 에이전트, 환경 및 동적 진화 규칙을 포함하는 모델을 구성합니다. 진화 규칙은 에이전트가 환경 내에서 보상을 추구할 수 있게 허용하며, 이를 관찰이라고도 합니다.\n\n에이전트는 환경으로부터 결정까지의 출력으로 정의됩니다. 특정 결정을 행동이라고 합니다. 네트워크의 현재 상태에서 행동으로의 매핑을 정책이라고 합니다. 정책은 상태에서 결과로의 매핑으로서 행동을 안내합니다.\n\n따라서 형식적으로 정책은 상태를 행동으로 매핑하는 함수입니다. 현재 상태가 주어졌을 때 행동의 조건부 확률로 나타낼 수 있으며, 여기서 그리스 문자 𝛑은 정책을 나타냅니다:\n\n![정책](https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전이 역학은 모든 가능한 상태 및 보상 값에 대한 확률 분포로 주어진 입력 보상에 따라 다음 상태를 정의합니다:\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_2.png)\n\n위의 공식은 다음 상태와 보상 쌍의 확률을 현재 상태 s와 행동 a가 주어졌을 때 다음 상태 s'와 보상 r의 조건부 확률과 같다고 정의합니다.\n\n행동은 보상을 누적하여 환경을 변경합니다. 그 결과로 보상은 에이전트 상태나 관측을 변경합니다. 보상 입력은 정책에 기반하여 미래의 행동 출력을 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로, 정책에는 두 가지 유형이 있습니다:\n\n보상은 일반적으로 스칼라 값 x로 형식화됩니다.\n\n특정 보상이 주어지면, 에이전트는 최적화 딜레마에 직면합니다: 에이전트는 단기 보상을 극대화해야 하는지, 아니면 완전한 생생력 기록을 통해 누적 보상을 극대화해야 하는지를 결정해야 합니다.\n\n이것은 탐색-이용 딜레마로 알려져 있습니다. 다시 말해, 전이 함수는 환경을 탐색하고 누적한 지식을 활용하여 최대 보상을 얻으며, 그 둘 사이의 균형을 최적화해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n탐색-활용 딜레마에 대한 최적의 해결책은 모델이 학습해야 하는 작업의 유형에 따라 달라집니다. 이 작업은 유한에서 무한(연속적 또는 이산적으로)로 범위가 있을 수 있습니다. 예를 들어 체스 게임은 에피소드 작업으로 형식화될 수 있습니다. 왜냐하면 유한한 구성 공간과 승, 패, 무승부 세 가지 가능한 결과를 가진 미리 정의된 종료 상태가 있기 때문입니다. 이는 현재 상태를 기준으로 최적의 후속 상태를 결정할 수 있는 것을 의미하며, 결정론적 전이 동역학을 통해 계산됩니다. 따라서 각 상태에 대해 단일 최적의 행동이 존재합니다.\n\n그러나 대부분의 작업은 유한한 구성 공간이나 미리 정의된 종료 상태를 가지고 있지 않습니다. 우리는 이러한 것들을 연속적인 작업으로 분류하고, 모델이 없는 방법을 통해 최적화합니다. 모델 없는 방법론에서는 전환 동역학을 계산하는 대신 모델이 환경에서 샘플링하여 최적의 후속 상태를 계산합니다. 다르게 말하면, 선견지명을 통해 행동을 계획하는 대신 환경에 대해 배우기 위해 시행착오를 사용합니다.\n\n모델 없이 강화 학습하는 두 가지 접근법이 일반적으로 있습니다: 몬테 카를로 접근법과 시간차 학습. 충분한 샘플의 평균이 기대값으로 수렴하기 때문에, 모델 없는 방법은 샘플 평균을 통해 예상값을 추정합니다. 몬테 카를로 방법은 충분히 큰 상태-행동 쌍의 샘플로 예상 누적 반환을 추정하여 가치 함수를 계산합니다. 일부 몬테 카를로 방법은 에피소드 작업의 끝에서만 가치 함수를 평가합니다. 연속적인 작업에서는 에피소드의 정의가 다양하게 변할 수 있고, 디자이너에 따라 시간 간격에 따라 설정할 수 있습니다.\n\n몬테 카를로 탐색과 반대로 시간차 학습은 시간 단계 간의 차이를 활용하여 가치 함수를 증분적으로 추정합니다. 시간차 방법의 접근 방식을 두면 몬테 카를로 방법에 비해 실제 예상 값과의 분산을 낮추는 특성이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면: 에이전트는 현재 상태와 액션 공간 쌍에서 상태 공간으로의 매핑을 통해 환경을 탐색합니다. 전이 동적은 미리 정의된 종단 상태를 가진 유한한 구성 공간에 대한 모든 가능한 매핑을 계산합니다. 미리 정의된 종단 상태와 유한한 상태 공간 대신에, 모델 무작위 접근법은 최상의 정책을 찾기 위해 환경에서 계속 샘플링합니다.\n\n동적 프로그래밍은 모든 상태-액션 쌍에서 상태 전이 확률과 예상 보상을 계산합니다. 이 프로세스가 어떻게 작동하는지 이해하기 위해서는, 마르코프 프로세스를 이해해야 합니다.\n\n다음에는 에이전트가 최적 후속 상태를 계산할 수 있도록 하는 수학적 모델을 배우게 됩니다. 앞서 논의한 대로, 최적성은 탐사-이용 딜레마로 이어지며, 이는 모델링하려는 작업 유형에 따라 다릅니다. 보상 구조를 자세히 살펴봄으로써 이를 더 잘 이해할 수 있을 겁니다.\n\n## 보상 평가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n강화 학습에서 보상을 측정하는 방법은 에이전트가 행동을 취함으로써 환경으로부터 얻는 스칼라 값으로 계량화됩니다. 이 보상의 가치는 행동의 즉각적인 선호도를 나타냅니다.\n\n반면에 누적 보상 또는 반환은 해당 시점까지 환경으로부터 누적된 모든 보상의 합을 나타냅니다. 에이전트의 목표는 단순히 즉각적 보상을 최적화하는 것이 아니라 누적 보상을 최적화하는 것입니다. 전자는 근시적 에이전트를 나타내며, 후자는 장기간 수익을 극대화하려는 장기 노력 에이전트를 나타냅니다.\n\n대부분의 경우 에이전트가 가장 높은 보상을 최대한 빨리 극대화하길 원하기 때문에 할인은 현재 최대 보상을 나중에 최대 보상보다 우선시하는 방식으로 도입됩니다.\n\n할인을 적용한 누적 보상 G는 아래 식으로 표현됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Reinforcement Learning Introduction](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_3.png)\n\n여기서 누적 보상 G는 보상과 해당 할인 계수 감마 𝜸의 곱의 합과 같습니다. 감마는 항상 0과 1 사이의 값인 '0,1'입니다. 감마는 각 시간 단계마다 지수적으로 증가되므로 무한한 시간 단계를 통해 감마가 0에 접근합니다.\n\n감마가 0에 접근할수록 단기 이익을 장려하고, 감마가 1에 가까워지면 무한한 반복을 통해 보상 합이 자체적으로 무한에 접근하므로 장기 이익을 장려합니다.\n\n대부분의 작업은 시간에 제한이 있기 때문에, 감마 할인은 값이 1 미만일 때 보상에 상한선을 부과합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n할인을 고려한 누적 보상의 압축된 방정식은 아래와 같습니다. 여기서 G는 보상 R의 예상 합을 나타내며, 이는 할인 요소 감마로 곱해집니다. 따라서 누적 보상은 보상과 할인 요소의 합으로 계산됩니다:\n\n\n![cumulative reward equation](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_4.png)\n\n\n## 마르코프 의사결정 과정 (MDP)\n\n지금까지 정책을 상태에서 행동으로 매핑하는 확률적 정의, 보상이 주어졌을 때 한 상태에서 다른 상태로 움직일 확률인 전이 역학, 그리고 보상이 어떻게 계산되는지에 대한 공식에 대해 논의해 왔습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자, 이제 조금 물러나서 확률적 전이 체인을 정의하는 보충 이론을 제공하겠습니다. 먼저 마르코프 과정이라고 하는 것부터 시작해봅시다. 마르코프 과정은 마르코프 성질을 만족하는 확률 과정입니다. 확률 과정은 무작위로 변하는 과정입니다. 마르코프 성질은 모든 상태에 대해 후속 상태가 현재 상태에만 의존된다는 것을 말합니다.\n\n과거 상태가 미래 상태에 영향을 미치지 않기 때문에 마르코프 성질을 만족하는 과정을 메모리리스라고 합니다. 집을 나가서 일하러 나가 다시 집으로 돌아오는 매일 재발되는 일정된 목적지 집합을 상상해보세요. 즉, 시작과 끝이 있는 순환 과정이 있습니다. 이제 더 나아가서 다음 목적지로 움직일 결정이 현재 목적지에만 의존한다고 상상해보세요. 처음에는 각 연결된 목적지가 동일한 확률 분포를 갖게 될 것입니다. 예를 들어, 집을 나가면 운전하거나 지하철을 탈 수 있는 선택지가 있다면, 두 가능한 미래 상태에 대한 초기 확률을 각각 0.5로 정할 수 있습니다. 모든 가능한 경로의 반복을 통해 이러한 확률은 어떤 경로가 다른 경로보다 선호되는 빈도 분포로 안정화될 수 있습니다. (이 유형의 확률을 경험적 확률이라고 부르며, 가능한 사건에 대한 결과를 한정된 테스트 수에 대해 평균화합니다) 그 분포 평형은 마르코프 체인 또는 과정이 될 것입니다.\n\n이제 아마도 생각 중일 것입니다: 어떻게 사건과 상태를 정의하나요? 고정된 가능한 상태와 안정한 확률 분포에 대해 얘기하려면 세상이 너무 복잡하지 않나요?\n\n매우 그렇습니다. 그러나 우리는 환경 속 요소들의 수학적 형식론을 원하기 때문에 모델링하려는 작업 또는 환경 유형을 구별해야 합니다. 이를 위해 시간 단계와 상태 공간의 표현, 즉 모든 가능한 상태의 분포를 명시해야 합니다. 아래의 정사각 행렬은 상태 공간과 시간의 축을 기준으로 마르코프 체인의 정의를 제공합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_5.png)\n\n상태 공간은 셀 수 있는/유한한 상태거나 연속적일 수 있습니다. 유한 상태 공간은 시스템의 모든 가능한 구성을 조합 이론을 통해 설명하고, 연속 상태 공간은 연속 함수를 통해 모든 가능한 구성을 설명합니다.\n\n유한 및 가산 무한 공간은 측정 가능한 공간으로 정수 또는 유리수를 취하며, 연속 공간은 실수를 취합니다.\n\n마찬가지로 시간 축은 이산 또는 연속적으로 정의될 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이산시간 프로세스는 이산적으로 상태 전이를 계산하지만, 가산 또는 비가산 상태 공간에서 모델링할 수 있습니다. 여기서 비가산이라 함은 실수의 무한한 10진 확장을 의미합니다. 실제로 컴퓨터가 시간을 세는 방식도 이와 같습니다. 이를 이산 단계로 처리합니다. 단계 사이의 간격은 아키텍처에 따라 다르지만, 주기는 보통 레지스터 상태를 변경하는 데 필요한 시간 단계의 길이로 측정됩니다.\n\n연속시간 체인은 연속으로 상태 전이를 계산하며, 가산 또는 비가산 상태 공간에 모델링될 수 있습니다.\n\n마르코프 프로세스라는 용어는 일반적으로 연속시간 프로세스에 사용되며, 마르코프 체인이라는 용어는 이 중 일부인 것을 나타냅니다: 이산시간, 확률적 제어 프로세스입니다. 이 기사에서는 이산시간, 유한 상태 공간에 초점을 맞출 것입니다.\n\n지금까지 우리의 마르코프 체인은 상태간 전이를 고정된 확률로 설명하는 매우 단순한 모델입니다. 행동과 보상이라는 모델링에 중요한 두 가지 요소가 빠져 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보상을 전이 확률로 할당하는 것이 마르코프 보상 과정입니다. 마르코프 보상 과정은 각 전이 상태에 보상을 할당합니다(양수 또는 음수 정수로 정의됨)으로써 시스템을 원하는 상태로 이끕니다. 누적 보상 공식을 상기해 보겠습니다. 기대 보상의 합에 일정한 할인 계수가 곱해진 값입니다. 마르코프 보상 과정을 사용하면 초기 상태 S가 주어졌을 때 상태 v(s)의 값과 누적 보상 G의 확률을 계산할 수 있습니다(여기서 G는 많은 반복 시행에서 평균화된 값입니다):\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_6.png)\n\n마르코프 결정 과정으로 나아가기 위해 필요한 마지막 변수는 행동입니다. 에이전트는 가능한 행동 집합에 대해 동등하게 분포된 확률로 시작하고 이후에 전이 함수를 업데이트하여 현재 상태와 행동을 다음 상태와 보상으로 매핑합니다. 이렇게 하면 앞서 설명한 전이 동학에 다시 도달하게 됩니다:\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 동적 계획법 및 벨만 최적성\n\n이것은 벨만(1957)에 의해 개발된 동적 프로그래밍의 개념으로 이어집니다.\n\n동적 프로그래밍을 이해하면, 동적 프로그래밍과 같은 완벽한 환경 지식이 필요하지 않는 근사 방법인 몬테카를로 탐색 및 시간차이 메소드도 이해할 수 있습니다. 이러한 모델-프리 방법은 완벽한 정보 대신 동적 프로그래밍의 결정적 정책을 근사화합니다. 따라서, 실제 세계 학습을 근사화하는 강력한 메커니즘이 제공됩니다.\n\n동적 프로그래밍이 최적의 에이전트 상태를 검색하고 찾는 핵심 아이디어는 상태 가치 함수와 행동 가치 함수 사이의 관계에 있습니다. 이들은 재귀적으로 관련되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 아이디어를 관련성 있는 예시로 확장해 봅시다. 예를 들어, 당신이 삶에서 최적 상태가 아니고 이를 바꾸고 싶다고 가정해 봅시다. 미래에 이루고 싶은 구체적인 목표나 위치가 있다고 해 봅시다. 이 큰 목표에 도달하기 위해 (더 좋은 직장을 얻는다던가, 가족을 꾸린다던가 등을 대체할 수 있습니다), 당신은 원하는 결과에 도움이 되는 일련의 작은 단계나 행동을 취해야 할 것입니다. 강화 학습의 언어로 번역하면, 현재 상태에는 특정 가치가 할당될 것입니다. 현재 상태와 가치를 고려하여 당신은 행동을 취할 것입니다. 이러한 행동은 전체 목표와 현재 상태에 따라 평가될 것입니다. 좋은 행동은 나쁜 행동보다 높은 가치를 받을 것입니다. 환경으로부터의 피드백은 행동의 가치를 결정할 것입니다 (이 값들이 어떻게 결정되는지는 작업마다 다릅니다). 상태의 평가는 사용 가능한 행동과 후속 상태의 가치에 영향을 미칠 것입니다. 그리고 행동의 평가는 현재 상태의 가치를 재귀적으로 영향을 줄 것입니다. 다시 말해, 행동과 상태는 재귀적으로 연결되어 있습니다.\n\n이제 현실에서, 당신의 목표와 그 목표에 이르는 행동 단계들은 이산 시간 단계 및 이산 상태 공간을 갖는 결정론적 시스템으로 명시할 수 없습니다 (비록 이 방식으로 근사화할 수도 있습니다). 대신, 동적 프로그래밍은 체스와 같은 게임처럼 정의 가능한 환경을 가정합니다. 여기서 시간 단계와 행동 공간이 이산적이고 유한하게 추상화됩니다. 현실과의 중요한 점은 더 큰 목표가 해당 큰 목표에 유리한 작은 부목표를 최적화함으로써 다가올 것이라는 점입니다.\n\n따라서 동적 프로그래밍은 다음 값들을 가정할 것입니다: (Ω, A, 𝒫), 여기서 Ω는 모든 가능한 상태의 합을 나타냅니다, A는 유한 샘플 공간의 부분집합인 행동 이벤트를 나타내며, P는 일정 정책 함수 𝝅에 의해 각 행동 이벤트에 할당된 확률을 나타냅니다.\n\n이제 우리가 결정론적 전이 역학에 대해 생각해 보면, 상태, 행동 및 보상의 집합이 유한하기 때문에, 특정 상태와 보상 쌍은 일정한 상태 및 행동 쌍이 주어졌을 때 그 값들이 발생할 확률을 갖게 될 것입니다. 이러한 확률은 상태 공간이 이산적이기 때문에 이산 확률 분포로 명시됩니다. 우리는 상태, 행동 및 보상으로 구성된 일련의 순서가 누적 보상을 최대화하려는 마르코프 결정 과정(MDPs)이라고 했습니다. 이때 보상을 스칼라 값으로 표현하며, 시간이 흐름에 따라 예상되는 누적 보상을 최대화하려고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 다루어야 할 질문은 우리가 지정한 가정에 따라 마르코프 의사결정 프로세스가 누적 보상을 최대화하는 방법입니다. 이 답은 벨만 최적 방정식에 의해 제공되며 두 함수인 상태 가치 함수와 행동 가치 함수 사이의 관계를 설명합니다.\n\n## 상태 가치 함수\n\n상태 가치 함수는 에이전트가 정책 𝝅에 따라 취할 수 있는 모든 가능한 조치의 확률의 합으로 정의될 수 있습니다. 각 조치에 대해 가능한 후속 상태의 가중치 값의 합으로 그 가치가 결정됩니다.\n\n보다 간단하게 말하자면, 상태 가치 함수는 특정 상태(s)에서 정책 𝝅을 따라 시작하여 에이전트가 얻을 수 있는 예상 누적 보상을 정의합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![equation_8](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_8.png)\n\n위의 방정식은 두 항으로 구성되어 있습니다: a) 정책 (𝝅)을 따라 상태 (s)에서 에이전트가 취할 수 있는 모든 가능한 조치들의 확률의 합, 그리고 b) 각 가능한 조치마다 가능한 후속 상태의 가중치 값을 계산하는 내부 합계입니다. 대괄호 내의 항은 각 조치의 가능한 상태의 기여도를 즉각적 보상 R(s, a, s’)의 합과 감마 요소 𝛾에 의한 할인된 보상의 합으로 계산합니다.\n\n상태-가치 함수를 표현하는 다른 방법은 다음과 같습니다:\n\n![equation_9](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 공식은 다음 상태의 가치를 조건부 확률로 계산된 예상 반환 E𝝅로 정의합니다. 시간 t에서 상태 s가 주어졌을 때 시간 t에서 보상 R을 받을 조건부 확률로 계산됩니다. 보상 R은 후속 상태의 예상 반환의 곱의 합과 감마 감쇠를 고려하여 계산됩니다.\n\n더 잘 이해하기 위해 3 x 3 그리드 월드의 에이전트를 상상해보세요. 각 시간 단계마다 상, 하, 오른쪽, 왼쪽 네 가지의 가능한 조치가 사용 가능합니다.\n\n![grid](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_10.png)\n\n우리는 상태 가치를 0으로 초기화하고, 상태 가치 함수에 대한 벨만 방정식을 사용하여 그리드 내의 보상 분포가 주어졌을 때 상태 가치를 최적화합니다. 우리는 (행, 열) 색인을 사용하여 그리드의 각 위치를 식별합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 테이블 태그를 마크다운 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Action-Value Function\n\n우리는 행동-가치 함수가 상태-가치 함수의 두 번째 항목으로 내장되어 있다는 것을 보았습니다. 이는 행동-가치 함수가 상태 (s)에서 가능한 모든 행동의 가치를 계산한다는 것을 의미합니다. (s)에서 (s')로의 전이로부터 얻은 즉각적인 보상의 합과 다음 상태 (s')에서의 예상 누적 보상을 고려하여 주어진 작업으로부터 계산됩니다.\n\n다시 말해, 행동 가치 함수는 상태 (s)에서 작업 (a)를 수행하는 것에 대한 누적 보상을 계산합니다. 여기서 기대 수익은 즉각적인 상태 전이 — R(s, a, s')로 표시됨 — 및 다음 상태 s'의 누적 보상의 할인 가치 —𝛾∑𝝅(a'|s')Q(s',a')​​로 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행동 가치 함수를 정하는 또 다른 방법은 최적 정책 𝝅을 따라 상태와 행동 쌍 (s, a)이 주어졌을 때 기대 반환값 E로 나타내는 것입니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_14.png)\n\n상태 가치 함수와 행동 가치 함수는 상태 가치 함수가 정책과 행동 가치 함수 Q(s, a)로 구할 수 있다는 관점에서 관련이 있습니다.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 행동-가치 함수와 상태-가치 함수는 재귀적으로 관련이 있습니다: 행동-상태 쌍의 가치가 상태의 가치를 결정하며, 상태는 반대로 행동의 가치를 결정합니다.\n\n상태-가치 함수는 상태를 우선으로 하고 기대값 E를 출력합니다. 행동 가치 함수는 상태와 행동 쌍을 우선으로 하여 보상을 계산하고 기대 누적 반환 E를 얻습니다.\n\n따라서 벨만 최적 방정식은 상태-가치와 행동-가치 함수의 재귀적 반복을 나타내며, 최적 값에 수렴할 때까지 반복됩니다. 상태-가치 함수를 위한 벨만 방정식은 아래와 같이 표현됩니다: \n\n![image](https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 상태의 값은 가능한 모든 행동의 최대 보상으로 정의되며, 이는 (s) 상태에서 행동 a를 취할 때 얻는 보상과 다음 행동 s'의 값 및 할인 계수 감마의 곱으로 계산됩니다.\n\n벨만 방정식은 현재 상태에서 모든 가능한 행동을 평균화하고 발생 확률에 따라 가중치를 부여합니다.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_17.png)\n\n## 모델 프리 메소드: 몬테카를로 \u0026 시간차학습\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 예시는 전환 동역학이 알려져 있어 따라서 완벽하게 계산될 수 있는 결정론적 모델을 설명합니다. 이것은 환경에 대한 완전한 지식을 가지고 있기 때문입니다.\n\n그러나 대부분의 작업에서는 환경에 대해 완전한 지식을 갖고 있지 않습니다. 이 정보 대신에 우리는 동적 프로그래밍 방정식을 해결할 수 없기 때문에 정확한 결정론적 전환 동역학으로 진행할 수 없습니다. 이 문제를 극복하기 위해 통계에서 빌려온 기술을 사용하여 환경의 상태를 샘플에서 추론할 수 있습니다.\n\nMonte Carlo 방법론에서는 예상 수익을 샘플 수익의 평균으로 근사화합니다. 샘플이 무한대로 접근함에 따라 평균 수익이 예상 수익의 실제 값으로 수렴합니다. 에이전트가 종료될 때까지 전체 에피소드를 실행한 다음 가치 함수를 계산하는 방식으로 이를 수행합니다. 그런 다음 N개의 에피소드 샘플을 취하여 평균을 사용하여 대상 상태의 예상 가치를 근사화합니다. 지금까지 궁금해 하고 계실 수 있듯이, 에피소드가 어떻게 정의되는지는 작업과 모델의 목적에 따라 달라집니다. 예를 들어, 체스 게임에서는 전체 게임을 실행하거나 임의의 단계 시리즈를 에피소드로 정의할 수 있습니다.\n\nMC 업데이트 규칙을 다음과 같이 작성할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_18.png)\n\nV(s) n+1은 다음 에피소드의 가치를 나타내며, S(s)n은 상태의 누적 가치를 나타내고 G는 보상의 가치를 나타냅니다. 누적 보상 G를 상태 값에 추가하고 에피소드 또는 샘플의 수로 나눕니다.\n\n우리는 MC 업데이트 규칙을 대수적으로 재배치할 수 있습니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_19.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMonte Carlo 방법과는 달리 Temporal Difference (TD)에서는 각 에피소드가 끝난 후가 아니라 각 시간 단계나 증분마다 상태 가치 함수를 평가합니다. 환경에 대한 정보가 없는 초기 상태에서는 상태의 가치 V(s)를 0이나 다른 값으로 초기화해야 하며, 이후 매 시간 단계마다 업데이트됩니다.\n\nTD에서는 상태의 가치를 계산하는 데 두 단계가 필요합니다. 먼저 한 단계의 오차를 계산한 다음 업데이트 규칙을 적용하여 상태의 가치를 변경합니다. 오차는 다음과 같은 차이 공식으로 주어집니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_20.png)\n\n여기서 𝜹t는 오차를 나타내며, R(t+1)은 행동에서 얻는 보상, V(S t+1)은 다음 상태의 추정 가치, V(S)는 현재 상태의 가치를 의미합니다. TD가 다음 상태의 추정 가치를 사용하여 현재 상태를 평가하는 것을 부트스트랩이라고 합니다. 이를 통해 현재 상태의 값에서 행동의 보상과 할인 계수와 다음 상태의 가치의 곱을 더하고 빼는 것으로 상태의 값이 즉시 시간 단계마다 업데이트됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예상 보상과 실제 관측 값 사이의 차이를 𝜹(탐색-백업 간격)을 𝛼(학습률)에 곱해주면 관측과 기대 사이의 차이를 줄일 수 있어요.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_21.png)\n\n𝛼의 역할은 TD 알고리즘의 학습 정도를 결정하는데, 𝛼는 실수형양수값이에요. 일반적으로 𝛼는 0.1, 0.01, 0.001 같은 값으로 설정돼요. 높은 𝛼 값은 업데이트를 더 적극적으로 진행하도록 해주고, 낮은 값은 보수적인 업데이트를 보장해요. 𝛼의 값은 탐험-활용 균형에 영향을 미치는데, 더 높은 𝛼는 탐험을 선호하고, 낮은 𝛼는 활용을 선호하게 됩니다.\n\nMC와 TD 방법은 환경에 대한 사전 지식 없이 진행되지만, Temporal Difference의 장점은 매 시간 단계에서 온라인 업데이트를 계산한다는 것이고, 몬테카를로 방법의 장점은 값 추정을 위해 샘플링에만 의존하여 편향이 없다는 것이에요. TD 방법의 단점은 높은 편향이고, MC 방법의 단점은 중요한 업데이트를 간과하여 높은 분산을 유발한다는 점이에요. 이것은 두 학습 전략 사이의 최적점이 어딘가에서 존재해야 한다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTD 방법론은 단계 평가 전략을 n단계로 변경함으로써 최적화할 수 있습니다. 이렇게 함으로써, TD와 MC 사이에서 타협할 수 있는 기회를 얻게 됩니다. 우리가 n단계마다 상태 가치를 평가할 때, 우리는 매 단계 후가 아닌 미래 n단계를 추정함으로써 이를 수행합니다.\n\nn단계 TD에 대한 수정된 접근 방식은 TD(𝝀)입니다. TD(𝝀) 방법은 지나간 상태-액션 쌍에 대한 신용을 할당하기 위해 '적격성 흔적(eligibility traces)'이라 불리는 매개변수를 사용합니다. 미래 n단계를 추정하는 대신, 적격성 흔적은 여러 TD 단계에 걸쳐 상태-액션 쌍에 대한 신용을 할당합니다. 적격성 흔적은 지난 상태-액션 쌍이 관찰된 보상 전환에 기여한 정도에 대해 신용을 부여합니다. 적격성 흔적은 각 상태-액션 쌍에 연관된 벡터나 행렬로 표현됩니다. 시간 단계에 대한 적격성 흔적은 다음과 같이 재귀적으로 계산됩니다:\n\n![Eligibility Trace Formula](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_22.png)\n\n여기서 람다(𝝀) 매개변수는 부트스트래핑의 정도를 제어합니다. 𝝀 = 1일 때, 부트스트래핑이 없어지고 업데이트 규칙은 Monte Carlo로 축소됩니다. 𝝀 = 0일 때는 부트스트래핑이 있는 TD(0)로 축소됩니다. TD(𝝀)는 TD와 MC를 연속체로 일반화한 것으로, 여기서 TD(0)는 단일 단계 TD를 의미하며, TD(1)은 TD를 ∞ 단계까지 확장한 극한값인 MC를 의미합니다. 수식에서 보듯이, 적격성 흔적 매개변수는 재귀적으로 계산됩니다. 다음 시간 단계의 적격성 흔적값은 이전 단계의 적격성 흔적 값을 입력으로 취합니다. E(s) = 0일 때, 부트스트랩이 없어집니다. TD(𝝀) 업데이트 규칙은 아래와 같이 TD 및 MC 업데이트 규칙과 동일하게 계산되지만, 애러에 적격성 흔적을 곱하는 것이 차이입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_23.png)\n\n## ANNs를 활용한 강화 학습 확장\n\n모델 기반 또는 모델 무관 RL 알고리즘은 차원의 저주로 인한 스케일링 문제에 직면하며, 다양한 유형의 환경 간 일반화에 어려움을 겪으며, 샘플 효율성에 대한 어려움이 있습니다.\n\n인공 신경망(ANN)은 RL 아키텍처 내재한 일부 한계를 교정하는 강력한 방법을 제공합니다. 특히, ANNs는 샘플링 효율성, 환경 일반화, 차원의 저주로 인한 스케일링 문제를 개선합니다. 데이터로부터 일반 함수를 학습하기 때문에 우수한 일반화 능력을 통해 샘플 효율성을 줄이고 환경 일반화를 향상시킵니다. 이는 숨겨진 계층의 수와 각 숨겨진 계층 당 뉴런 수를 늘릴 수 있어 더 잘 스케일링할 수 있도록 합니다. 그러나 너무 많은 숨겨진 계층과 뉴런은 계산 스케일링 문제를 야기할 수도 있습니다(특정 범위를 벗어날 경우 차원의 저주를 피할 수 없습니다). 또한 전통적으로 ANNs가 사전에 목표 상태의 비정상성 문제에 시달리며, RL 알고리즘은 정책 상이든 오프-정책 상에 상관없이 업데이트 함수를 통해 최적 상태를 찾습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전통적인 강화 학습 알고리즘이 확률적 전이 규칙에 의존하는 데 반해, 강화 학습에 인공 신경망(ANNs)을 적용하면 함수 근사를 사용하여 상태 및 상태-행동 값들을 계산하게 됩니다. 선형 근사 및 타일 코딩과 같은 여러 함수 근사 방법을 적용할 수 있지만, 인공 신경망은 비선형 함수 근사를 활용한 일반화 능력으로 가장 강력한 기술을 구성합니다.\n\n강화 학습에 인공 신경망을 적용하는 두 가지 접근 방식인 딥 Q 학습(DQN)과 시간차 학습( Temporal Difference; TD(𝝀))에 대해 알아봅시다. 미리 목표 값들을 모르기 때문에 MC 또는 TD를 사용하여 목표 상태의 추정인 예상 반환값을 생성합니다. 그런 다음 이 값은 함수(실제로는 네트워크 매개 변수 𝜃에 대한 전체 네트워크의 오차의 편도함수인 경사)가 근사화할 목표값으로 사용됩니다. 인공 신경망은 목표값을 추정 값과 출력 사이의 오차를 계산하여 그 오차를 역전파 및 최적화 알고리즘을 통해 감소시킴으로써 목표 값을 근사화합니다. 가장 일반적인 최적화 알고리즘은 확률적 경사 하강법의 변형인 것이며, 이를테면 확률적 경사 하강법이 대표적입니다.\n\n![강화 학습 소개](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_24.png)\n\n## 오프-폴리시 DQN\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n큐-러닝은 SARSA(State, Action, Reward, State', Action')의 오프-폴리시 버전으로, 다음 상태-액션 쌍 Q(s', a')은 다음 상태에서 이용 가능한 액션들 중에서 최대 예측 값으로 추정됩니다. 다시 말해, 큐-러닝은 다음 상태 s'에서 이용 가능한 액션들 사이에서 Q(s',a')의 최댓값을 선택합니다. 이는 정책(𝜋)을 사용하지 않고 Q(s',a')를 학습한다는 것을 의미합니다. 반면, SARSA는 이전 액션을 선택하고 다음 상태-액션 쌍인 Q(s',a')를 추정하는 온-폴리시 방법입니다. 이는 상태가 주어졌을 때 액션의 확률인 정책(𝜋)을 사용하여 Q-함수를 학습한다는 것을 의미합니다.\n\n딥 큐-러닝에서는 액션-가치 함수 Q(a, s)가 Q(a,s, 𝜃)로 표현되며, 여기서 𝜃은 신경망 매개변수를 나타냅니다. Theta(𝜃) 매개변수는 신경망에서의 가중치 w에 해당하며, 뉴런들 간의 연결에 관련되어 있습니다. 이 가중치는 연결의 강도를 결정하고, 오차를 최소화하기 위해 역전파를 통해 후방으로 조정됩니다. DQN은 환경의 고차원 표현을 입력으로 삼고, 각 가능한 액션에 대한 액션-가치 벡터를 출력합니다. 예상 수익은 일반적으로 MC 또는 TD 접근법을 통해 근사됩니다. 이후, 역전파와 최적화 함수를 사용하여 정책 기울기를 계산하고 정책 네트워크의 매개변수(𝜃)를 조정하여 오차를 줄입니다.\n\n인공신경망은 새로운 정보에 매우 민감하기 때문에, 새로운 정보가 이전에 작성된 정보를 덮어쓰는 치명적인 잊혀짐을 초래할 수 있습니다. 이러한 치명적인 잊혀짐을 다루는 방법 중 하나는 경험 재생을 적용하는 것입니다. 이 기법은 과거 경험을 저장하고 네트워크를 훈련하는 데 재사용합니다.\n\n## 온-폴리시 딥 TD(𝝀) \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nANNs는 TD(λ) 방법에도 적용할 수 있습니다. 여기서 상태 관찰은 ANN에 입력으로 공급되고, ANN은 그것을 통해 액션-가치 함수를 출력으로 근사화합니다. TD(λ)의 온-폴리시 성격 때문에 Deep TD(λ) 방법은 상태 간의 장기 의존성이 필요한 작업에 가장 적합합니다.\n\nTD(λ)와 같은 온라인 학습 방법을 훈련하는 것은 도전적일 수 있습니다. 왜냐하면 환경의 분포가 부트스트래핑으로 인해 매 단계마다 또는 n 단계마다 변경되기 때문이기 때문입니다. 이를 비정상성(nonstationarity)이라고 부르며, ANN 매개변수 𝜃가 최적으로 수렴하는 것을 방해합니다. 온라인 학습에서 연이어 발생하는 상태 간의 종속성은 치명적인 잊혀짐(catastrophic forgetting)을 발생시킬 수 있어, 업데이트가 과거 학습에 영향을 미칩니다. 더욱이, 과거 조치에 신용을 할당하는 자격 흔적 및 ANNs의 결합은 역전파 단계에서 추가적인 복잡성을 초래할 수 있습니다.\n\n위기에 대처하는 한 가지 방법은 경험 재생(experience replay) 기법을 활용하는 것입니다. 경험 재생은 학습된 에이전트 에피소드를 [s, a, r, s’]의 벡터로 메모리 버퍼에 저장합니다. 훈련 중에 네트워크는 저장된 학습 벡터의 메모리 버퍼에서 샘플을 추출하여 네트워크 매개변수를 업데이트합니다. 이를 통해 네트워크는 더 큰 안정성을 제공받고, 새로운 경험으로 인한 큰 오류나 단계 간의 시간 차이로 인한 치명적인 간섭에서 덜 영향을 받습니다.\n\nDeep TD(λ) 알고리즘은 상태 공간이 연속적이고 대상이 알려지지 않거나 불분명한 연속 제어 작업에서 뛰어난 성과를 보여주었습니다. 이러한 작업에는 로봇 과제, 자율 주행 자동차, 금융 시장의 연속 제어 작업 등이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 강화 학습과 인공 일반 지능\n\n강화 학습이 인공 일반 지능에 미칠 영향은 무엇인가요?\n\n\"지능\"이라는 것은 서로 다른 능력을 단일한 개념으로 결합하기 때문에 모호한 변수이지만, \"일반 지능\"은 생물의 진화된 능력들을 기반으로 하며, 생존과 번식을 위해 세계적인 정보를 변환하는 것이 요구된다. 심지어 인간의 맥락에서도 지능은 유기적인 생존 가능성의 윤곽에서 분리될 수 없다. 그러나 이것이 통용되는 견해는 아니다. 일반적인 지혜는 지능이 이용 가능한 정보를 기반으로 추론을 계산하는 프로그램 또는 소프트웨어와 유사하다고 주장한다.\n\n후자의 개념은 경쟁한다고 여겨지는 두 가지 모델로 구성되어 있는데, 하나는 절차를 따르는 지능으로 설명되고, 다른 하나는 최적의 예측을 위해 데이터로부터 일반화하는 지능으로 설명된다. 전자는 일반적으로 더 잘 이해되지만, 후자는 예측의 강도를 신뢰성 있게 향상시키는 기법 집합으로 이루어져 있다고 할 수 있다. 동물의 지능은 대부분 후자 모델에 기반한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 모델의 가장 성공적인 패러다임은 인공 신경망을 통한 딥 러닝입니다. 인공 신경망 구조의 주요 장점은 사전 정보나 개념 없이 데이터로부터 일반화를 가능하게 한다는 것입니다. 이는 비지도 학습과 혼동해서는 안됩니다. 인공 신경망은 먼저 훈련을 통해 모델을 구축한 다음 새로운 데이터에 대해 해당 모델을 기반으로 예측합니다. 따라서 두뇌도 (진화를 통한 사전 훈련을 고려하면) 비슷한 일을 한다고 생각됩니다. 그러나 현재 인공 신경망에서는 두 가지 약점이 있습니다. 첫 번째 약점은 목표나 결과가 인간 디자이너에 의해 설정되어야 한다는 것입니다. 인공 신경망은 스스로 목표를 설정할 수 없습니다. 또한, 스스로 진실과 거짓을 구별할 수 없습니다. 모델이 그 결과를 근사화하는 방법을 배우기 위해 참된 결과를 제공해야 합니다. 두 번째 약점은 강화 학습 없이 인공 신경망이 자체 상태를 최적화하기 위해 환경을 탐색할 수 없다는 것입니다. 이러한 이유로 인공 신경망의 일반화와 예측 능력을 강화 학습의 결정 최적화 능력과 결합하는 것이 엄청난 섞임을 만들어냅니다.\n\n이 기반 위에서 강화 학습이 인공 일반 지능으로 가는 가장 명확한 길을 대표한다고 주장한 사람들도 있습니다(Sutton, 2014). 이에 대한 직관은 분명합니다. 강화 학습은 생명체를 모델링하는 데 가장 가깝습니다. 이것이 다른 성공적인 구조들과 결합되면 (예를 들어 변환기와 같은), 모든 인간 능력을 복제하고 능가하는 AI 모델로 이어질 수 있습니다.\n\n그러나 만약 인간이 일반 지능의 기초라면, 일반 지능의 개념은 인지능력을 생존 제약과 어떤 형태의 구현체와 이혼시키지 않는 것일 수 없습니다. 반면에, 일반 지능을 생명 형태에 언급하지 않고 정의할 수 있다면, 그것이 어떤 모습인지는 분명하지 않습니다. 즉 순수한 추상 모델은 Marcus Hutter의 AIXI와 같은 시도에도 불구하고 만족스러운 형식화를 피해갑니다. 추상적으로는 논리적 추론과 계산 능력만으로 문제를 해결하는 완벽하게 합리적인 에이전트로 생각할 수 있습니다. 정보와 구현체 간의 격차는 이 기사의 범위를 넘어서는 더 큰 토론을 유발하는 것이며, 관심이 있다면 이 논문이 좋은 시작점을 제공합니다.\n\n그러나 강화 학습만으로는 인공 일반 지능을 충분히 표현할 수 있는지에 대한 의문이 있습니다. 이에 대한 이유로는 일반 지능의 정의에서 비롯한 것들이며 현재의 대부분의 AI 연구자들이 명시적인 내부 표현을 필수적인 요소로 간주하지 않고 있기 때문입니다. 이에 대한 상당한 이유가 있습니다. 딥 러닝의 성공 이전에 인공 일반지능의 희망을 걸고 있던 상징적 AI는 실패로 이어졌습니다. 상징적 AI는 주로 명시적으로 코딩된 논리 규칙과 최적 추론 생성을 위한 지식 저장소에 기초한 인공 지능 접근 방식을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상징적 인공지능과 신경망 사이의 긴장은 그러나 근거 없을 수도 있습니다. 많은 연구자들은 인공 일반 지능을 얻기 위한 탐구가 이러한 접근 방식을 적절히 결합하는 데 있다고 믿습니다. 신경망이 뇌의 본질적 오네톨로지를 근사화한다고 생각하는 이유는 수학적 논리가 뇌가 추론하는 방식이 아니기 때문입니다. 즉, 그것은 필요충분 조건을 계산하지 않거나 정확한 멤버십을 계산하는 것보다는 점수 있는 멤버십에 중점을 두며, 이는 퍼지 논리와 같은 방식에 의해 근사화되며 ANN(인공신경망)이 뛰어납니다.\n\n신경망은 원하는 출력을 달성하기 위해 파라미터화된 은닉층의 계층적 아키텍처로 구성되어 있으며, 고도로 보정된 동적 학습률, 활성화 함수, 연결 가중치 및 최적화 알고리즘을 통해 오차를 최소화하기 위해 보정된 학습율을 통해 원하는 출력을 달성합니다. 위와 같이 고도로 보정된 하이퍼파라미터를 넘어서 사람이 이해하지 못하는 정보가 은닉층에서 처리된다는 가정입니다. 정보가 이산적인 표현 단위(아날로그나 이미지 등)의 조합으로 저장되지 않으며 수십억 개의 뉴런들로 이루어진 분산 아키텍처로 저장된다는 것이 뇌의 경우와 같다는 가정입니다. 언어적으로 구조화된 생각이 고정적인 뉴런 조합으로 내부적으로 표현되는 것이 아닌데, 예를 들어 '존재 자체가 다른 존재를 위한 존재임을 하자’ 라는 문장 또는 단어를 나타내는 뉴런의 특정 조합이 없습니다.\n\n언어적 능력은 대신 경험에 의해 강화된 의미 연결과 재생규칙으로 내장된 거대한 네트워크에 포함되어 있습니다. 다시 말해, 우리가 반영적으로 문장을 작성하고 말할 때 언어와 생각이 뇌의 본질적 오네톨로지와 문법 간 이쾌적 매핑이 아닌, 연결의 정도와 연결 강도에 의해 특징을 가진 신경 접속의 분산 네트워크에 내재되어 있다는것을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 AI는 세계에서 자체적으로 추진되는 자율 시스템을 근사하지 않습니다. 또한 인간과 다른 동물들이 하는 방식으로 자체 환경적 환경을 생성하거나 자체 검색 공간을 재구성하지도 않습니다. 현재 이 제약이 없기 때문에 인간 디자이너는 AI의 정보적 중요성을 설정할 수 있습니다. 예를 들어 텍스트 생성, 환경 감지 등이 있습니다. 아키텍처가 진정한 문제 해결 기계로 진화하더라도 반성적 인식 능력이 없다면 일반적 지능을 보유하고 있다고 할 수 없습니다. 전통적으로 일반 지능의 정의에서는 인간 지능의 상징인 전체 인식의 변수를 생략합니다. 전통적인 지능 정의는 반사적이고 전체적 인식은 역공학 및 구성 요소의 분석에 대한 강한 저항을 가지고 있기 때문입니다. 그 이유로 반성적 인식은 지능의 구성 요소로서 배제됩니다. 그러나 현재의 과학적 설명에 대한 저항을 인정한다고 해도, 물리주의를 배제하거나 비자연론의 지지를 함축하지 않습니다. 오히려 이해력의 부재를 인정하는 신호일 뿐입니다. 이해력의 공방 속에, 반성적인 인식이 생명 유기체의 기본 속성인 감각의 확장임을 가설합니다. 이를 주장함으로써, 자연선택 이외의 방법을 통해 자율 시스템을 설계할 수 없다는 의미는 아니지만, 그들이 가까운 미래에도 과학적 분석에는 어려움을 줄 수 있다는 가능성은 여전히 열려 있습니다. 강화 학습이 일반적 지능으로 이어지길 바란다면, 에이전트는 세계의 복잡한 표현을 보유할 뿐만 아니라 그 표현의 내부에서 전반적인 관점을 유지할 수 있는 강력한 아키텍처를 사전으로 가져야 합니다. 이는 모델-세계 상호작용이 과제에 반드시 필수적이지만, 원조 아키텍처는 다중 모달 정보 처리와 통합 능력을 갖춘 복합적인 계층적 내부 구조를 요구할 것입니다.\n\n## 선택된 참고 자료\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., \u0026 Hassabis, D. (2015). Deep Reinforcement Learning을 통한 Human-level Control. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n\nNeftci, E. O., \u0026 Averbeck, B. B. (2019, March 4). 인공 및 생물학적 시스템에서 강화 학습. Nature News. https://www.nature.com/articles/s42256-019-0025-4\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSharma, S. (2024, March 7). Learning to Mix 𝑛-Step Returns: Generalizing 𝜆-Returns for Deep Reinforcement Learning. Ar5iv. [Link](https://ar5iv.labs.arxiv.org/html/1705.07445)\n\nSanghi, Nimish. Deep Reinforcement Learning with Python: With PYTORCH, Tensorflow, and Openai Gym. Apress, 2021.\n\nSilver, D., Singh, S., Precup, D., \u0026 Sutton, R. S. (2021). Reward is enough. Artificial Intelligence, 299, 103535. [Link](https://doi.org/10.1016/j.artint.2021.103535)\n\nSpens, E., \u0026 Burgess, N. (2024, January 19). A generative model of memory construction and consolidation. Nature News. [Link](https://www.nature.com/articles/s41562-023-01799-z)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSutton, Richard S. Introduction to Reinforcement Learning. MIT Press.\n\nTyng, C. M., Amin, H. U., Saad, M. N. M., \u0026 Malik, A. S. (2017, August 24). The influences of emotion on learning and memory. Frontiers in psychology. [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/)\n\nWhite, A., Modayil, J., \u0026 Sutton, R. (2014). Surprise and Curiosity for Big Data Robotics. Association for the Advancement of Artificial Intelligence, 19–22.","ogImage":{"url":"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png","tag":["Tech"],"readingTime":28},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e강화 학습의 기초에 대한 심층 탐구, 모델 기반 및 모델 없는 방법 포함\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003ch2\u003e강화 학습이란?\u003c/h2\u003e\n\u003cp\u003e공학 지능의 한 경로는 생물학적 생물체를 모방하는 것에 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e생물학적 생명체들은 환경으로부터 정보를 전도하고, 이를 처리(인지과학이 연구하는 바)하며, 생존에 유리한 행동을 출력합니다. 이러한 행동들은 가장 기본적인 수준에서 음식 수확, 번식, 위험 회피와 관련됩니다. 또한, 이는 놀이, 창의성, 문제 해결, 설계 및 공학, 사교, 로맨스, 지성 생활과 같은 다양한 인간 활동도 포함합니다.\u003c/p\u003e\n\u003cp\u003e그렇다면, 위의 모든 것을 수행할 수 있는 시스템을 어떻게 설계할까요?\u003c/p\u003e\n\u003cp\u003e만약 우리가 간단한 생물체를 어떤 환경의 함수로 모델링한다면, 우리는 에이전트, 환경의 모델, 그 에이전트를 현재 상태에서 원하는 상태로 이동시키는 함수가 필요할 것입니다.\u003c/p\u003e\n\u003cp\u003e심리학에서, 두 가지 주요 학파인 행동주의와 인지과학은 인간 행동을 설명하기 위해 양립하고 있습니다. 행동주의자는 학습 메커니즘의 함수로써 행동을 이해하며, 학습은 행동적 출력에 귀속될 수 있다고 합니다. 반면에, 인지과학은 환경과의 상호작용을 정보 처리 접근법을 통해 모델링합니다. 이 접근법에서, 에이전트는 외부 자극을 처음에는 감각을 통해 내부 표현으로 변환하고, 그 후 사고와 추론 능력에 이르기까지 변형 및 통합 과정을 거쳐 행동적 출력을 반환합니다. 전자 접근법에서는, 학습은 주로 환경적 조건부로서 이해됩니다. 반면에 후자에서는, 정신적 표현이 행동을 예측하는 데 필수적이라고 여겨집니다. 강화 학습은 주로 행동주의 접근법에서 영향을 받아, 환경적 보상이 에이전트의 탐색 공간 내에서 진화를 결정한다고 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e작업자 조건화, 1950년대-60년대에 유행했던 행동주의 심리학 학파로서, 학습을 보상과 처벌이라는 환경 메커니즘의 결과물로 정의했습니다. 조작적 조건화의 전제 조건으로는 에드워드 손다이크가 제안한 효과의 법칙이 포함되어 있습니다. 이 법칙은 만족스러운 효과를 일으키는 행동은 재발을 더 많이 유발하며, 불만족스러운 효과를 일으키는 행동은 재발을 덜 유발한다는 것을 제안합니다. B.F. 스키너는 효과를 강화와 처벌의 용어로 운용했습니다. 강화는 행동의 재발 발생 가능성을 증가시키며, 이것은 접근 또는 억제 요인의 제거를 말할 수 있습니다. 접근은 긍정적 강화, 회피의 역전인 부정적 강화로 표현됩니다. 긍정적 강화의 예로는 스포츠에서 뛰어나고 자주 이기는 것이 포함됩니다. 부정적 강화의 예는 억제적 자극을 제거하는 것인데, 이를 예로 들 수 있는 것은 경기 도중 당신을 조롱하는 학교 폭력가입니다. 작업자 조건화는 가장 큰 보상을 받는 행동을 반복할 가능성이 높다고 예측합니다. 반면 처벌은 행동 효과를 제어하기 위해 부정적 결과를 추가하거나 행동과 관련된 보상을 제거함으로써 구성됩니다. 파울링으로 게임에서 퇴장당했을 때의 경우는 긍정적 처벌을 보여줍니다. 성적이 좋지 않고 게임에서 패배한 경우는 부정적 처벌을 나타내며, 이는 미래에 더 이상 게임을 하지 않을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e인간 사회의 삶의 게임은 행동을 구성하는 보조적 강화나 사회적으로 구성된 보상과 처벌로 가득 차 있습니다. 이는 돈, 학점, 대학 입학 기준, 게임에서 이기고 지는 규칙과 같이 사회적 보상과 처벌을 포함합니다. 이러한 것들은 음식, 번식, 사회적 승인과 같은 생물학적 요구에 더 가까운 자연적 강화요인을 보완합니다.\u003c/p\u003e\n\u003cp\u003e기억은 이전 경험을 유지할 수 있도록 하는 학습에서 중요한 역할을 합니다. 증거에 따르면 기억은 경험의 콘텐츠보다는 보상과 처벌을 부호화합니다. 실험 대상은 보상을 받는 경험을 기억할 가능성이 더 높아지며, 따라서 이를 반복하는 경향이 있습니다. 부정적인 경험은 불리하게 기억될 가능성이 더 높아지며, 이를 피하려고 합니다. 기억 메커니즘은 복잡하고 다양하며, 실험 대상들이 기억을 회상함으로써 기억을 다시 구성함에 있어서 적극적인 역할을 하는 것으로 나타납니다. 이 사실은 행동주의에 대한 예측을 어렵게 만들며, 단독으로 조건화 원리에 근거한 예측을 하기 어렵게 만든다. 게다가 보상과 처벌은 긍정적과 부정적 영향의 풍경을 단순화하며, 이것은 복합한 골짜기와 웅덩이들, 중첩된 의존성으로 이루어진 복잡한 지형이며, 이는 이진 공간보다는 연속적 스펙트럼으로 더 잘 모델링됩니다.\u003c/p\u003e\n\u003cp\u003e불구하고, 강화 학습은 인공지능을 모델링하기 위해 에이전트, 환경 및 보상의 행동 온톨로지를 적응하는 다양한 수학적 기법으로 이루어져 있습니다. 아래에서 보게 되겠지만, 강화 학습의 측면 중 일부는 통제 이론에서 비롯되어 물리학과 공학으로 확장되는 전제 조건으로부터 나오며, 다른 측면은 심리학과 생물학으로부터 직접적으로 나오는 것입니다. 통제 이론의 대상과 생명체는 열역학적 균형으로부터 멀리 떨어진 최적 범위 내에 남아야 하는 동력학 시스템으로 구성되기 때문에, 기본 원리는 강화 학습과 인공지능의 목표에 부합됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다이내믹 프로그래밍은 주로 제어 이론에서 시작되어, 더 큰 문제를 하위 문제로 재귀적으로 분해하여 해결하는 수학적 최적화 방법으로 발전했습니다. 보통 재귀는 함수가 직접 또는 간접적으로 자기 자신을 파라미터로 전달하는 것을 말합니다.\u003c/p\u003e\n\u003cp\u003e본 글에서는 주로 동적 프로그래밍의 요소에 초점을 맞추며, 이를 이산적이고 유한한 게임에 초점을 맞출 것입니다. 그러나 동적 프로그래밍은 모델 없이 강화학습 접근 방식과 결합하여 해결되는 한계를 가지고 있으며, 이를 보완하기 위해 동적 프로그래밍과 인공 신경망을 결합한 방법이 있습니다. 이는 한 때 신경동적 프로그래밍이라 불렸습니다. 보다 넓게는 강화학습과 인공 신경망의 결합을 딥 강화학습이라고 합니다. 이러한 모델은 강화학습 기법 내에서 딥 러닝의 강점을 통합하고 있습니다. 이러한 알고리즘 가운데 가장 인기 있는 것은 2013년 DeepMind에 의해 소개된 딥 Q-네트워크(DQN)입니다. 이 알고리즘 계열은 Q 함수를 근사화하기 위해 딥 러닝을 활용합니다. Q 함수의 근사화가 강화학습의 한 약점 중 하나이므로, 이러한 알고리즘들은 강화학습 패러다임의 주요 개선점을 제공합니다.\u003c/p\u003e\n\u003cp\u003eDQN이 해결한 다른 약점에는 비선형 동역학을 캡처하는 유연성 부여, 차원의 저주로 인해 계산적으로 처리하기 어려워지는 일반적 범위의 차원을 수용하는 능력, 그리고 환경에 대한 보다 큰 일반화 능력이 포함됩니다.\u003c/p\u003e\n\u003cp\u003e신경동적 프로그래밍은 순수히 행동주의 접근 방식의 약점을 해결하기 위해 심리학의 인지 패러다임을 활용하는 방향으로 발전하고 있습니다. 그러나 하위 수준 지각 정보의 계층적 구조와 처리에 대한 과학적 진전이 이루어지는 반면, 그 정보를 생각과 의식에 연결시키는 데는 더 많은 노력이 필요하며, 이는 과학적으로 약간 불가능한 것으로 남아 있습니다. 이러한 이유로 인공 신경망(ANNs)은 아직까지 사람의 지능의 복잡한 일반화 능력을 갖추고 있지 않습니다. 이는 ANNs보다 지수적으로 적은 샘플로 학습하는 인간의 지능과 대조적입니다. 본 글의 마지막 섹션에서 강화학습의 원칙을 인공 일반 지능(AGI) 쪽으로 채택하는 시사점을 논의하겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e의사 결정 이론 및 제어 이론\u003c/h2\u003e\n\u003cp\u003e동적 프로그래밍과 강화 학습의 수학적 요소에 깊이 파고들기 전에, 철학적이고 수학적인 의사 결정 이론과 강화 학습 간의 관계를 명확히해야 합니다. 의사 결정 이론은 주로 합리적 선택 이론의 수학적 형식화로 구성되어 있지만, 강화 학습의 목표와 겹치는 부분이 있습니다. 강화 학습은 복잡한 환경과 정보 환경과 상호작용할 수 있는 성공적인 인공 에이전트로의 모델을 구축하려고 합니다.\u003c/p\u003e\n\u003cp\u003e의사 결정 이론, 또는 선택 이론으로도 알려진 이론은 20세기에 이상적인 이유의 형식화가 짙어진 가운데 발전하였습니다. 구체적으로, 에이전트의 행위 확률을 그들의 선호도를 고려하여 양적화하기 위해 확률 이론을 사용합니다. 이 형식화 노력의 꼭대기는 폰 노이만-모건슈턴 유틸리티 절차였습니다. 요약하자면, 이 절차는 에이전트가 유틸리티 기대치에 따라 최대 이익을 가져다주는 행동을 선택하는 경향이 있음을 설명합니다.\u003c/p\u003e\n\u003cp\u003e제어 이론은 기계 및 전기 공학 분야에서 나타나며, 동적 시스템의 상태 및 성능을 원하는 매개변수에 대해 최적화하는 데 관심이 있습니다. 중요한 메커니즘은 희망 변수를 측정하고 설정점과 비교한 후 그 차이를 수정을 위한 피드백으로 전달하는 컨트롤러로 이루어져 있습니다. 제어 이론의 큰 그림은 생명체의 대사 과정과 유사하며, 외부 변수 조건에 대비해 내부 온도의 설정 점을 유지하는 생물들의 과정을 반영합니다. 제어 이론과 의사 결정 이론의 연결은 명백합니다: 둘 다 시스템의 상태를 최적화하거나 발전시키기 위해 환경으로부터의 피드백에 의존합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e수학적으로, 제어 및 의사 결정 문제의 부분 집합은 모두 동적 프로그래밍을 통해 해결할 수 있는 최적화 문제로 축소될 수 있습니다. 동적 프로그래밍은 상태 변수의 수가 지수적으로 증가함에 따라 계산 요구 사항이 지수적으로 증가하는 차원의 저주에 시달린 일반적인 확률적 최적 제어 문제를 해결하기 위해 그것을 더 작은 하위 문제로 분해하고 가치 함수를 계산함으로써 해결합니다. 저희는 강화 학습의 기본 원칙을 시연하면서, 동적 프로그래밍의 핵심인 에이전트의 상태 및 가치 함수 사이의 재귀적 관계에 대해 깊이 파헤쳐볼 것입니다.\u003c/p\u003e\n\u003cp\u003e강화 학습과 의사 결정 이론은 보상 또는 유틸리티를 극대화하기 위한 절차를 정의하는 부분에서 겹칩니다. 그러나 의사 결정 이론에서는 유틸리티가 명시적으로 정의되지만, 경제 행동을 모델링하려는 것인 반면, 강화 학습에서는 유틸리티가 누적 보상으로 대체됩니다. 서로 다른 작업 목표에 대한 다른 정책을 적용하여 누적 보상을 극대화할 수 있으며, 탐구와 개발의 극성 방향 간의 상호 관계에 따라 달라집니다. 우리가 볼 것처럼, 탐색과 개발의 상호 관계를 탐색하는 것으로 표현되는 탐사-개발 딜레마에 따라 누적 보상을 극대화하는 것이 달라집니다.\u003c/p\u003e\n\u003cp\u003e강화 모델의 기반이 되는 온톨로지를 개요화하는 것으로 시작해 봅시다.\u003c/p\u003e\n\u003ch2\u003e상태, 동작 및 보상\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e강화 학습은 의사 결정 이론의 이론적 장치를 활용하여 에이전트, 환경 및 동적 진화 규칙을 포함하는 모델을 구성합니다. 진화 규칙은 에이전트가 환경 내에서 보상을 추구할 수 있게 허용하며, 이를 관찰이라고도 합니다.\u003c/p\u003e\n\u003cp\u003e에이전트는 환경으로부터 결정까지의 출력으로 정의됩니다. 특정 결정을 행동이라고 합니다. 네트워크의 현재 상태에서 행동으로의 매핑을 정책이라고 합니다. 정책은 상태에서 결과로의 매핑으로서 행동을 안내합니다.\u003c/p\u003e\n\u003cp\u003e따라서 형식적으로 정책은 상태를 행동으로 매핑하는 함수입니다. 현재 상태가 주어졌을 때 행동의 조건부 확률로 나타낼 수 있으며, 여기서 그리스 문자 𝛑은 정책을 나타냅니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_1.png\" alt=\"정책\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e전이 역학은 모든 가능한 상태 및 보상 값에 대한 확률 분포로 주어진 입력 보상에 따라 다음 상태를 정의합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e위의 공식은 다음 상태와 보상 쌍의 확률을 현재 상태 s와 행동 a가 주어졌을 때 다음 상태 s'와 보상 r의 조건부 확률과 같다고 정의합니다.\u003c/p\u003e\n\u003cp\u003e행동은 보상을 누적하여 환경을 변경합니다. 그 결과로 보상은 에이전트 상태나 관측을 변경합니다. 보상 입력은 정책에 기반하여 미래의 행동 출력을 결정합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e일반적으로, 정책에는 두 가지 유형이 있습니다:\u003c/p\u003e\n\u003cp\u003e보상은 일반적으로 스칼라 값 x로 형식화됩니다.\u003c/p\u003e\n\u003cp\u003e특정 보상이 주어지면, 에이전트는 최적화 딜레마에 직면합니다: 에이전트는 단기 보상을 극대화해야 하는지, 아니면 완전한 생생력 기록을 통해 누적 보상을 극대화해야 하는지를 결정해야 합니다.\u003c/p\u003e\n\u003cp\u003e이것은 탐색-이용 딜레마로 알려져 있습니다. 다시 말해, 전이 함수는 환경을 탐색하고 누적한 지식을 활용하여 최대 보상을 얻으며, 그 둘 사이의 균형을 최적화해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e탐색-활용 딜레마에 대한 최적의 해결책은 모델이 학습해야 하는 작업의 유형에 따라 달라집니다. 이 작업은 유한에서 무한(연속적 또는 이산적으로)로 범위가 있을 수 있습니다. 예를 들어 체스 게임은 에피소드 작업으로 형식화될 수 있습니다. 왜냐하면 유한한 구성 공간과 승, 패, 무승부 세 가지 가능한 결과를 가진 미리 정의된 종료 상태가 있기 때문입니다. 이는 현재 상태를 기준으로 최적의 후속 상태를 결정할 수 있는 것을 의미하며, 결정론적 전이 동역학을 통해 계산됩니다. 따라서 각 상태에 대해 단일 최적의 행동이 존재합니다.\u003c/p\u003e\n\u003cp\u003e그러나 대부분의 작업은 유한한 구성 공간이나 미리 정의된 종료 상태를 가지고 있지 않습니다. 우리는 이러한 것들을 연속적인 작업으로 분류하고, 모델이 없는 방법을 통해 최적화합니다. 모델 없는 방법론에서는 전환 동역학을 계산하는 대신 모델이 환경에서 샘플링하여 최적의 후속 상태를 계산합니다. 다르게 말하면, 선견지명을 통해 행동을 계획하는 대신 환경에 대해 배우기 위해 시행착오를 사용합니다.\u003c/p\u003e\n\u003cp\u003e모델 없이 강화 학습하는 두 가지 접근법이 일반적으로 있습니다: 몬테 카를로 접근법과 시간차 학습. 충분한 샘플의 평균이 기대값으로 수렴하기 때문에, 모델 없는 방법은 샘플 평균을 통해 예상값을 추정합니다. 몬테 카를로 방법은 충분히 큰 상태-행동 쌍의 샘플로 예상 누적 반환을 추정하여 가치 함수를 계산합니다. 일부 몬테 카를로 방법은 에피소드 작업의 끝에서만 가치 함수를 평가합니다. 연속적인 작업에서는 에피소드의 정의가 다양하게 변할 수 있고, 디자이너에 따라 시간 간격에 따라 설정할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e몬테 카를로 탐색과 반대로 시간차 학습은 시간 단계 간의 차이를 활용하여 가치 함수를 증분적으로 추정합니다. 시간차 방법의 접근 방식을 두면 몬테 카를로 방법에 비해 실제 예상 값과의 분산을 낮추는 특성이 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e요약하자면: 에이전트는 현재 상태와 액션 공간 쌍에서 상태 공간으로의 매핑을 통해 환경을 탐색합니다. 전이 동적은 미리 정의된 종단 상태를 가진 유한한 구성 공간에 대한 모든 가능한 매핑을 계산합니다. 미리 정의된 종단 상태와 유한한 상태 공간 대신에, 모델 무작위 접근법은 최상의 정책을 찾기 위해 환경에서 계속 샘플링합니다.\u003c/p\u003e\n\u003cp\u003e동적 프로그래밍은 모든 상태-액션 쌍에서 상태 전이 확률과 예상 보상을 계산합니다. 이 프로세스가 어떻게 작동하는지 이해하기 위해서는, 마르코프 프로세스를 이해해야 합니다.\u003c/p\u003e\n\u003cp\u003e다음에는 에이전트가 최적 후속 상태를 계산할 수 있도록 하는 수학적 모델을 배우게 됩니다. 앞서 논의한 대로, 최적성은 탐사-이용 딜레마로 이어지며, 이는 모델링하려는 작업 유형에 따라 다릅니다. 보상 구조를 자세히 살펴봄으로써 이를 더 잘 이해할 수 있을 겁니다.\u003c/p\u003e\n\u003ch2\u003e보상 평가\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e강화 학습에서 보상을 측정하는 방법은 에이전트가 행동을 취함으로써 환경으로부터 얻는 스칼라 값으로 계량화됩니다. 이 보상의 가치는 행동의 즉각적인 선호도를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e반면에 누적 보상 또는 반환은 해당 시점까지 환경으로부터 누적된 모든 보상의 합을 나타냅니다. 에이전트의 목표는 단순히 즉각적 보상을 최적화하는 것이 아니라 누적 보상을 최적화하는 것입니다. 전자는 근시적 에이전트를 나타내며, 후자는 장기간 수익을 극대화하려는 장기 노력 에이전트를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e대부분의 경우 에이전트가 가장 높은 보상을 최대한 빨리 극대화하길 원하기 때문에 할인은 현재 최대 보상을 나중에 최대 보상보다 우선시하는 방식으로 도입됩니다.\u003c/p\u003e\n\u003cp\u003e할인을 적용한 누적 보상 G는 아래 식으로 표현됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_3.png\" alt=\"Reinforcement Learning Introduction\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 누적 보상 G는 보상과 해당 할인 계수 감마 𝜸의 곱의 합과 같습니다. 감마는 항상 0과 1 사이의 값인 '0,1'입니다. 감마는 각 시간 단계마다 지수적으로 증가되므로 무한한 시간 단계를 통해 감마가 0에 접근합니다.\u003c/p\u003e\n\u003cp\u003e감마가 0에 접근할수록 단기 이익을 장려하고, 감마가 1에 가까워지면 무한한 반복을 통해 보상 합이 자체적으로 무한에 접근하므로 장기 이익을 장려합니다.\u003c/p\u003e\n\u003cp\u003e대부분의 작업은 시간에 제한이 있기 때문에, 감마 할인은 값이 1 미만일 때 보상에 상한선을 부과합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e할인을 고려한 누적 보상의 압축된 방정식은 아래와 같습니다. 여기서 G는 보상 R의 예상 합을 나타내며, 이는 할인 요소 감마로 곱해집니다. 따라서 누적 보상은 보상과 할인 요소의 합으로 계산됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_4.png\" alt=\"cumulative reward equation\"\u003e\u003c/p\u003e\n\u003ch2\u003e마르코프 의사결정 과정 (MDP)\u003c/h2\u003e\n\u003cp\u003e지금까지 정책을 상태에서 행동으로 매핑하는 확률적 정의, 보상이 주어졌을 때 한 상태에서 다른 상태로 움직일 확률인 전이 역학, 그리고 보상이 어떻게 계산되는지에 대한 공식에 대해 논의해 왔습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e자, 이제 조금 물러나서 확률적 전이 체인을 정의하는 보충 이론을 제공하겠습니다. 먼저 마르코프 과정이라고 하는 것부터 시작해봅시다. 마르코프 과정은 마르코프 성질을 만족하는 확률 과정입니다. 확률 과정은 무작위로 변하는 과정입니다. 마르코프 성질은 모든 상태에 대해 후속 상태가 현재 상태에만 의존된다는 것을 말합니다.\u003c/p\u003e\n\u003cp\u003e과거 상태가 미래 상태에 영향을 미치지 않기 때문에 마르코프 성질을 만족하는 과정을 메모리리스라고 합니다. 집을 나가서 일하러 나가 다시 집으로 돌아오는 매일 재발되는 일정된 목적지 집합을 상상해보세요. 즉, 시작과 끝이 있는 순환 과정이 있습니다. 이제 더 나아가서 다음 목적지로 움직일 결정이 현재 목적지에만 의존한다고 상상해보세요. 처음에는 각 연결된 목적지가 동일한 확률 분포를 갖게 될 것입니다. 예를 들어, 집을 나가면 운전하거나 지하철을 탈 수 있는 선택지가 있다면, 두 가능한 미래 상태에 대한 초기 확률을 각각 0.5로 정할 수 있습니다. 모든 가능한 경로의 반복을 통해 이러한 확률은 어떤 경로가 다른 경로보다 선호되는 빈도 분포로 안정화될 수 있습니다. (이 유형의 확률을 경험적 확률이라고 부르며, 가능한 사건에 대한 결과를 한정된 테스트 수에 대해 평균화합니다) 그 분포 평형은 마르코프 체인 또는 과정이 될 것입니다.\u003c/p\u003e\n\u003cp\u003e이제 아마도 생각 중일 것입니다: 어떻게 사건과 상태를 정의하나요? 고정된 가능한 상태와 안정한 확률 분포에 대해 얘기하려면 세상이 너무 복잡하지 않나요?\u003c/p\u003e\n\u003cp\u003e매우 그렇습니다. 그러나 우리는 환경 속 요소들의 수학적 형식론을 원하기 때문에 모델링하려는 작업 또는 환경 유형을 구별해야 합니다. 이를 위해 시간 단계와 상태 공간의 표현, 즉 모든 가능한 상태의 분포를 명시해야 합니다. 아래의 정사각 행렬은 상태 공간과 시간의 축을 기준으로 마르코프 체인의 정의를 제공합니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e상태 공간은 셀 수 있는/유한한 상태거나 연속적일 수 있습니다. 유한 상태 공간은 시스템의 모든 가능한 구성을 조합 이론을 통해 설명하고, 연속 상태 공간은 연속 함수를 통해 모든 가능한 구성을 설명합니다.\u003c/p\u003e\n\u003cp\u003e유한 및 가산 무한 공간은 측정 가능한 공간으로 정수 또는 유리수를 취하며, 연속 공간은 실수를 취합니다.\u003c/p\u003e\n\u003cp\u003e마찬가지로 시간 축은 이산 또는 연속적으로 정의될 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이산시간 프로세스는 이산적으로 상태 전이를 계산하지만, 가산 또는 비가산 상태 공간에서 모델링할 수 있습니다. 여기서 비가산이라 함은 실수의 무한한 10진 확장을 의미합니다. 실제로 컴퓨터가 시간을 세는 방식도 이와 같습니다. 이를 이산 단계로 처리합니다. 단계 사이의 간격은 아키텍처에 따라 다르지만, 주기는 보통 레지스터 상태를 변경하는 데 필요한 시간 단계의 길이로 측정됩니다.\u003c/p\u003e\n\u003cp\u003e연속시간 체인은 연속으로 상태 전이를 계산하며, 가산 또는 비가산 상태 공간에 모델링될 수 있습니다.\u003c/p\u003e\n\u003cp\u003e마르코프 프로세스라는 용어는 일반적으로 연속시간 프로세스에 사용되며, 마르코프 체인이라는 용어는 이 중 일부인 것을 나타냅니다: 이산시간, 확률적 제어 프로세스입니다. 이 기사에서는 이산시간, 유한 상태 공간에 초점을 맞출 것입니다.\u003c/p\u003e\n\u003cp\u003e지금까지 우리의 마르코프 체인은 상태간 전이를 고정된 확률로 설명하는 매우 단순한 모델입니다. 행동과 보상이라는 모델링에 중요한 두 가지 요소가 빠져 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e보상을 전이 확률로 할당하는 것이 마르코프 보상 과정입니다. 마르코프 보상 과정은 각 전이 상태에 보상을 할당합니다(양수 또는 음수 정수로 정의됨)으로써 시스템을 원하는 상태로 이끕니다. 누적 보상 공식을 상기해 보겠습니다. 기대 보상의 합에 일정한 할인 계수가 곱해진 값입니다. 마르코프 보상 과정을 사용하면 초기 상태 S가 주어졌을 때 상태 v(s)의 값과 누적 보상 G의 확률을 계산할 수 있습니다(여기서 G는 많은 반복 시행에서 평균화된 값입니다):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_6.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e마르코프 결정 과정으로 나아가기 위해 필요한 마지막 변수는 행동입니다. 에이전트는 가능한 행동 집합에 대해 동등하게 분포된 확률로 시작하고 이후에 전이 함수를 업데이트하여 현재 상태와 행동을 다음 상태와 보상으로 매핑합니다. 이렇게 하면 앞서 설명한 전이 동학에 다시 도달하게 됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e동적 계획법 및 벨만 최적성\u003c/h2\u003e\n\u003cp\u003e이것은 벨만(1957)에 의해 개발된 동적 프로그래밍의 개념으로 이어집니다.\u003c/p\u003e\n\u003cp\u003e동적 프로그래밍을 이해하면, 동적 프로그래밍과 같은 완벽한 환경 지식이 필요하지 않는 근사 방법인 몬테카를로 탐색 및 시간차이 메소드도 이해할 수 있습니다. 이러한 모델-프리 방법은 완벽한 정보 대신 동적 프로그래밍의 결정적 정책을 근사화합니다. 따라서, 실제 세계 학습을 근사화하는 강력한 메커니즘이 제공됩니다.\u003c/p\u003e\n\u003cp\u003e동적 프로그래밍이 최적의 에이전트 상태를 검색하고 찾는 핵심 아이디어는 상태 가치 함수와 행동 가치 함수 사이의 관계에 있습니다. 이들은 재귀적으로 관련되어 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이러한 아이디어를 관련성 있는 예시로 확장해 봅시다. 예를 들어, 당신이 삶에서 최적 상태가 아니고 이를 바꾸고 싶다고 가정해 봅시다. 미래에 이루고 싶은 구체적인 목표나 위치가 있다고 해 봅시다. 이 큰 목표에 도달하기 위해 (더 좋은 직장을 얻는다던가, 가족을 꾸린다던가 등을 대체할 수 있습니다), 당신은 원하는 결과에 도움이 되는 일련의 작은 단계나 행동을 취해야 할 것입니다. 강화 학습의 언어로 번역하면, 현재 상태에는 특정 가치가 할당될 것입니다. 현재 상태와 가치를 고려하여 당신은 행동을 취할 것입니다. 이러한 행동은 전체 목표와 현재 상태에 따라 평가될 것입니다. 좋은 행동은 나쁜 행동보다 높은 가치를 받을 것입니다. 환경으로부터의 피드백은 행동의 가치를 결정할 것입니다 (이 값들이 어떻게 결정되는지는 작업마다 다릅니다). 상태의 평가는 사용 가능한 행동과 후속 상태의 가치에 영향을 미칠 것입니다. 그리고 행동의 평가는 현재 상태의 가치를 재귀적으로 영향을 줄 것입니다. 다시 말해, 행동과 상태는 재귀적으로 연결되어 있습니다.\u003c/p\u003e\n\u003cp\u003e이제 현실에서, 당신의 목표와 그 목표에 이르는 행동 단계들은 이산 시간 단계 및 이산 상태 공간을 갖는 결정론적 시스템으로 명시할 수 없습니다 (비록 이 방식으로 근사화할 수도 있습니다). 대신, 동적 프로그래밍은 체스와 같은 게임처럼 정의 가능한 환경을 가정합니다. 여기서 시간 단계와 행동 공간이 이산적이고 유한하게 추상화됩니다. 현실과의 중요한 점은 더 큰 목표가 해당 큰 목표에 유리한 작은 부목표를 최적화함으로써 다가올 것이라는 점입니다.\u003c/p\u003e\n\u003cp\u003e따라서 동적 프로그래밍은 다음 값들을 가정할 것입니다: (Ω, A, 𝒫), 여기서 Ω는 모든 가능한 상태의 합을 나타냅니다, A는 유한 샘플 공간의 부분집합인 행동 이벤트를 나타내며, P는 일정 정책 함수 𝝅에 의해 각 행동 이벤트에 할당된 확률을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e이제 우리가 결정론적 전이 역학에 대해 생각해 보면, 상태, 행동 및 보상의 집합이 유한하기 때문에, 특정 상태와 보상 쌍은 일정한 상태 및 행동 쌍이 주어졌을 때 그 값들이 발생할 확률을 갖게 될 것입니다. 이러한 확률은 상태 공간이 이산적이기 때문에 이산 확률 분포로 명시됩니다. 우리는 상태, 행동 및 보상으로 구성된 일련의 순서가 누적 보상을 최대화하려는 마르코프 결정 과정(MDPs)이라고 했습니다. 이때 보상을 스칼라 값으로 표현하며, 시간이 흐름에 따라 예상되는 누적 보상을 최대화하려고 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 다루어야 할 질문은 우리가 지정한 가정에 따라 마르코프 의사결정 프로세스가 누적 보상을 최대화하는 방법입니다. 이 답은 벨만 최적 방정식에 의해 제공되며 두 함수인 상태 가치 함수와 행동 가치 함수 사이의 관계를 설명합니다.\u003c/p\u003e\n\u003ch2\u003e상태 가치 함수\u003c/h2\u003e\n\u003cp\u003e상태 가치 함수는 에이전트가 정책 𝝅에 따라 취할 수 있는 모든 가능한 조치의 확률의 합으로 정의될 수 있습니다. 각 조치에 대해 가능한 후속 상태의 가중치 값의 합으로 그 가치가 결정됩니다.\u003c/p\u003e\n\u003cp\u003e보다 간단하게 말하자면, 상태 가치 함수는 특정 상태(s)에서 정책 𝝅을 따라 시작하여 에이전트가 얻을 수 있는 예상 누적 보상을 정의합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_8.png\" alt=\"equation_8\"\u003e\u003c/p\u003e\n\u003cp\u003e위의 방정식은 두 항으로 구성되어 있습니다: a) 정책 (𝝅)을 따라 상태 (s)에서 에이전트가 취할 수 있는 모든 가능한 조치들의 확률의 합, 그리고 b) 각 가능한 조치마다 가능한 후속 상태의 가중치 값을 계산하는 내부 합계입니다. 대괄호 내의 항은 각 조치의 가능한 상태의 기여도를 즉각적 보상 R(s, a, s’)의 합과 감마 요소 𝛾에 의한 할인된 보상의 합으로 계산합니다.\u003c/p\u003e\n\u003cp\u003e상태-가치 함수를 표현하는 다른 방법은 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_9.png\" alt=\"equation_9\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 공식은 다음 상태의 가치를 조건부 확률로 계산된 예상 반환 E𝝅로 정의합니다. 시간 t에서 상태 s가 주어졌을 때 시간 t에서 보상 R을 받을 조건부 확률로 계산됩니다. 보상 R은 후속 상태의 예상 반환의 곱의 합과 감마 감쇠를 고려하여 계산됩니다.\u003c/p\u003e\n\u003cp\u003e더 잘 이해하기 위해 3 x 3 그리드 월드의 에이전트를 상상해보세요. 각 시간 단계마다 상, 하, 오른쪽, 왼쪽 네 가지의 가능한 조치가 사용 가능합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_10.png\" alt=\"grid\"\u003e\u003c/p\u003e\n\u003cp\u003e우리는 상태 가치를 0으로 초기화하고, 상태 가치 함수에 대한 벨만 방정식을 사용하여 그리드 내의 보상 분포가 주어졌을 때 상태 가치를 최적화합니다. 우리는 (행, 열) 색인을 사용하여 그리드의 각 위치를 식별합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 테이블 태그를 마크다운 형식으로 변경해주세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eAction-Value Function\u003c/h2\u003e\n\u003cp\u003e우리는 행동-가치 함수가 상태-가치 함수의 두 번째 항목으로 내장되어 있다는 것을 보았습니다. 이는 행동-가치 함수가 상태 (s)에서 가능한 모든 행동의 가치를 계산한다는 것을 의미합니다. (s)에서 (s')로의 전이로부터 얻은 즉각적인 보상의 합과 다음 상태 (s')에서의 예상 누적 보상을 고려하여 주어진 작업으로부터 계산됩니다.\u003c/p\u003e\n\u003cp\u003e다시 말해, 행동 가치 함수는 상태 (s)에서 작업 (a)를 수행하는 것에 대한 누적 보상을 계산합니다. 여기서 기대 수익은 즉각적인 상태 전이 — R(s, a, s')로 표시됨 — 및 다음 상태 s'의 누적 보상의 할인 가치 —𝛾∑𝝅(a'|s')Q(s',a')​​로 표시됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e행동 가치 함수를 정하는 또 다른 방법은 최적 정책 𝝅을 따라 상태와 행동 쌍 (s, a)이 주어졌을 때 기대 반환값 E로 나타내는 것입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_14.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e상태 가치 함수와 행동 가치 함수는 상태 가치 함수가 정책과 행동 가치 함수 Q(s, a)로 구할 수 있다는 관점에서 관련이 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_15.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e따라서 행동-가치 함수와 상태-가치 함수는 재귀적으로 관련이 있습니다: 행동-상태 쌍의 가치가 상태의 가치를 결정하며, 상태는 반대로 행동의 가치를 결정합니다.\u003c/p\u003e\n\u003cp\u003e상태-가치 함수는 상태를 우선으로 하고 기대값 E를 출력합니다. 행동 가치 함수는 상태와 행동 쌍을 우선으로 하여 보상을 계산하고 기대 누적 반환 E를 얻습니다.\u003c/p\u003e\n\u003cp\u003e따라서 벨만 최적 방정식은 상태-가치와 행동-가치 함수의 재귀적 반복을 나타내며, 최적 값에 수렴할 때까지 반복됩니다. 상태-가치 함수를 위한 벨만 방정식은 아래와 같이 표현됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_16.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e현재 상태의 값은 가능한 모든 행동의 최대 보상으로 정의되며, 이는 (s) 상태에서 행동 a를 취할 때 얻는 보상과 다음 행동 s'의 값 및 할인 계수 감마의 곱으로 계산됩니다.\u003c/p\u003e\n\u003cp\u003e벨만 방정식은 현재 상태에서 모든 가능한 행동을 평균화하고 발생 확률에 따라 가중치를 부여합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_17.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e모델 프리 메소드: 몬테카를로 \u0026#x26; 시간차학습\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 예시는 전환 동역학이 알려져 있어 따라서 완벽하게 계산될 수 있는 결정론적 모델을 설명합니다. 이것은 환경에 대한 완전한 지식을 가지고 있기 때문입니다.\u003c/p\u003e\n\u003cp\u003e그러나 대부분의 작업에서는 환경에 대해 완전한 지식을 갖고 있지 않습니다. 이 정보 대신에 우리는 동적 프로그래밍 방정식을 해결할 수 없기 때문에 정확한 결정론적 전환 동역학으로 진행할 수 없습니다. 이 문제를 극복하기 위해 통계에서 빌려온 기술을 사용하여 환경의 상태를 샘플에서 추론할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eMonte Carlo 방법론에서는 예상 수익을 샘플 수익의 평균으로 근사화합니다. 샘플이 무한대로 접근함에 따라 평균 수익이 예상 수익의 실제 값으로 수렴합니다. 에이전트가 종료될 때까지 전체 에피소드를 실행한 다음 가치 함수를 계산하는 방식으로 이를 수행합니다. 그런 다음 N개의 에피소드 샘플을 취하여 평균을 사용하여 대상 상태의 예상 가치를 근사화합니다. 지금까지 궁금해 하고 계실 수 있듯이, 에피소드가 어떻게 정의되는지는 작업과 모델의 목적에 따라 달라집니다. 예를 들어, 체스 게임에서는 전체 게임을 실행하거나 임의의 단계 시리즈를 에피소드로 정의할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eMC 업데이트 규칙을 다음과 같이 작성할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_18.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eV(s) n+1은 다음 에피소드의 가치를 나타내며, S(s)n은 상태의 누적 가치를 나타내고 G는 보상의 가치를 나타냅니다. 누적 보상 G를 상태 값에 추가하고 에피소드 또는 샘플의 수로 나눕니다.\u003c/p\u003e\n\u003cp\u003e우리는 MC 업데이트 규칙을 대수적으로 재배치할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_19.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eMonte Carlo 방법과는 달리 Temporal Difference (TD)에서는 각 에피소드가 끝난 후가 아니라 각 시간 단계나 증분마다 상태 가치 함수를 평가합니다. 환경에 대한 정보가 없는 초기 상태에서는 상태의 가치 V(s)를 0이나 다른 값으로 초기화해야 하며, 이후 매 시간 단계마다 업데이트됩니다.\u003c/p\u003e\n\u003cp\u003eTD에서는 상태의 가치를 계산하는 데 두 단계가 필요합니다. 먼저 한 단계의 오차를 계산한 다음 업데이트 규칙을 적용하여 상태의 가치를 변경합니다. 오차는 다음과 같은 차이 공식으로 주어집니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_20.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 𝜹t는 오차를 나타내며, R(t+1)은 행동에서 얻는 보상, V(S t+1)은 다음 상태의 추정 가치, V(S)는 현재 상태의 가치를 의미합니다. TD가 다음 상태의 추정 가치를 사용하여 현재 상태를 평가하는 것을 부트스트랩이라고 합니다. 이를 통해 현재 상태의 값에서 행동의 보상과 할인 계수와 다음 상태의 가치의 곱을 더하고 빼는 것으로 상태의 값이 즉시 시간 단계마다 업데이트됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e예상 보상과 실제 관측 값 사이의 차이를 𝜹(탐색-백업 간격)을 𝛼(학습률)에 곱해주면 관측과 기대 사이의 차이를 줄일 수 있어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_21.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e𝛼의 역할은 TD 알고리즘의 학습 정도를 결정하는데, 𝛼는 실수형양수값이에요. 일반적으로 𝛼는 0.1, 0.01, 0.001 같은 값으로 설정돼요. 높은 𝛼 값은 업데이트를 더 적극적으로 진행하도록 해주고, 낮은 값은 보수적인 업데이트를 보장해요. 𝛼의 값은 탐험-활용 균형에 영향을 미치는데, 더 높은 𝛼는 탐험을 선호하고, 낮은 𝛼는 활용을 선호하게 됩니다.\u003c/p\u003e\n\u003cp\u003eMC와 TD 방법은 환경에 대한 사전 지식 없이 진행되지만, Temporal Difference의 장점은 매 시간 단계에서 온라인 업데이트를 계산한다는 것이고, 몬테카를로 방법의 장점은 값 추정을 위해 샘플링에만 의존하여 편향이 없다는 것이에요. TD 방법의 단점은 높은 편향이고, MC 방법의 단점은 중요한 업데이트를 간과하여 높은 분산을 유발한다는 점이에요. 이것은 두 학습 전략 사이의 최적점이 어딘가에서 존재해야 한다는 것을 시사합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eTD 방법론은 단계 평가 전략을 n단계로 변경함으로써 최적화할 수 있습니다. 이렇게 함으로써, TD와 MC 사이에서 타협할 수 있는 기회를 얻게 됩니다. 우리가 n단계마다 상태 가치를 평가할 때, 우리는 매 단계 후가 아닌 미래 n단계를 추정함으로써 이를 수행합니다.\u003c/p\u003e\n\u003cp\u003en단계 TD에 대한 수정된 접근 방식은 TD(𝝀)입니다. TD(𝝀) 방법은 지나간 상태-액션 쌍에 대한 신용을 할당하기 위해 '적격성 흔적(eligibility traces)'이라 불리는 매개변수를 사용합니다. 미래 n단계를 추정하는 대신, 적격성 흔적은 여러 TD 단계에 걸쳐 상태-액션 쌍에 대한 신용을 할당합니다. 적격성 흔적은 지난 상태-액션 쌍이 관찰된 보상 전환에 기여한 정도에 대해 신용을 부여합니다. 적격성 흔적은 각 상태-액션 쌍에 연관된 벡터나 행렬로 표현됩니다. 시간 단계에 대한 적격성 흔적은 다음과 같이 재귀적으로 계산됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_22.png\" alt=\"Eligibility Trace Formula\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 람다(𝝀) 매개변수는 부트스트래핑의 정도를 제어합니다. 𝝀 = 1일 때, 부트스트래핑이 없어지고 업데이트 규칙은 Monte Carlo로 축소됩니다. 𝝀 = 0일 때는 부트스트래핑이 있는 TD(0)로 축소됩니다. TD(𝝀)는 TD와 MC를 연속체로 일반화한 것으로, 여기서 TD(0)는 단일 단계 TD를 의미하며, TD(1)은 TD를 ∞ 단계까지 확장한 극한값인 MC를 의미합니다. 수식에서 보듯이, 적격성 흔적 매개변수는 재귀적으로 계산됩니다. 다음 시간 단계의 적격성 흔적값은 이전 단계의 적격성 흔적 값을 입력으로 취합니다. E(s) = 0일 때, 부트스트랩이 없어집니다. TD(𝝀) 업데이트 규칙은 아래와 같이 TD 및 MC 업데이트 규칙과 동일하게 계산되지만, 애러에 적격성 흔적을 곱하는 것이 차이입니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_23.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003ch2\u003eANNs를 활용한 강화 학습 확장\u003c/h2\u003e\n\u003cp\u003e모델 기반 또는 모델 무관 RL 알고리즘은 차원의 저주로 인한 스케일링 문제에 직면하며, 다양한 유형의 환경 간 일반화에 어려움을 겪으며, 샘플 효율성에 대한 어려움이 있습니다.\u003c/p\u003e\n\u003cp\u003e인공 신경망(ANN)은 RL 아키텍처 내재한 일부 한계를 교정하는 강력한 방법을 제공합니다. 특히, ANNs는 샘플링 효율성, 환경 일반화, 차원의 저주로 인한 스케일링 문제를 개선합니다. 데이터로부터 일반 함수를 학습하기 때문에 우수한 일반화 능력을 통해 샘플 효율성을 줄이고 환경 일반화를 향상시킵니다. 이는 숨겨진 계층의 수와 각 숨겨진 계층 당 뉴런 수를 늘릴 수 있어 더 잘 스케일링할 수 있도록 합니다. 그러나 너무 많은 숨겨진 계층과 뉴런은 계산 스케일링 문제를 야기할 수도 있습니다(특정 범위를 벗어날 경우 차원의 저주를 피할 수 없습니다). 또한 전통적으로 ANNs가 사전에 목표 상태의 비정상성 문제에 시달리며, RL 알고리즘은 정책 상이든 오프-정책 상에 상관없이 업데이트 함수를 통해 최적 상태를 찾습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e전통적인 강화 학습 알고리즘이 확률적 전이 규칙에 의존하는 데 반해, 강화 학습에 인공 신경망(ANNs)을 적용하면 함수 근사를 사용하여 상태 및 상태-행동 값들을 계산하게 됩니다. 선형 근사 및 타일 코딩과 같은 여러 함수 근사 방법을 적용할 수 있지만, 인공 신경망은 비선형 함수 근사를 활용한 일반화 능력으로 가장 강력한 기술을 구성합니다.\u003c/p\u003e\n\u003cp\u003e강화 학습에 인공 신경망을 적용하는 두 가지 접근 방식인 딥 Q 학습(DQN)과 시간차 학습( Temporal Difference; TD(𝝀))에 대해 알아봅시다. 미리 목표 값들을 모르기 때문에 MC 또는 TD를 사용하여 목표 상태의 추정인 예상 반환값을 생성합니다. 그런 다음 이 값은 함수(실제로는 네트워크 매개 변수 𝜃에 대한 전체 네트워크의 오차의 편도함수인 경사)가 근사화할 목표값으로 사용됩니다. 인공 신경망은 목표값을 추정 값과 출력 사이의 오차를 계산하여 그 오차를 역전파 및 최적화 알고리즘을 통해 감소시킴으로써 목표 값을 근사화합니다. 가장 일반적인 최적화 알고리즘은 확률적 경사 하강법의 변형인 것이며, 이를테면 확률적 경사 하강법이 대표적입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_24.png\" alt=\"강화 학습 소개\"\u003e\u003c/p\u003e\n\u003ch2\u003e오프-폴리시 DQN\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e큐-러닝은 SARSA(State, Action, Reward, State', Action')의 오프-폴리시 버전으로, 다음 상태-액션 쌍 Q(s', a')은 다음 상태에서 이용 가능한 액션들 중에서 최대 예측 값으로 추정됩니다. 다시 말해, 큐-러닝은 다음 상태 s'에서 이용 가능한 액션들 사이에서 Q(s',a')의 최댓값을 선택합니다. 이는 정책(𝜋)을 사용하지 않고 Q(s',a')를 학습한다는 것을 의미합니다. 반면, SARSA는 이전 액션을 선택하고 다음 상태-액션 쌍인 Q(s',a')를 추정하는 온-폴리시 방법입니다. 이는 상태가 주어졌을 때 액션의 확률인 정책(𝜋)을 사용하여 Q-함수를 학습한다는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e딥 큐-러닝에서는 액션-가치 함수 Q(a, s)가 Q(a,s, 𝜃)로 표현되며, 여기서 𝜃은 신경망 매개변수를 나타냅니다. Theta(𝜃) 매개변수는 신경망에서의 가중치 w에 해당하며, 뉴런들 간의 연결에 관련되어 있습니다. 이 가중치는 연결의 강도를 결정하고, 오차를 최소화하기 위해 역전파를 통해 후방으로 조정됩니다. DQN은 환경의 고차원 표현을 입력으로 삼고, 각 가능한 액션에 대한 액션-가치 벡터를 출력합니다. 예상 수익은 일반적으로 MC 또는 TD 접근법을 통해 근사됩니다. 이후, 역전파와 최적화 함수를 사용하여 정책 기울기를 계산하고 정책 네트워크의 매개변수(𝜃)를 조정하여 오차를 줄입니다.\u003c/p\u003e\n\u003cp\u003e인공신경망은 새로운 정보에 매우 민감하기 때문에, 새로운 정보가 이전에 작성된 정보를 덮어쓰는 치명적인 잊혀짐을 초래할 수 있습니다. 이러한 치명적인 잊혀짐을 다루는 방법 중 하나는 경험 재생을 적용하는 것입니다. 이 기법은 과거 경험을 저장하고 네트워크를 훈련하는 데 재사용합니다.\u003c/p\u003e\n\u003ch2\u003e온-폴리시 딥 TD(𝝀)\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eANNs는 TD(λ) 방법에도 적용할 수 있습니다. 여기서 상태 관찰은 ANN에 입력으로 공급되고, ANN은 그것을 통해 액션-가치 함수를 출력으로 근사화합니다. TD(λ)의 온-폴리시 성격 때문에 Deep TD(λ) 방법은 상태 간의 장기 의존성이 필요한 작업에 가장 적합합니다.\u003c/p\u003e\n\u003cp\u003eTD(λ)와 같은 온라인 학습 방법을 훈련하는 것은 도전적일 수 있습니다. 왜냐하면 환경의 분포가 부트스트래핑으로 인해 매 단계마다 또는 n 단계마다 변경되기 때문이기 때문입니다. 이를 비정상성(nonstationarity)이라고 부르며, ANN 매개변수 𝜃가 최적으로 수렴하는 것을 방해합니다. 온라인 학습에서 연이어 발생하는 상태 간의 종속성은 치명적인 잊혀짐(catastrophic forgetting)을 발생시킬 수 있어, 업데이트가 과거 학습에 영향을 미칩니다. 더욱이, 과거 조치에 신용을 할당하는 자격 흔적 및 ANNs의 결합은 역전파 단계에서 추가적인 복잡성을 초래할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e위기에 대처하는 한 가지 방법은 경험 재생(experience replay) 기법을 활용하는 것입니다. 경험 재생은 학습된 에이전트 에피소드를 [s, a, r, s’]의 벡터로 메모리 버퍼에 저장합니다. 훈련 중에 네트워크는 저장된 학습 벡터의 메모리 버퍼에서 샘플을 추출하여 네트워크 매개변수를 업데이트합니다. 이를 통해 네트워크는 더 큰 안정성을 제공받고, 새로운 경험으로 인한 큰 오류나 단계 간의 시간 차이로 인한 치명적인 간섭에서 덜 영향을 받습니다.\u003c/p\u003e\n\u003cp\u003eDeep TD(λ) 알고리즘은 상태 공간이 연속적이고 대상이 알려지지 않거나 불분명한 연속 제어 작업에서 뛰어난 성과를 보여주었습니다. 이러한 작업에는 로봇 과제, 자율 주행 자동차, 금융 시장의 연속 제어 작업 등이 포함됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e강화 학습과 인공 일반 지능\u003c/h2\u003e\n\u003cp\u003e강화 학습이 인공 일반 지능에 미칠 영향은 무엇인가요?\u003c/p\u003e\n\u003cp\u003e\"지능\"이라는 것은 서로 다른 능력을 단일한 개념으로 결합하기 때문에 모호한 변수이지만, \"일반 지능\"은 생물의 진화된 능력들을 기반으로 하며, 생존과 번식을 위해 세계적인 정보를 변환하는 것이 요구된다. 심지어 인간의 맥락에서도 지능은 유기적인 생존 가능성의 윤곽에서 분리될 수 없다. 그러나 이것이 통용되는 견해는 아니다. 일반적인 지혜는 지능이 이용 가능한 정보를 기반으로 추론을 계산하는 프로그램 또는 소프트웨어와 유사하다고 주장한다.\u003c/p\u003e\n\u003cp\u003e후자의 개념은 경쟁한다고 여겨지는 두 가지 모델로 구성되어 있는데, 하나는 절차를 따르는 지능으로 설명되고, 다른 하나는 최적의 예측을 위해 데이터로부터 일반화하는 지능으로 설명된다. 전자는 일반적으로 더 잘 이해되지만, 후자는 예측의 강도를 신뢰성 있게 향상시키는 기법 집합으로 이루어져 있다고 할 수 있다. 동물의 지능은 대부분 후자 모델에 기반한다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e두 번째 모델의 가장 성공적인 패러다임은 인공 신경망을 통한 딥 러닝입니다. 인공 신경망 구조의 주요 장점은 사전 정보나 개념 없이 데이터로부터 일반화를 가능하게 한다는 것입니다. 이는 비지도 학습과 혼동해서는 안됩니다. 인공 신경망은 먼저 훈련을 통해 모델을 구축한 다음 새로운 데이터에 대해 해당 모델을 기반으로 예측합니다. 따라서 두뇌도 (진화를 통한 사전 훈련을 고려하면) 비슷한 일을 한다고 생각됩니다. 그러나 현재 인공 신경망에서는 두 가지 약점이 있습니다. 첫 번째 약점은 목표나 결과가 인간 디자이너에 의해 설정되어야 한다는 것입니다. 인공 신경망은 스스로 목표를 설정할 수 없습니다. 또한, 스스로 진실과 거짓을 구별할 수 없습니다. 모델이 그 결과를 근사화하는 방법을 배우기 위해 참된 결과를 제공해야 합니다. 두 번째 약점은 강화 학습 없이 인공 신경망이 자체 상태를 최적화하기 위해 환경을 탐색할 수 없다는 것입니다. 이러한 이유로 인공 신경망의 일반화와 예측 능력을 강화 학습의 결정 최적화 능력과 결합하는 것이 엄청난 섞임을 만들어냅니다.\u003c/p\u003e\n\u003cp\u003e이 기반 위에서 강화 학습이 인공 일반 지능으로 가는 가장 명확한 길을 대표한다고 주장한 사람들도 있습니다(Sutton, 2014). 이에 대한 직관은 분명합니다. 강화 학습은 생명체를 모델링하는 데 가장 가깝습니다. 이것이 다른 성공적인 구조들과 결합되면 (예를 들어 변환기와 같은), 모든 인간 능력을 복제하고 능가하는 AI 모델로 이어질 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그러나 만약 인간이 일반 지능의 기초라면, 일반 지능의 개념은 인지능력을 생존 제약과 어떤 형태의 구현체와 이혼시키지 않는 것일 수 없습니다. 반면에, 일반 지능을 생명 형태에 언급하지 않고 정의할 수 있다면, 그것이 어떤 모습인지는 분명하지 않습니다. 즉 순수한 추상 모델은 Marcus Hutter의 AIXI와 같은 시도에도 불구하고 만족스러운 형식화를 피해갑니다. 추상적으로는 논리적 추론과 계산 능력만으로 문제를 해결하는 완벽하게 합리적인 에이전트로 생각할 수 있습니다. 정보와 구현체 간의 격차는 이 기사의 범위를 넘어서는 더 큰 토론을 유발하는 것이며, 관심이 있다면 이 논문이 좋은 시작점을 제공합니다.\u003c/p\u003e\n\u003cp\u003e그러나 강화 학습만으로는 인공 일반 지능을 충분히 표현할 수 있는지에 대한 의문이 있습니다. 이에 대한 이유로는 일반 지능의 정의에서 비롯한 것들이며 현재의 대부분의 AI 연구자들이 명시적인 내부 표현을 필수적인 요소로 간주하지 않고 있기 때문입니다. 이에 대한 상당한 이유가 있습니다. 딥 러닝의 성공 이전에 인공 일반지능의 희망을 걸고 있던 상징적 AI는 실패로 이어졌습니다. 상징적 AI는 주로 명시적으로 코딩된 논리 규칙과 최적 추론 생성을 위한 지식 저장소에 기초한 인공 지능 접근 방식을 나타냅니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e상징적 인공지능과 신경망 사이의 긴장은 그러나 근거 없을 수도 있습니다. 많은 연구자들은 인공 일반 지능을 얻기 위한 탐구가 이러한 접근 방식을 적절히 결합하는 데 있다고 믿습니다. 신경망이 뇌의 본질적 오네톨로지를 근사화한다고 생각하는 이유는 수학적 논리가 뇌가 추론하는 방식이 아니기 때문입니다. 즉, 그것은 필요충분 조건을 계산하지 않거나 정확한 멤버십을 계산하는 것보다는 점수 있는 멤버십에 중점을 두며, 이는 퍼지 논리와 같은 방식에 의해 근사화되며 ANN(인공신경망)이 뛰어납니다.\u003c/p\u003e\n\u003cp\u003e신경망은 원하는 출력을 달성하기 위해 파라미터화된 은닉층의 계층적 아키텍처로 구성되어 있으며, 고도로 보정된 동적 학습률, 활성화 함수, 연결 가중치 및 최적화 알고리즘을 통해 오차를 최소화하기 위해 보정된 학습율을 통해 원하는 출력을 달성합니다. 위와 같이 고도로 보정된 하이퍼파라미터를 넘어서 사람이 이해하지 못하는 정보가 은닉층에서 처리된다는 가정입니다. 정보가 이산적인 표현 단위(아날로그나 이미지 등)의 조합으로 저장되지 않으며 수십억 개의 뉴런들로 이루어진 분산 아키텍처로 저장된다는 것이 뇌의 경우와 같다는 가정입니다. 언어적으로 구조화된 생각이 고정적인 뉴런 조합으로 내부적으로 표현되는 것이 아닌데, 예를 들어 '존재 자체가 다른 존재를 위한 존재임을 하자’ 라는 문장 또는 단어를 나타내는 뉴런의 특정 조합이 없습니다.\u003c/p\u003e\n\u003cp\u003e언어적 능력은 대신 경험에 의해 강화된 의미 연결과 재생규칙으로 내장된 거대한 네트워크에 포함되어 있습니다. 다시 말해, 우리가 반영적으로 문장을 작성하고 말할 때 언어와 생각이 뇌의 본질적 오네톨로지와 문법 간 이쾌적 매핑이 아닌, 연결의 정도와 연결 강도에 의해 특징을 가진 신경 접속의 분산 네트워크에 내재되어 있다는것을 보여줍니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e현재 AI는 세계에서 자체적으로 추진되는 자율 시스템을 근사하지 않습니다. 또한 인간과 다른 동물들이 하는 방식으로 자체 환경적 환경을 생성하거나 자체 검색 공간을 재구성하지도 않습니다. 현재 이 제약이 없기 때문에 인간 디자이너는 AI의 정보적 중요성을 설정할 수 있습니다. 예를 들어 텍스트 생성, 환경 감지 등이 있습니다. 아키텍처가 진정한 문제 해결 기계로 진화하더라도 반성적 인식 능력이 없다면 일반적 지능을 보유하고 있다고 할 수 없습니다. 전통적으로 일반 지능의 정의에서는 인간 지능의 상징인 전체 인식의 변수를 생략합니다. 전통적인 지능 정의는 반사적이고 전체적 인식은 역공학 및 구성 요소의 분석에 대한 강한 저항을 가지고 있기 때문입니다. 그 이유로 반성적 인식은 지능의 구성 요소로서 배제됩니다. 그러나 현재의 과학적 설명에 대한 저항을 인정한다고 해도, 물리주의를 배제하거나 비자연론의 지지를 함축하지 않습니다. 오히려 이해력의 부재를 인정하는 신호일 뿐입니다. 이해력의 공방 속에, 반성적인 인식이 생명 유기체의 기본 속성인 감각의 확장임을 가설합니다. 이를 주장함으로써, 자연선택 이외의 방법을 통해 자율 시스템을 설계할 수 없다는 의미는 아니지만, 그들이 가까운 미래에도 과학적 분석에는 어려움을 줄 수 있다는 가능성은 여전히 열려 있습니다. 강화 학습이 일반적 지능으로 이어지길 바란다면, 에이전트는 세계의 복잡한 표현을 보유할 뿐만 아니라 그 표현의 내부에서 전반적인 관점을 유지할 수 있는 강력한 아키텍처를 사전으로 가져야 합니다. 이는 모델-세계 상호작용이 과제에 반드시 필수적이지만, 원조 아키텍처는 다중 모달 정보 처리와 통합 능력을 갖춘 복합적인 계층적 내부 구조를 요구할 것입니다.\u003c/p\u003e\n\u003ch2\u003e선택된 참고 자료\u003c/h2\u003e\n\u003cp\u003eMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., \u0026#x26; Hassabis, D. (2015). Deep Reinforcement Learning을 통한 Human-level Control. Nature, 518(7540), 529–533. \u003ca href=\"https://doi.org/10.1038/nature14236\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://doi.org/10.1038/nature14236\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eNeftci, E. O., \u0026#x26; Averbeck, B. B. (2019, March 4). 인공 및 생물학적 시스템에서 강화 학습. Nature News. \u003ca href=\"https://www.nature.com/articles/s42256-019-0025-4\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://www.nature.com/articles/s42256-019-0025-4\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eSharma, S. (2024, March 7). Learning to Mix 𝑛-Step Returns: Generalizing 𝜆-Returns for Deep Reinforcement Learning. Ar5iv. \u003ca href=\"https://ar5iv.labs.arxiv.org/html/1705.07445\" rel=\"nofollow\" target=\"_blank\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSanghi, Nimish. Deep Reinforcement Learning with Python: With PYTORCH, Tensorflow, and Openai Gym. Apress, 2021.\u003c/p\u003e\n\u003cp\u003eSilver, D., Singh, S., Precup, D., \u0026#x26; Sutton, R. S. (2021). Reward is enough. Artificial Intelligence, 299, 103535. \u003ca href=\"https://doi.org/10.1016/j.artint.2021.103535\" rel=\"nofollow\" target=\"_blank\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSpens, E., \u0026#x26; Burgess, N. (2024, January 19). A generative model of memory construction and consolidation. Nature News. \u003ca href=\"https://www.nature.com/articles/s41562-023-01799-z\" rel=\"nofollow\" target=\"_blank\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eSutton, Richard S. Introduction to Reinforcement Learning. MIT Press.\u003c/p\u003e\n\u003cp\u003eTyng, C. M., Amin, H. U., Saad, M. N. M., \u0026#x26; Malik, A. S. (2017, August 24). The influences of emotion on learning and memory. Frontiers in psychology. \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/\" rel=\"nofollow\" target=\"_blank\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWhite, A., Modayil, J., \u0026#x26; Sutton, R. (2014). Surprise and Curiosity for Big Data Robotics. Association for the Advancement of Artificial Intelligence, 19–22.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-AnIntroductiontoReinforcementLearning"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>