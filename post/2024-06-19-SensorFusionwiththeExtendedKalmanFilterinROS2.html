<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>로스 2에서 확장 칼만 필터를 활용한 센서 융합 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="로스 2에서 확장 칼만 필터를 활용한 센서 융합 | itposting" data-gatsby-head="true"/><meta property="og:title" content="로스 2에서 확장 칼만 필터를 활용한 센서 융합 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2" data-gatsby-head="true"/><meta name="twitter:title" content="로스 2에서 확장 칼만 필터를 활용한 센서 융합 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 06:22" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">로스 2에서 확장 칼만 필터를 활용한 센서 융합</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="로스 2에서 확장 칼만 필터를 활용한 센서 융합" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">31<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png" alt="Sensor Fusion with the Extended Kalman Filter in ROS2"></p>
<p>안녕하세요! 이 글은 가우시안 필터를 소개하는 시리즈 중 두 번째 글입니다. 구체적으로, 이 글은 칼만 필터 패밀리에 대한 세 번째 세부 소개입니다. 이미 칼만 필터에 대해 익숙하지 않다면, 계속하기 전에 첫 번째 글을 읽기를 권장합니다. 다음 글에서는 언센티드 칼만 필터를 소개할 예정입니다. 이 글의 결과를 재현하는 데 사용된 데이터와 코드는 이 글의 끝 부분에 찾을 수 있습니다.</p>
<h1>소개</h1>
<p>이전 글에서 소개된 대로, 성공적인 로봇 시스템은 유용한 작업을 수행하기 위해 물리적 세계를 인식하고 조작할 수 있어야 합니다. 이를 달성하기 위해 로봇은 환경의 중요한 불확실성을 고려해야 합니다. 현대 로봇 공학에서 가장 기본적인 문제 중 하나는 상태 추정입니다. 상태 추정은 로봇과 환경(랜드마크 및 기타 객체의 위치 등)의 가장 확률적인 상태(예: 위치, 방향, 속도)를 불확실한(잡음이 있는) 및 아마도 불완전한 정보를 기반으로 결정하는 것을 포함합니다.</p>
<div class="content-ad"></div>
<p>칼만 필터는 불확실한 환경에서 상태 추정에 대응합니다. 필터는 상태의 각 요소 (예: x, y 좌표, 헤딩)을 가우시안 확률 변수로 모델링합니다. 가우시안은 상태 x에 대한 확률 밀도를 벡터 μ (뮤)와 공분산 행렬 Σ (시그마)만 사용하여 표현할 수 있게 합니다. 이 매개변수화에서 우리의 상태 x는 기대값 μ에 의해 표현되며, Σ는 제어, 이동 및 관측 잡음으로 인한 상태의 내재적 불확실성을 포착합니다.</p>
<p>칼만 필터가 사용하는 베이지안 프레임워크에서는 전체 상태를 믿음(belief)이라고 합니다. 가우시안 (또는 정규분포)을 사용하는 장점은 그들의 수학적 성질에 있습니다. 이 성질은 칼만 필터 방정식을 단순화합니다. 가우시안 믿음이 선형 변환을 겪을 때의 특징 중요한데, 가우시안 믿음이 선형 변환을 겪으면 결과는 여전히 가우시안 확률 변수로 유지됩니다. 이 성질은 칼만 필터의 방정식이 우아하고 다루기 쉬운 상태를 유지하도록 보장합니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_1.png" alt="image"></p>
<p>사후 믿음을 계산하기 위해 칼만 필터는 이전 믿음을 시간을 경과함에 따라 전달하는 모션 모델을 사용합니다. 그런 다음 관측 모델은 로봇 센서에서의 데이터를 통합하여 예측된 믿음을 업데이트하고 이를 사후로 변환합니다. 칼만 필터 알고리즘은 아래에서 요약됩니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_2.png" alt="Linearity of the Linear Kalman Filter"></p>
<p>선형 칼만 필터의 "선형성"은 알고리즘의 2번째와 5번째 줄에서 가장 명백합니다. 2번째 줄에서 예측된 상태는 이전 상태 μ_t−1과 제어 입력 u_t의 선형 함수입니다. 5번째 줄에서 예측된 관측값 (y = Cμ)은 인수 μ^bar_t의 선형 함수입니다. 이 선형성은 가우시안 특성을 유지하여 필터를 구현하기 쉽게 만듭니다. 그러나 이것은 또한 주요 약점 중 하나를 나타냅니다.</p>
<p>선형 칼만 필터가 실제 문제에 부적합한 이유는 실제 문제가 종종 선형적이지 않기 때문입니다. 이전 글에서 도입했던 간단한 상태 추정의 경우, 상태가 모바일 로봇의 2차원 포즈 (x, y, θ)로 표현되었지만 정확히 선형적이지 않았습니다. 아래에 다시 소개되는 이동 모델에서 확인할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_3.png" alt="Motion Model"></p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_4.png" alt="image"></p>
<p>모델이 쉽게 알아볼 수 있듯이, 삼각 함수가 사용되어 로봇의 좌표를 업데이트하는데 사용되며, 이러한 함수들은 선형이 아닙니다. 이 모델에서 선형 칼만 필터는 어느 순간 발산할 가능성이 있습니다. 이 이유로 선형 칼만 필터를 소개한 후, 대부분의 실제 현상의 비선형성을 고려하기 위해 확장 방법을 고안하는 작업이 즉시 시작되었습니다.</p>
<h1>확장 칼만 필터</h1>
<h2>비선형성의 문제</h2>
<div class="content-ad"></div>
<p>비선형성에 관한 문제를 더 자세히 설명하기 위해 다음 애니메이션을 살펴볼 수 있습니다. 첫 번째 애니메이션에서는 선형 칼만 필터의 가정이 성립하는 세계에 있으며, 새로운 상태가 인수에 대해 단순하게 선형인 경우를 살펴봅니다. 시각화를 이해하기 쉽게 하기 위해 1차원 상태 x를 가정합니다. 이 애니메이션은 가우스를 선형 함수 g를 통과시켰을 때 다른 가우스가 되는 과정을 보여줍니다. 이 경우 g = -0.5*x+1입니다.</p>
<p>x의 가우시안 표현에서 시작하지만 비선형 함수 g를 선택하는 경우, 결과 확률 밀도 함수는 더 이상 가우시안이 아닙니다. 새로운 밀도를 계산하기 위한 폐쇄형 방법이 없습니다. 대신 입력 분포에서 점들을 샘플링하고 이를 g를 통과시켜 출력 히스토그램을 구축하여 출력 분포를 만들어야 합니다. 아래에 표시된 출력의 형태에서 확인할 수 있듯이, 이는 가우시안이 아닌 것을 알 수 있습니다. 또한 이 출력 분포는 칼만 필터의 단봉성 가정을 위배하며, 단일 피크를 요구합니다. P(y)의 가우시안 근사는 출력 데이터에 가우시안을 맞추어 얻었습니다. 이는 실제 모델의 비선형성을 다루기 위한 선형 칼만 필터의 한계를 강조합니다.</p>
<p>요약하면 비선형 모델을 다룰 때 출력 밀도는 가우시안이 아니며 폐쇄형으로 계산할 수 없으며 종종 다중 피크를 갖기 때문에 칼만 필터 방정식이 무용지물이 됩니다. 이 문제를 해결하기 위해 확장 칼만 필터(EKF)는 선형성 가정을 버립니다. 대신 상태 전이 확률과 측정 확률은 비선형 함수 g 및 h에 따라 결정됨을 수용합니다.</p>
<img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_5.png">
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_6.png" alt="그림"></p>
<p>모든 것이 잘 진행되고 있어요. 칼만 필터의 우아한 방정식을 활용하려면 여전히 우리의 신뢰를 가우시안으로 표현해야 합니다. 이 필수성은 정확한 사후분포를 계산하는 것에서 EKF의 초점을 이동시켜 실제 신뢰의 좋은 가우시안 근사값을 찾게 됩니다. 아래 그림에서 보듯, 몬테칼로를 사용하여 출력 분포를 계산한 후, 그에 대한 가우시안을 fitting하고 필요한 매개변수 μ (뮤)와 Σ (시그마)를 얻을 수 있습니다. 그러나 아직도 가우시안을 닫힌 형태로 계산할 수 없는 문제가 남아 있습니다.</p>
<h2>테일러 전개를 통한 선형화</h2>
<p>이 문제를 해결하기 위해 확장 칼만 필터는 선형화라는 추가 근사값을 적용합니다. 선형화의 핵심 아이디어는 비선형 함수 g를 해당 관심점에서 g에 접하는 접선인 선형 함수로 근사하는 것입니다. 비선형 함수를 선형화하는 다양한 기술 중, EKF가 사용하는 것은 일차 테일러 전개입니다. 함수의 테일러 전개는 함수의 도함수를 하나의 점 a에서 표현된 다항식 항들의 무한 합입니다.</p>
<div class="content-ad"></div>
<p>위의 이미지에서 높은 차수의 테일러 전개가 점 a = 0 주변에서 g를 더 가까운 근사로 제공하는 것을 볼 수 있습니다. 그러나 고차 다항식이 늘어날수록 요구되는 계산도 증가하며, 문제가 빠르게 풀기 어려워집니다. 다행히도 Kalman 필터가 자주 업데이트되는 경우(작은 Δt), 관심점 a의 차이가 매우 작아야합니다. 따라서 우리는 다음 (매우 가까운) 각 지점 a에서의 함수 g의 값을 및 기울기(점 a에서의 미분)를 사용하여 함수 g의 선형 근사를 얻기 위해 1차 다항식(선)을 사용할 수 있습니다. 이 문제는 본질적으로 아래와 같이 간소화됩니다:</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png" alt="2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png"></p>
<div class="content-ad"></div>
<p>이 동작이 왜 잘 작동하는지 설명하기 위해 g라는 함수를 가정해 볼게요.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_10.png" alt="그림"></p>
<p>점 a에서 함수 g의 일차 테일러 전개는 아래 그림에서 빨간색으로 표시되어 있어요. 큰 x 값 범위를 관찰하면 좋은 근사치를 제공하지 않음이 명백해요.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_11.png" alt="그림"></p>
<div class="content-ad"></div>
<p>그러나 우리가 사후 분포의 값들에 대한 근사치에만 주의를 기울이기 때문에, 새로운 사후 분포에 대해 그러한 근사치를 매우 짧은 시간 이후에 다시 계산할 것을 알고 있기 때문에, 아래 그래프에서 볼 수 있듯이, 우리의 근사치가 관심 지점 주변에서 매우 좋다는 것을 알 수 있습니다. 이는 첫 번째 차수 테일러 전개가 우리의 목적에 대해 충분한 근사치를 제공하며, 시스템 내부의 비선형성에도 불구하고 확장 칼만 필터를 효과적으로 적용할 수 있도록 합니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_12.png" alt="image"></p>
<p>마지막으로, 아래 이미지는 두 프로세스를 비교하려고 합니다: 첫 번째는 원래의 가우시안으로 시작하여 비선형 함수 g를 통과시키고, 몬테카를로를 사용하여 비가우시안 출력 분포를 얻은 후, 이 출력에 가우시안을 적합시킵니다. 두 번째로, EKF가 사용하는 프로세스는 g를 선형화시키고, 원래의 가우시안을 이 선형 근사치를 통해 통과시킨 후, 선형화를 통해 닫힌 형태로 출력을 직접 얻습니다. 이 비교는 비선형 시스템 다루기에 대한 EKF 접근 방식의 효율성과 실용성을 강조합니다.</p>
<p>명확성을 위해, 아래에서는 출력만 표시됩니다. 보시다시피, EKF 가우시안이 몬테카를로 시뮬레이션에서 적합된 가우시안과 정확히 같지는 않지만, 충분히 가깝습니다. 이 작은 차이는 실제 분포의 닫힌 형태의 추정치를 효율적으로 얻기 위해 지불하는 대가입니다.</p>
<div class="content-ad"></div>
<p>위에서 설명한 간단한 예제는 스칼라 경우에 대한 것이었지만, 우리의 상태는 벡터입니다. 따라서 기울기를 구하기 위해 우리는 상태에 대한 g의 편미분을 계산합니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_13.png" alt="image"></p>
<p>g와 그 기울기 g′의 값은 그들의 인자 (u_t와 x_t-1)에 따라 달라지는데요, 이는 우리의 관심 지점입니다 (스칼라 경우의 a와 대조적입니다). u_t의 값에 대해서는 로봇에 제공된 제어 명령을 사용합니다. x_t-1에 대해서는 선형화할 시기에서 가장 가능성이 높은 상태의 값으로 선택할 수 있습니다. 가우시안의 경우, 최대 가능성 값은 이전 시간 단계에서 계산된 사후값의 평균인 μ_t-1로 표시됩니다. 이 선형화는 업데이트 속도가 매우 빠른 필터 (매우 작은 Δt)에 대해 잘 작동하며, 이때 μ_t-1의 값과 우리가 추정하려는 현재 상태 간의 차이가 크지 않을 때 잘 작동합니다.</p>
<p>이제 기울기를 계산했기 때문에 g를 다음과 같이 추정할 수 있습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_14.png" alt="sensor fusion"></p>
<p>가우시안에서, 모션 모델 또는 상태 전이 확률은 아래와 같이 표기됩니다. 여기서 R_t는 보통의 프로세스 노이즈 공분산입니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_15.png" alt="motion model"></p>
<p>기울기가 숫자인 스칼라 케이스와는 달리, g'(u_t, μ_t-1)으로 알려진 G_t는 행렬입니다. 비선형 함수 g에 대한 상태 x의 일차 편미분값을 모두 포함하는 이 행렬은 야코비안이라고 합니다. 이 야코비안 행렬은 상태의 차원인 n×n의 크기를 가지며, 현재 제어 및 이전 사후 평균에 따라 값이 달라집니다. 따라서 야코비안 값은 시간이 지남에 따라 변합니다.</p>
<div class="content-ad"></div>
<p>확장 칼만 필터(Extended Kalman Filter)는 함수 h에 의해 표현되는 비선형 관측 모델을 다룹니다. 특히, 타일러 전개(Taylor Expansion)는 새롭게 예측된 믿음 μ^bar_t을 중심으로 진행되며, 이는 h를 선형화하는 시점에서 가장 가능성이 높은 상태입니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_16.png" alt="이미지"></p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_17.png" alt="이미지"></p>
<p>가우시안으로, 측정 모델은 아래와 같이 표기됩니다. 여기서 Q_t는 전통적인 측정 잡음 공분산입니다. 여기서 야코비안 H_t는 관측 모델의 비선형 함수 h에 대한 상태 x에 대한 일차 편미분의 m×n 행렬입니다. 따라서 m은 관측의 차원이고, n은 상태의 차원입니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_18.png">
<h2>Extended Kalman Filter Algorithm</h2>
<p>요약하면, 아래에 표시된 EKF 알고리즘은 이 기사의 앞부분에 표시된 LKF 알고리즘과 매우 유사하지만, 주요한 차이점은 모션 및 관측 모델의 선형화가 2번 줄과 5번 줄에서 이루어진다는 것입니다.</p>
<img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_19.png">
<div class="content-ad"></div>
<p>아래 이미지는 EKF 알고리즘을 LKF와 나란히 재진 다음 주요 차이점을 강조합니다. 예측 단계에서 EKF는 선형 시스템 행렬 A 및 B 대신 상태를 시간에 따라 진화시키는 비선형 함수 g를 사용합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter�...</p>
<div class="content-ad"></div>
<h2>모델 선형화</h2>
<p>선형 칼만 필터와 사용된 모션 모델은 정확히 선형은 아니지만 여전히 단순하여 LKF가 처리할 수 있는 간단한 상수 속도 모델이었습니다. 참고로, 아래에 다시 표시해 드립니다. 이 모델은 확장 칼만 필터의 능력을 보여주기 위해 사용될 더 복잡한 변형을 소개하는 기초 역할을 할 것입니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_21.png" alt="image"></p>
<p>이 방정식들은 다음과 같이 LKF에서 요구하는 선형 시스템 행렬로 변환되었습니다:</p>
<div class="content-ad"></div>
<p>아래는 Markdown 형식입니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_22.png" alt="sensorfusion1"></p>
<p>EKF를 사용하여 동일한 모델을 사용하려면 선형화해야 합니다. 먼저로봇의 상태 형식을 재정의합니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_23.png" alt="sensorfusion2"></p>
<p>이후 비선형 함수 g를 정의합니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_24.png">
<p>다음으로, 우리는 각 상태 변수 x, y, 그리고 θ에 대한 함수 g의 편도함수를 포함하는 야코비안 행렬 G를 정의합니다. 이 행렬은 상태 변수의 변화가 운동 모델에 어떻게 영향을 미치는지를 포착합니다. 야코비안 행렬 G는 EKF 알고리즘에서 공분산 행렬을 업데이트하는 데 사용될 것이며, 우리에게 운동 모델의 비선형성들을 고려하면서도 칼만 필터 프레임워크의 계산 효율성을 유지할 수 있게 해줍니다.</p>
<img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_25.png">
<p>x 업데이트 방정식의 x, y, 그리고 θ에 대한 도함수를 나타내는 야코비안 행렬 G의 첫 번째 행을 살펴보면, 다음과 같은 것을 볼 수 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>첫 번째 항목은 x에 대한 부분 도함수이기 때문에 1입니다.</li>
<li>두 번째 항목은 x가 y에 의존하지 않음을 나타내는 0입니다.</li>
<li>세 번째 항목은 −vsin(θ)Δt이며, 이것은 x가 θ에 −vsin(θ)Δt항으로 의존함을 보여줍니다. 이것은 θ의 변화가 x 좌표에 미치는 영향을 반영합니다.</li>
</ul>
<p>다음으로 측정 함수 h가 필요합니다. 이전 Linear Kalman Filter의 구현과 유사하게, 사용할 측정은 로봇의 오도메트리 시스템에서 제공하는 것만 사용할 것입니다. 이 시스템은 바퀴 엔코더에서 계산된 로봇의 추정 자세를 직접 제공합니다. 그 데이터 형식이 우리의 상태 형식과 일치하기 때문에, h(μ^bar_t)는 단순히 μ^bar_t를 반환합니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_26.png" alt="image"></p>
<p>그리고 상태에 대한 측정 함수의 부분 미분을 포함하는 Jacobian 행렬 H는 단위 행렬이며 아래와 같이 표시됩니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_27.png" alt="sensor fusion"></p>
<p>이전 그림에서 EKF와 LKF를 비교한 것처럼, 선형화가 두 알고리즘의 주요 차이점입니다. 관련 기능 및 해당 야코비안을 확인한 후, EKF의 구현은 LKF의 구현을 밀접하게 따릅니다. 아래는 선형화된 속도 모션 모델의 Python 구현입니다. 행렬 g와 야코비안 G를 모두 반환합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">velocity_motion_model_linearized_1</span>():

 <span class="hljs-keyword">def</span> <span class="hljs-title function_">state_transition_function_g</span>(<span class="hljs-params">mu = <span class="hljs-literal">None</span>, u = <span class="hljs-literal">None</span>, delta_t = <span class="hljs-literal">None</span></span>):
  x = mu[<span class="hljs-number">0</span>]
  y = mu[<span class="hljs-number">1</span>]
  theta = mu[<span class="hljs-number">2</span>]
  
  v = u[<span class="hljs-number">0</span>]       
  w = u[<span class="hljs-number">1</span>]       
  
  g = np.array([
            x + v * np.cos(theta) * delta_t,
            y + v * np.sin(theta) * delta_t,
            theta + w * delta_t
        ])

  <span class="hljs-keyword">return</span> g

 <span class="hljs-keyword">def</span> <span class="hljs-title function_">jacobian_of_g_wrt_state_G</span>(<span class="hljs-params">mu = <span class="hljs-literal">None</span>, u = <span class="hljs-literal">None</span>, delta_t = <span class="hljs-literal">None</span></span>):
  theta = mu[<span class="hljs-number">2</span>]
  v = u[<span class="hljs-number">0</span>]       
  w = u[<span class="hljs-number">1</span>]       
  
  G = np.array([
   [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -v * np.sin(theta) * delta_t],
   [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, v * np.cos(theta) * delta_t],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
  ])

  <span class="hljs-keyword">return</span> G

 <span class="hljs-keyword">return</span> state_transition_function_g, jacobian_of_g_wrt_state_G
</code></pre>
<p>다음으로, 다음과 같이 간단한 관측 모델을 구현합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">def <span class="hljs-title function_">odometry_observation_model_linearized</span>():
 def <span class="hljs-title function_">observation_function_h</span>(mu):
  <span class="hljs-keyword">return</span> mu
 
 def <span class="hljs-title function_">jacobian_of_h_wrt_state_H</span>(mu):
  <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">eye</span>(<span class="hljs-number">3</span>)

 <span class="hljs-keyword">return</span> observation_function_h, jacobian_of_h_wrt_state_H
</code></pre>
<p>마지막으로, 아래에 구현된 Extended Kalman Filter 코드를 확인해보세요. 직전 게시물에서 소개된 Linear Kalman Filter 코드와 얼마나 비슷한지 주목하세요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np 

<span class="hljs-keyword">from</span> rse_motion_models.<span class="hljs-property">velocity_motion_models</span> <span class="hljs-keyword">import</span> velocity_motion_model_linearized_1
<span class="hljs-keyword">from</span> rse_observation_models.<span class="hljs-property">odometry_observation_models</span> <span class="hljs-keyword">import</span> odometry_observation_model_linearized

<span class="hljs-keyword">class</span> <span class="hljs-title class_">KalmanFilter</span>:

 def <span class="hljs-title function_">__init__</span>(self, initial_state, initial_covariance, proc_noise_std = [<span class="hljs-number">0.02</span>, <span class="hljs-number">0.02</span>, <span class="hljs-number">0.01</span>], obs_noise_std = [<span class="hljs-number">0.02</span>, <span class="hljs-number">0.02</span>, <span class="hljs-number">0.01</span>]):

  self.<span class="hljs-property">mu</span> = initial_state # 초기 상태 추정
  self.<span class="hljs-property">Sigma</span> = initial_covariance # 초기 불확실성

  self.<span class="hljs-property">g</span>, self.<span class="hljs-property">G</span> = <span class="hljs-title function_">velocity_motion_model_linearized_1</span>() # 사용할 액션 모델

  # 과정 모델 노이즈의 표준 편차
  self.<span class="hljs-property">proc_noise_std</span> = np.<span class="hljs-title function_">array</span>(proc_noise_std)
  # 과정 노이즈 공분산 (R)
  self.<span class="hljs-property">R</span> = np.<span class="hljs-title function_">diag</span>(self.<span class="hljs-property">proc_noise_std</span> ** <span class="hljs-number">2</span>) 

  self.<span class="hljs-property">h</span>, self.<span class="hljs-property">H</span> = <span class="hljs-title function_">odometry_observation_model_linearized</span>() # 사용할 관측 모델

  # 관측 모델 노이즈의 표준 편차
  self.<span class="hljs-property">obs_noise_std</span> = np.<span class="hljs-title function_">array</span>(obs_noise_std)
  # 관측 노이즈 공분산 (Q)
  self.<span class="hljs-property">Q</span> = np.<span class="hljs-title function_">diag</span>(self.<span class="hljs-property">obs_noise_std</span> ** <span class="hljs-number">2</span>)

 def <span class="hljs-title function_">predict</span>(self, u, dt):
  # 상태 추정 (mu) 예측
  self.<span class="hljs-property">mu</span> = self.<span class="hljs-title function_">g</span>(self.<span class="hljs-property">mu</span>, u, dt)
  # 공분산 (<span class="hljs-title class_">Sigma</span>) 예측
  self.<span class="hljs-property">Sigma</span> = self.<span class="hljs-title function_">G</span>(self.<span class="hljs-property">mu</span>, u, dt) @ self.<span class="hljs-property">Sigma</span> @ self.<span class="hljs-title function_">G</span>(self.<span class="hljs-property">mu</span>, u, dt).<span class="hljs-property">T</span> + self.<span class="hljs-property">R</span>

  <span class="hljs-keyword">return</span> self.<span class="hljs-property">mu</span>, self.<span class="hljs-property">Sigma</span>

 def <span class="hljs-title function_">update</span>(self, z, dt):
  # 칼만 이득 (K) 계산
  K = self.<span class="hljs-property">Sigma</span> @ self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>).<span class="hljs-property">T</span> @ np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">inv</span>(self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>) @ self.<span class="hljs-property">Sigma</span> @ self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>).<span class="hljs-property">T</span> + self.<span class="hljs-property">Q</span>)
  
  # 상태 추정 (mu) 업데이트
  self.<span class="hljs-property">mu</span> = self.<span class="hljs-property">mu</span> + K @ (z - self.<span class="hljs-title function_">h</span>(self.<span class="hljs-property">mu</span>))

  # 공분산 (<span class="hljs-title class_">Sigma</span>) 업데이트
  I = np.<span class="hljs-title function_">eye</span>(<span class="hljs-title function_">len</span>(K)) 
  self.<span class="hljs-property">Sigma</span> = (I - K @ self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>)) @ self.<span class="hljs-property">Sigma</span>

  <span class="hljs-keyword">return</span> self.<span class="hljs-property">mu</span>, self.<span class="hljs-property">Sigma</span>
</code></pre>
<p>성능:</p>
<div class="content-ad"></div>
<p>Extended Kalman Filter(EKF)의 성능을 평가하기 위해, 동일한 상수 속도 모델과 오도메트리 관측 모델을 사용하는 선형 칼만 필터(LKF)의 성능과 비교할 것입니다. 먼저, 초기 과정 노이즈를 크게 설정하고 관측 노이즈를 매우 낮게 설정할 것입니다. 이 설정은 사실상 필터에게 행동 모델보다는 관측을 신뢰하도록 지시합니다. 예상했듯이, 두 버전의 칼만 필터는 관측을 따라가며 비슷한 성능을 발휘합니다. 관측 모델의 간단함과 선형성을 고려하면 이 결과가 예상된 것입니다.</p>
<p>다음 테스트는 더 어려울 것입니다. 과정 노이즈를 매우 낮게 설정하고 관측 노이즈를 매우 높게 설정할 것입니다. 이 설정은 필터가 대부분의 관측을 무시하고 상태 추정에 운동 모델에 크게 의존하게 만듭니다. 운동 방정식이 같더라도, EKF가 적용한 선형화 때문에 크게 다른 결과를 예상할 것입니다.</p>
<p>예상대로, 차이는 상당합니다. 이러한 노이즈 설정 하에서 LKF는 성능이 저조합니다. 비선형 운동 모델에 완전히 의존할 때, LKF의 결과는 실제 값과 로봇 센서에서 보고된 오도메트리에서 크게 벗어납니다. 반면에, EKF는 관측 데이터에 더 가까이 머물며 훨씬 나은 성능을 발휘합니다. 또한 오른쪽 그림의 큰 타원에서 나타나는 바와 같이 EKF는 높은 불확실성을 정확히 나타냅니다. 반면에, LKF는 낮은 불확실성을보고하는데, 이것은 부정확합니다.</p>
<h2>대안 운동 모델</h2>
<div class="content-ad"></div>
<p>과거 두 필터에서 사용된 모션 모델은 꽤 간단하며 로봇이 헤딩 각도 θ(세타)의 방향으로 직선으로 이동한다고 가정합니다. 보다 정교한 모션 모델이 존재하며, 그 모델이 더 나은 성능을 발휘할 수 있는지 확인하는 것이 중요합니다. 다음에 탐구할 새로운 상수 속도 모션 모델은 로봇을 변환(직선) 및 회전(각도) 속도 v 및 ω를 통해 제어할 수 있도록 합니다. 직선 이동을 가정하는 대신, 이 모델은 아래에 나와 있는 것처럼 반지름이 r인 원 위를 로봇이 이동한다고 가정합니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_28.png" alt="image"></p>
<p>직선 모션 가정과 같이 원형 모션 가정도 근사값일 뿐이며, 시간 간격이 매우 작은 경우에만 유효합니다(움직임이 원이건 직선이건 구분할 수 없을 정도로 아주 작을 때). ω 값이 0에 가까워질수록, 반지름은 아주 크게되어 거의 직선 상에서의 움직임을 나타내게 됩니다. 시간에 따라 상태를 진화시키기 위해 이 모델을 따르는 비선형 함수 g에 해당하는 방정식 벡터는 아래에 나와 있습니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_29.png" alt="image"></p>
<div class="content-ad"></div>
<p>이제이 모델을 선형화하기 위해 g의 자코비안을 계산해야 합니다. 이는 상태에 대한 g의 편도함수에 해당합니다. 우리의 상태는 다음과 같습니다:</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_30.png" alt="State"></p>
<p>따라서 자코비안 G는 다음과 같이 표현됩니다:</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_31.png" alt="Jacobian"></p>
<div class="content-ad"></div>
<p>위에서 보았듯이, 관련 행렬을 결정한 후에 구현은 간단합니다. 먼저, 아래 속도 모델 코드를 정의합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">velocity_motion_model_linearized_2</span>():

 def <span class="hljs-title function_">state_transition_function_g</span>(mu = <span class="hljs-title class_">None</span>, u = <span class="hljs-title class_">None</span>, delta_t = <span class="hljs-title class_">None</span>):
  x = mu[<span class="hljs-number">0</span>]
  y = mu[<span class="hljs-number">1</span>]
  theta = mu[<span class="hljs-number">2</span>]
  
  v = u[<span class="hljs-number">0</span>]       
  w = u[<span class="hljs-number">1</span>]       
  <span class="hljs-keyword">if</span> w == <span class="hljs-number">0</span>:
   w = <span class="hljs-number">1e-6</span>   # 직선 이동의 경우 <span class="hljs-number">0</span>으로 나누는 것을 피하기 위해

  g = np.<span class="hljs-title function_">array</span>([
     x + -v/w * np.<span class="hljs-title function_">sin</span>(theta) + v/w * np.<span class="hljs-title function_">sin</span>(theta + w * delta_t),
     y + v/w * np.<span class="hljs-title function_">cos</span>(theta) - v/w * np.<span class="hljs-title function_">cos</span>(theta + w * delta_t),
     theta + w * delta_t
  ])

  <span class="hljs-keyword">return</span> g

 def <span class="hljs-title function_">jacobian_of_g_wrt_state_G</span>(mu = <span class="hljs-title class_">None</span>, u = <span class="hljs-title class_">None</span>, delta_t = <span class="hljs-title class_">None</span>):
  theta = mu[<span class="hljs-number">2</span>]
  v = u[<span class="hljs-number">0</span>]       
  w = u[<span class="hljs-number">1</span>]       
  <span class="hljs-keyword">if</span> w == <span class="hljs-number">0</span>:
   w = <span class="hljs-number">1e-6</span>   # 직선 이동의 경우 <span class="hljs-number">0</span>으로 나누는 것을 피하기 위해

  G = np.<span class="hljs-title function_">array</span>([
   [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -v / w * np.<span class="hljs-title function_">cos</span>(theta) + v / w * np.<span class="hljs-title function_">cos</span>(theta + w * delta_t)],
   [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, -v / w * np.<span class="hljs-title function_">sin</span>(theta) + v / w * np.<span class="hljs-title function_">sin</span>(theta + w * delta_t)],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
  ])

  <span class="hljs-keyword">return</span> G

 <span class="hljs-keyword">return</span> state_transition_function_g, jacobian_of_g_wrt_state_G
</code></pre>
<p>관측 모델은 이전 예제에서 사용한 것과 정확히 동일하며 완전성을 위해 여기에 다시 제시하겠습니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">odometry_observation_model_linearized</span>():
 def <span class="hljs-title function_">observation_function_h</span>(mu):
  <span class="hljs-keyword">return</span> mu
 
 def <span class="hljs-title function_">jacobian_of_h_wrt_state_H</span>(mu):
  <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">eye</span>(<span class="hljs-number">3</span>)

 <span class="hljs-keyword">return</span> observation_function_h, jacobian_of_h_wrt_state_H
</code></pre>
<div class="content-ad"></div>
<p>마침내 두 번째 확장 칼만 필터가 아래에 구현되었습니다. 코드는 이전 것과 정확히 동일하지만 움직임 모델을 정의하는 라인만 예외입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">KalmanFilter</span>:

 def <span class="hljs-title function_">__init__</span>(self, initial_state, initial_covariance, proc_noise_std = [<span class="hljs-number">0.02</span>, <span class="hljs-number">0.02</span>, <span class="hljs-number">0.01</span>], obs_noise_std = [<span class="hljs-number">0.02</span>, <span class="hljs-number">0.02</span>, <span class="hljs-number">0.01</span>]):

  self.<span class="hljs-property">mu</span> = initial_state # 초기 상태 추정
  self.<span class="hljs-property">Sigma</span> = initial_covariance # 초기 불확실성

  self.<span class="hljs-property">g</span>, self.<span class="hljs-property">G</span> = <span class="hljs-title function_">velocity_motion_model_linearized_2</span>() # 사용할 액션 모델
  
  # 프로세스 또는 액션 모델 노이즈의 표준 편차
  self.<span class="hljs-property">proc_noise_std</span> = np.<span class="hljs-title function_">array</span>(proc_noise_std)
  # 프로세스 노이즈 공분산 (R)
  self.<span class="hljs-property">R</span> = np.<span class="hljs-title function_">diag</span>(self.<span class="hljs-property">proc_noise_std</span> ** <span class="hljs-number">2</span>) 

  self.<span class="hljs-property">h</span>, self.<span class="hljs-property">H</span> = <span class="hljs-title function_">odometry_observation_model_linearized</span>() # 사용할 관측 모델

  # 관측 또는 센서 모델 노이즈의 표준 편차
  self.<span class="hljs-property">obs_noise_std</span> = np.<span class="hljs-title function_">array</span>(obs_noise_std)
  # 관측 노이즈 공분산 (Q)
  self.<span class="hljs-property">Q</span> = np.<span class="hljs-title function_">diag</span>(self.<span class="hljs-property">obs_noise_std</span> ** <span class="hljs-number">2</span>)

 def <span class="hljs-title function_">predict</span>(self, u, dt):
  # 상태 추정 (mu) 예측
  self.<span class="hljs-property">mu</span> = self.<span class="hljs-title function_">g</span>(self.<span class="hljs-property">mu</span>, u, dt)
  # 공분산 (<span class="hljs-title class_">Sigma</span>) 예측
  self.<span class="hljs-property">Sigma</span> = self.<span class="hljs-title function_">G</span>(self.<span class="hljs-property">mu</span>, u, dt) @ self.<span class="hljs-property">Sigma</span> @ self.<span class="hljs-title function_">G</span>(self.<span class="hljs-property">mu</span>, u, dt).<span class="hljs-property">T</span> + self.<span class="hljs-property">R</span> 

  <span class="hljs-keyword">return</span> self.<span class="hljs-property">mu</span>, self.<span class="hljs-property">Sigma</span>

 def <span class="hljs-title function_">update</span>(self, z, dt):
  # 칼만 이득 (K) 계산
  K = self.<span class="hljs-property">Sigma</span> @ self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>).<span class="hljs-property">T</span> @ np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">inv</span>(self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>) @ self.<span class="hljs-property">Sigma</span> @ self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>).<span class="hljs-property">T</span> + self.<span class="hljs-property">Q</span>)
  
  # 상태 추정 (mu) 업데이트
  self.<span class="hljs-property">mu</span> = self.<span class="hljs-property">mu</span> + K @ (z - self.<span class="hljs-title function_">h</span>(self.<span class="hljs-property">mu</span>))

  # 공분산 (<span class="hljs-title class_">Sigma</span>) 업데이트
  I = np.<span class="hljs-title function_">eye</span>(<span class="hljs-title function_">len</span>(K)) 
  self.<span class="hljs-property">Sigma</span> = (I - K @ self.<span class="hljs-title function_">H</span>(self.<span class="hljs-property">mu</span>)) @ self.<span class="hljs-property">Sigma</span>

  <span class="hljs-keyword">return</span> self.<span class="hljs-property">mu</span>, self.<span class="hljs-property">Sigma</span>
</code></pre>
<p>성능:</p>
<p>이 시점에서 EKF가 LKF보다 우월함이 명확하며, 이제는 더 정교한 움직임 모델이 뚜렷한 향상을 가져오는지를 결정하는 것에 중점을 두고 연구를 계속할 것입니다. 관측 모델이 명백히 움직임 모델보다 우선하는 잡음 설정에서 평가를 수행하지 않을 것이며, 이렇게 하면 모든 필터에 대해 단순히 관측 결과와 일치할 것으로 예상됩니다. 따라서 여기서는 관측을 대부분 무시하고 움직임 모델에 더 집중하는 잡음 설정으로 결과를 제시합니다.</p>
<div class="content-ad"></div>
<p>놀랍게도 더 고급 모델이 더 간단한 모델보다 우수한 성능을 내지 못했습니다. 위의 두 개의 도표에서는 순수한 눈으로는 중요한 성능 차이를 알아차리기 힘듭니다. 심지어 보고된 잡음 타원도 거의 같아 보입니다.</p>
<p>이전의 기사에서 얻은 결론 중 하나는 우리도 이곳에서 도출하는 것인데, 어떻게 소위 세련되고 고급스러운 필터를 사용하더라도 잘못된 데이터를 입력하면 잘못된 추정값을 제공할 것이라는 것입니다. 우리의 오도메트리 관측에 문제가 있다는 사실을 조정할 방법이 없네요. 이전의 모든 도표에서 주요 문제가 헤딩인 것처럼 보입니다. 올바른 헤딩 데이터가 없으면 어떤 고급 움직임 모델도 이를 보상할 수 없을 겁니다. 로봇이 헤딩의 대안 소스를 제공한다면 (그리고 다른 유용한 센서 데이터도 제공한다면), 필터를 사용하여 서로 다른 소스를 융합하고 더 나은 추정값을 얻을 수 있습니다. 다음에는 센서 융합을 살펴보겠습니다.</p>
<h2>센서 융합의 힘</h2>
<p>센서 융합에는 다양한 종류가 있으며, 우리가 적용할 유형은 저수준 융합으로 알려져 있습니다. 저수준 데이터 융합의 목표는 함께 더 유익한 여러 소스의 기본 센서 데이터를 결합하는 것입니다. 이 아이디어는 서로 다른 소스의 강점을 활용하여 더 나은 추정값을 만들어내는 것입니다.</p>
<div class="content-ad"></div>
<p>이 기사에서는 원래 오도메트리 데이터와 관성 측정 장치(IMU)에서 오는 데이터를 어떻게 결합하는지에 대해 연구할 것입니다. IMU는 일반적으로 우리에게 방향, 각속도 및 선형 가속도를 제공할 수 있습니다. 이는 우리가 부정확한 추정이 나쁜 방향 정보 때문인 것으로 의심하기 때문에 매우 유익합니다. IMU는 이러한 종류의 정보에 대해 오도메터보다 정확할 경향이 있기 때문에, IMU 데이터가 오도메트리와 어떻게 융합되어 더 나은 결과를 얻을 수 있는지 알아보겠습니다.</p>
<p>우리가 할 첫 번째 일은 상태와 모델을 확장하는 것입니다. 이전에는 추가 데이터의 좋은 소스가 없었기 때문에 로봇의 자세 이상을 고려하는 것이 별 의미가 없었습니다. 그러나 이제 우리에게 각속도와 선형 가속도를 제공할 수 있는 센서가 있기 때문에, 새로운 정보를 상태에 통합하는 것이 로봇의 상태를 더 잘 추정하는 데 도움이 될 것입니다. 이는 또한 우리의 동작 및 센서 모델을 확장할 것이기 때문에 필수적입니다. 이 기사에서는 새로운 7차원 상태로 시작하는 두 가지 다른 확장을 시도할 것입니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_32.png" alt="image"></p>
<p>여기서 x, y 및 θ는 친숙한 로봇 자세에 해당합니다. 과거 구현과는 달리, 속도 v와 각속도 ω가 제어 입력에서만 얻어졌던 경우, 이제 이들의 추정치도 상태의 일부로 유지합니다. 상태에는 x 및 y의 선형 가속도도 포함됩니다. 확장된 상태를 고려할 때, 동작 모델도 각 상태 구성 요소를 계산하기 위해 확장되어야 합니다. 가속도를 고려하는 동작 모델은 더 이상 상수 속도 모델이 될 수 없습니다. 따라서 속도는 추정 사이에 변화하고, 가속도는 일정하게 유지된다는 새로운 가속도 일정 모델을 사용할 것입니다. 이 새로운 가속도 상수 모델은 아래 표시된 함수 g와 야코비안 행렬 G에 의해 표현됩니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_33.png" alt="image"></p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_34.png" alt="image"></p>
<p>지금부터 조금 복잡해집니다. 이 큰 야코비안 행렬을 계산하는 것은 매우 에러가 발생하기 쉽습니다. 수동으로 시도해보는 것도 재미있지만, 항상 결과를 확인하기 위해 소프트웨어를 사용하는 것이 좋은 생각입니다. 계산이 올바르다고 확신하고 구현한 후에도 해당 구현이 정확한지 확인할 방법이 있어야 합니다. 이 기사에 구현된 모든 야코비안에 대해, 테스트 스크립트가 잡아낸 적어도 하나의 작은 오류가 있었습니다. 야코비안을 계산하고 구현이 올바른지 확인하는 데 소프트웨어를 사용하지 않으면 코드에 버그를 도입할 확률이 높습니다.</p>
<p>상태 및 운동 모델을 성공적으로 확장했습니다. 이제 센서 퓨전을 가능하게 하려면 관찰 모델도 확장해야 합니다. 확장된 상태 및 운동 모델과 결합하여 퓨전을 수행할 수 있도록 두 센서의 데이터를 연결하려 할 것입니다. 새로운 관찰 모델은 아래에 표시된 벡터의 첫 세 요소인 보통의 로봇 자세와 IMU 센서에서 파생된 방향 θ_imu, 각 속도 ω, 그리고 x 및 y 구성 요소에서의 가속도 a_x 및 a_y와 조합된 것입니다. 우리가 선속도 v를 직접 관측하지는 않지만 여전히 상태에 있을 수 있으며, Kalman 필터에 의해 운동 모델을 통해 추론될 것입니다. 이와 같은 관측되지 않는 변수를 숨겨진 또는 잠재 변수라고 부르는 경우가 종종 있습니다.</p>
<div class="content-ad"></div>
<p>다음은 아래에 정의된 자코비안 행렬입니다. 관측값에서 상태 변수로의 직접적인 매핑이 있으므로, 자코비안에서 관측값(행)이 상태 변수(열)에 해당될 때는 1이 있습니다. 자코비안에서 상태 변수 θ에 해당하는 3열에는 두 개의 1이 있음을 주목해 주세요 — odometry에서 θ에 대한 행과 IMU에서 θ에 대한 행이 각각 하나씩 있습니다. 또한, 선형 속도에 해당하는 4열은 직접 관측하지 않기 때문에 모두 0입니다.</p>
<p>구현된 모션 및 관측 모델은 아래에 나와 있습니다. 필터의 구현은 이전 것과 매우 유사하므로 간결함을 위해 생략하겠습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">def <span class="hljs-title function_">acceleration_motion_model_linearized_1</span>():

 def <span class="hljs-title function_">state_transition_function_g</span>(mu = <span class="hljs-title class_">None</span>, u = <span class="hljs-title class_">None</span>, delta_t = <span class="hljs-title class_">None</span>):
  
  x, y, theta, v, w, a_x, a_y = mu

  v = u[<span class="hljs-number">0</span>]      
  w = u[<span class="hljs-number">1</span>] 
  
  g = np.<span class="hljs-title function_">array</span>([
   x + v * np.<span class="hljs-title function_">cos</span>(theta) * delta_t + <span class="hljs-number">0.5</span> * a_x * delta_t**<span class="hljs-number">2</span>,      
      y + v * np.<span class="hljs-title function_">sin</span>(theta) * delta_t + <span class="hljs-number">0.5</span> * a_y * delta_t**<span class="hljs-number">2</span>,    
      theta + w * delta_t,
      v + a_x * np.<span class="hljs-title function_">cos</span>(theta) * delta_t + a_y * np.<span class="hljs-title function_">sin</span>(theta) * delta_t,
      w,                                                      
      a_x,                                                              
      a_y
  ])

  <span class="hljs-keyword">return</span> g

 def <span class="hljs-title function_">jacobian_of_g_wrt_state_G</span>(mu = <span class="hljs-title class_">None</span>, u = <span class="hljs-title class_">None</span>, delta_t = <span class="hljs-title class_">None</span>):
  x, y, theta, v, w, a_x, a_y = mu

  v = u[<span class="hljs-number">0</span>]       
  w = u[<span class="hljs-number">1</span>]       

  G = np.<span class="hljs-title function_">array</span>([[<span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, -delta_t * v * np.<span class="hljs-title function_">sin</span>(theta), delta_t  * np.<span class="hljs-title function_">cos</span>(theta), <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>*delta_t**<span class="hljs-number">2</span>, <span class="hljs-number">0.0</span>],   
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, delta_t * v * np.<span class="hljs-title function_">cos</span>(theta), delta_t * np.<span class="hljs-title function_">sin</span>(theta), <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>*delta_t**<span class="hljs-number">2</span>],       
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, delta_t, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],                                      
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, -delta_t * a_x * np.<span class="hljs-title function_">sin</span>(theta) + delta_t * a_y * np.<span class="hljs-title function_">cos</span>(theta), 
                   <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, delta_t * np.<span class="hljs-title function_">cos</span>(theta), delta_t * np.<span class="hljs-title function_">sin</span>(theta)],                  
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],                                          
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>],                                          
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>]])                                         
  
  <span class="hljs-keyword">return</span> G

 <span class="hljs-keyword">return</span> state_transition_function_g, jacobian_of_g_wrt_state_G
</code></pre>
<pre><code class="hljs language-js">def <span class="hljs-title function_">odometry_imu_observation_model_with_acceleration_motion_model_linearized_1</span>():
 def <span class="hljs-title function_">observation_function_h</span>(mu):
  x, y, theta, v, w, ax, ay = mu
  <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">array</span>([[x], [y], [theta], [theta], [w], [ax], [ay]]
 
 def <span class="hljs-title function_">jacobian_of_h_wrt_state_H</span>():
  <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">array</span>([[<span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],    
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],        
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>]])

 <span class="hljs-keyword">return</span> observation_function_h, jacobian_of_h_wrt_state_H
</code></pre>
<p>성능:</p>
<p>더 정교한 모션 모델을 사용하고 오도메트리 데이터와 IMU 데이터를 퓨즈하는 개선된 EKF를 평가하는 시간입니다. 특히, 노이즈 매개변수를 다음과 같이 설정하겠습니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">proc_noise_std = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>] # [x, y, theta, v, w, a_x, a_y]
obs_noise_std = [<span class="hljs-number">100.0</span>, <span class="hljs-number">100.0</span>, <span class="hljs-number">1000.0</span>, <span class="hljs-number">6.853891945200942e-06</span>, <span class="hljs-number">1.0966227112321507e-06</span>, <span class="hljs-number">0.0015387262937311438</span>, <span class="hljs-number">0.0015387262937311438</span>] #[x, y, theta, theta_imu, w, a_x, a_y]
</code></pre>
<p>이 설정은 우리의 모션 모델에 대해 높은 신뢰를 가지고 있지만 관측 모델에 대해서는 IMU 데이터를 오도메트리 데이터보다 더 신뢰한다고 필터에 알려줍니다. 특히, 우리는 필터에게 오도메트리에서의 헤딩이 미친 것이라고 생각하고 IMU에서의 헤딩이 매우 정확하다고 말하고 있습니다. 이는 IMU가 매우 정밀할 수 있고 오도메트리 데이터가 악명 높게 나쁠 수 있기 때문에 종종 사실입니다. 이 설정은 우리가 예측한 궤적을 수정하는 데 도움이 될 수 있는데, 이전에 본 것처럼 모양은 그리 나쁘지 않지만 방향은 매우 잘못된 경우가 많습니다.</p>
<p>아래 비디오와 이어지는 두 그림에서 볼 수 있듯이 가속도 모델과 센서 퓨전이 포함된 새 필터는 이전 필터보다 훨씬 더 잘 수행됩니다. 특히 IMU로부터 제공된 더 나은 방향성 덕분에 추정 궤적이 초기에 실제 궤적에 훨씬 가까웠음을 볼 수 있습니다. 그러나 마지막에는 지그재그 패턴을 따르기 시작했습니다.</p>
<p>더 나은 결과를 얻기 위한 레시피는 없습니다. 이것이 필터를 설계하는 것을 과학보다는 예술로 만드는 것입니다. 위의 지그재그 궤적을 개선할 수 있는 더 나은 모션 모델로 수정할 수 있을 것이라고 생각할 수 있습니다. 이 결과를 본 것처럼 나도 같은 방식으로 느꼈습니다. 모션 모델을 개선해 보겠습니다.</p>
<div class="content-ad"></div>
<h2>모션 모델 개선하기</h2>
<p>센서 퓨전은 도움이 되고 있지만, 아직 개선할 부분이 많이 남아 있습니다. 상태 추정을 위한 필터를 설계하려면 매우 교육된 추측을 하고 직관을 따라야 합니다. 앞서 논의한 대로, 최근 얻은 결과는 격려적이며 우리가 모델을 개선해 보아야 한다는 제안이 있습니다. 특히 상태를 모델링할 때 글로벌 선속도만을 고려하고, 가속도와 같이 다른 축을 따른 속도를 고려하지 않는 것으로 보입니다. 우리의 상태 벡터를 확장하여 이를 고려해보겠습니다.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_37.png" alt="이미지"></p>
<p>새로운 상태 벡터에는 로봇의 움직임에 대한 더 자세한 정보를 제공할 x와 y 성분의 속도가 포함되어 있습니다. 이 확장이 필터의 성능에 어떤 영향을 미치는지 알아보겠습니다. g 함수와 해당 야코비안 G을 이용하여 표현된 확장된 모션 모델은 아래와 같습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_38.png" alt="Image"></p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_39.png" alt="Image"></p>
<p>The observation model function h remains the same, but the Jacobian H now has two columns filled with zeros corresponding to the unobserved state variables v_x and v_y.</p>
<p><img src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_40.png" alt="Image"></p>
<div class="content-ad"></div>
<p>모델과 사용된 잡음 매개변수에 대한 코드는 다음과 같습니다. 여기서 추가적으로 필터 코드는 이전에 소개된 코드와 매우 유사하므로 생략될 것입니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">acceleration_motion_model_linearized_2</span>():

 def <span class="hljs-title function_">state_transition_function_g</span>(mu = <span class="hljs-title class_">None</span>, u = <span class="hljs-title class_">None</span>, delta_t = <span class="hljs-title class_">None</span>):
  
  x, y, theta, v_x, v_y , w, a_x, a_y = mu

  v = u[<span class="hljs-number">0</span>]       
  w = u[<span class="hljs-number">1</span>]       
  
  g = np.<span class="hljs-title function_">array</span>([
      x + v * np.<span class="hljs-title function_">cos</span>(theta) * delta_t + <span class="hljs-number">0.5</span> * a_x * delta_t**<span class="hljs-number">2</span>,  
      y + v * np.<span class="hljs-title function_">sin</span>(theta) * delta_t + <span class="hljs-number">0.5</span> * a_y * delta_t**<span class="hljs-number">2</span>,  
      theta + w * delta_t,                                   
      v * np.<span class="hljs-title function_">cos</span>(theta) + a_x * delta_t,                         
      v * np.<span class="hljs-title function_">sin</span>(theta) + a_y * delta_t,                         
      w,                                                         
      a_x,                                                       
      a_y                                                        
  ])

  <span class="hljs-keyword">return</span> g

 def <span class="hljs-title function_">jacobian_of_g_wrt_state_G</span>(mu = <span class="hljs-title class_">None</span>, u = <span class="hljs-title class_">None</span>, delta_t = <span class="hljs-title class_">None</span>):
  x, y, theta, v_x, v_y, w, a_x, a_y = mu

  v = u[<span class="hljs-number">0</span>]       
  w = u[<span class="hljs-number">1</span>]       

  G = np.<span class="hljs-title function_">array</span>([[<span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, -delta_t * v * np.<span class="hljs-title function_">sin</span>(theta), <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>*delta_t**<span class="hljs-number">2</span>, <span class="hljs-number">0.0</span>],   
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, delta_t * v * np.<span class="hljs-title function_">cos</span>(theta), <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>*delta_t**<span class="hljs-number">2</span>],        
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, delta_t, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],                                      
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, -v * np.<span class="hljs-title function_">sin</span>(theta), <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, delta_t, <span class="hljs-number">0.0</span>],                       
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, v * np.<span class="hljs-title function_">cos</span>(theta), <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, delta_t],                        
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],                                          
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>],                                          
                   [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>]])                                         
  
  <span class="hljs-keyword">return</span> G
 
 <span class="hljs-keyword">return</span> state_transition_function_g, jacobian_of_g_wrt_state_G
</code></pre>
<pre><code class="hljs language-js">def <span class="hljs-title function_">odometry_imu_observation_model_with_acceleration_motion_model_linearized_2</span>():
 def <span class="hljs-title function_">observation_function_h</span>(mu):
  x, y, theta, v_x, v_y, w, ax, ay = mu
  <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">array</span>([[x], [y], [theta], [theta], [w], [ax], [ay]]
 
 def <span class="hljs-title function_">jacobian_of_h_wrt_state_H</span>():
  <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">array</span>([[<span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],       
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],            
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>],         
                    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>]])
 
 <span class="hljs-keyword">return</span> observation_function_h, jacobian_of_h_wrt_state_H
</code></pre>
<pre><code class="hljs language-js">proc_noise_std = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>] # [x, y, theta, v_x, v_y, w, a_x, a_y]
obs_noise_std = [<span class="hljs-number">100.0</span>, <span class="hljs-number">100.0</span>, <span class="hljs-number">1000.0</span>, <span class="hljs-number">6.853891945200942e-06</span>, <span class="hljs-number">1.0966227112321507e-06</span>, <span class="hljs-number">0.0015387262937311438</span>, <span class="hljs-number">0.0015387262937311438</span>] #[x, y, theta, theta_imu, w, a_x, a_y]
</code></pre>
<div class="content-ad"></div>
<p>성능:</p>
<p>새로운 필터는 이전 동영상과 아래 그림에서 명백히 확인할 수 있듯이 인상적인 성과를 보여주었습니다. 예측된 궤적이 이제 센서 퓨전과 더 정교한 모델의 덕분에 실제 지면에 매우 가까워졌습니다. 게다가 그림의 최신 자세에서 타원으로 표시된 최종 불확실성은 이전 필터 버전보다 훨씬 작습니다. 이는 센서 퓨전이 잘 설계된 EKF의 효과를 보여줍니다. 추가로 소음 매개변수를 조정하면 더 많은 개선이 가능하지만, 이는 기사의 길이 때문에 여기서 탐구되지 않을 것입니다.</p>
<h2>직접 시도해보세요</h2>
<p>이전 기사와 마찬가지로, 코드는 전체 코드를 검사하거나 단순히 알고리즘을 실행하고 결과를 실시간으로 확인하려는 사람들을 위해 제공됩니다. 코드를 실행하려면 아래 지시사항을 따르십시오.</p>
<div class="content-ad"></div>
<p>먼저 필요한 것은 ROS 2입니다. ROS 2 Humble을 Ubuntu Jammy Jellyfish (22.04)에 설치하는 방법은 여기에서 찾을 수 있습니다. Ubuntu의 다른 버전이나 다른 운영 체제의 경우 공식 ROS 2 문서를 참고하십시오.</p>
<p>데이터를 얻으려면 이 링크에서 ROS 2 가방을 다운로드해야 합니다. 사용하기 전에 파일을 압축 해제해야 합니다.</p>
<p>마지막으로 ROS 2 패키지를 복제하고 빌드해야 합니다. 아래 단계를 따라 진행할 수 있습니다. ros2_ws를 실제 ROS 2 작업 공간으로 교체해야 합니다.</p>
<h1>종속성 설치</h1>
<p>sudo apt install python3-pykdl</p>
<h1>패키지를 복제하고 빌드</h1>
<p>cd ros2_ws/src
git clone <a href="https://github.com/carlos-argueta/rse_prob_robotics.git" rel="nofollow" target="_blank">https://github.com/carlos-argueta/rse_prob_robotics.git</a>
cd ..
colcon build --symlink-install</p>
<div class="content-ad"></div>
<p>확장 칼만 필터를 실행하려면 3개의 다른 터미널을 열어야 합니다.</p>
<p>터미널 1에서 (ros2_ws를 실제 워크스페이스로 교체해주세요) 다음 명령을 실행하여 Rviz를 열고 로봇이 보고 있는 것을 확인하세요.</p>
<pre><code class="hljs language-js">source ~<span class="hljs-regexp">/ros2_ws/i</span>nstall/setup.<span class="hljs-property">bash</span>
ros2 launch rse_gaussian_filters rviz_launch.<span class="hljs-property">launch</span>.<span class="hljs-property">py</span>
</code></pre>
<p>터미널 2에서 확장 칼만 필터의 버전에 따라 다음 명령 중 하나를 실행하세요. 먼저 아무 출력도 나오지 않을 것이며, ROS 2 가방(bag)을 실행할 때까지 기다리세요.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">source ~<span class="hljs-regexp">/ros2_ws/i</span>nstall/setup.<span class="hljs-property">bash</span>

# 아래 명령어 중 하나 실행

# 3D 상태, 기본 속도 모델
ros2 run rse_gaussian_filters ekf_estimation_3d_v1 

# 3D 상태, 고급 속도 모델
ros2 run rse_gaussian_filters ekf_estimation_3d_v2 

# 7D 상태, 가속 모델, 센서 퓨전
ros2 run rse_gaussian_filters ekf_estimation_7d 

# 8D 상태, 가속 모델, 센서 퓨전
ros2 run rse_gaussian_filters ekf_estimation_8d 
</code></pre>
<p>터미널 3에서 ROS 2 가방이 추출된 위치로 이동하여 다음 명령어로 재생하십시오. "Ignoring a topic '/navrelposned', reason: package 'ublox_msgs' not found"와 같은 경고 메시지를 무시해도 됩니다.</p>
<pre><code class="hljs language-js">ros2 bag play linkou-<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>-<span class="hljs-number">2</span>-med --clock
</code></pre>
<p>위 단계를 따르면 확장 칼만 필터를 실행하고 결과를 실시간으로 확인할 수 있습니다.</p>
<div class="content-ad"></div>
<h1>실용적인 고려사항 및 마지막으로</h1>
<p>이 글은 확장 칼만 필터 (EKF)를 기본적인 선형 칼만 필터 (LKF)의 확장으로 소개했습니다. EKF는 대부분의 실세계 시스템의 비선형성을 근사화를 통해 다룹니다. 이는 사후 평균 주변의 비선형 함수를 일차 테일러 전개를 이용하여 선형화합니다. 선형화된 후, EKF는 LKF와 유사하게 작동합니다.</p>
<p>보여진 대로, 센서 퓨전과 결합된 EKF는 놀라운 결과를 얻을 수 있습니다. 그러나 좋은 필터를 설계하는 것은 어렵고 오류를 범하기 쉽습니다. 종종 과학과 예술 사이의 균형을 요구합니다. 적합한 움직임 및 관측 모델을 찾는 것이 첫 번째 난관입니다. 이동 로봇에 대한 사용 가능한 모델이 있더라도, 다른 시나리오는 좋은 모델이 부족할 수 있습니다. 자코비안 계산 또한 어렵고 실수하기 쉬우며, 소프트웨어 확인이 필요함을 강조합니다.</p>
<p>효과적인 EKF 설계를 위해 여러 다른 고려사항과 결정이 중요합니다. 한 가지 중요한 측면은 올바른 시간 간격(delta_t)을 선택하는 것으로, 효과적인 선형화를 위해 충분히 작아야 합니다. 이 경우, 고정값 대신 동적 delta_t가 사용되었습니다. 필터의 업데이트 단계를 수행할 때 언제, 어떻게 결정하는지도 중요합니다. 특히 서로 다른 속도로 측정 값을 제공하는 다른 센서가 있을 때 (예: IMU는 보통 오도메트리보다 높은 주파수로), 적절한 노이즈 매개 변수 선택도 여러 가지 방법이 가능한 복잡한 작업입니다.</p>
<div class="content-ad"></div>
<p>요약하자면, 확장 칼만 필터는 특히 센서 퓨전과 함께 사용될 때 우수한 상태 추정 결과를 제공할 수 있습니다. 그러나 견고한 EKF를 설계하려면 상당한 노력과 연습이 필요합니다. 다음 글에서는 Unscented Kalman Filter (UKF)를 소개할 예정입니다. UKF는 EKF보다 여러 장점을 제공합니다. Unscented Transform을 사용하여 비선형 변환의 평균과 공분산을 더 정확하게 캡처합니다. UKF는 Jacobian을 필요로하지 않아 구현을 간단하게 만들어줍니다. 선형화로 인한 근사 오차를 줄이므로 매우 비선형 시스템에 더 효과적입니다. 또한 UKF는 비가우시안 분포를 더 잘 처리하여 견고성과 다양성을 향상시킵니다.</p>
<h1>독후감</h1>
<p>다음은 칼만 필터 패밀리에 대해 학습하기 위해 참고한 훌륭한 자료 목록입니다:</p>
<ul>
<li>Optimal State Estimation: Kalman, H∞, and Nonlinear Approaches</li>
<li>State Estimation for Robotics</li>
<li>Kalman and Bayesian Filters in Python</li>
<li>Probabilistic Robotics</li>
</ul>
<div class="content-ad"></div>
<p>이 기사가 도움이 되셨기를 바랍니다. 피드백이 있으시면 언제든지 댓글을 남겨주세요. 또한, 이후의 주제를 다룬 보다 심도 있는 강좌 시리즈를 시작하려고 합니다. 이 강좌는 비디오, 코딩 프로젝트 등이 포함될 예정이며 유료로 운영될 가능성이 높습니다. 이런 강좌에 관심이 있다면 댓글로 알려주시면 참여 의향을 파악할 수 있습니다.</p>
<p>저와 소통하고 싶다면 LinkedIn에서 저를 찾아보세요: <a href="https://www.linkedin.com/in/carlos-argueta" rel="nofollow" target="_blank">https://www.linkedin.com/in/carlos-argueta</a></p>
<p>저와 함께 ROS 2를 이용한 로보틱스를 배우고 싶으신가요? 제 라이브 강의에 참여해보세요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"로스 2에서 확장 칼만 필터를 활용한 센서 융합","description":"","date":"2024-06-19 06:22","slug":"2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2","content":"\n\n![Sensor Fusion with the Extended Kalman Filter in ROS2](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png)\n\n안녕하세요! 이 글은 가우시안 필터를 소개하는 시리즈 중 두 번째 글입니다. 구체적으로, 이 글은 칼만 필터 패밀리에 대한 세 번째 세부 소개입니다. 이미 칼만 필터에 대해 익숙하지 않다면, 계속하기 전에 첫 번째 글을 읽기를 권장합니다. 다음 글에서는 언센티드 칼만 필터를 소개할 예정입니다. 이 글의 결과를 재현하는 데 사용된 데이터와 코드는 이 글의 끝 부분에 찾을 수 있습니다.\n\n# 소개\n\n이전 글에서 소개된 대로, 성공적인 로봇 시스템은 유용한 작업을 수행하기 위해 물리적 세계를 인식하고 조작할 수 있어야 합니다. 이를 달성하기 위해 로봇은 환경의 중요한 불확실성을 고려해야 합니다. 현대 로봇 공학에서 가장 기본적인 문제 중 하나는 상태 추정입니다. 상태 추정은 로봇과 환경(랜드마크 및 기타 객체의 위치 등)의 가장 확률적인 상태(예: 위치, 방향, 속도)를 불확실한(잡음이 있는) 및 아마도 불완전한 정보를 기반으로 결정하는 것을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n칼만 필터는 불확실한 환경에서 상태 추정에 대응합니다. 필터는 상태의 각 요소 (예: x, y 좌표, 헤딩)을 가우시안 확률 변수로 모델링합니다. 가우시안은 상태 x에 대한 확률 밀도를 벡터 μ (뮤)와 공분산 행렬 Σ (시그마)만 사용하여 표현할 수 있게 합니다. 이 매개변수화에서 우리의 상태 x는 기대값 μ에 의해 표현되며, Σ는 제어, 이동 및 관측 잡음으로 인한 상태의 내재적 불확실성을 포착합니다.\n\n칼만 필터가 사용하는 베이지안 프레임워크에서는 전체 상태를 믿음(belief)이라고 합니다. 가우시안 (또는 정규분포)을 사용하는 장점은 그들의 수학적 성질에 있습니다. 이 성질은 칼만 필터 방정식을 단순화합니다. 가우시안 믿음이 선형 변환을 겪을 때의 특징 중요한데, 가우시안 믿음이 선형 변환을 겪으면 결과는 여전히 가우시안 확률 변수로 유지됩니다. 이 성질은 칼만 필터의 방정식이 우아하고 다루기 쉬운 상태를 유지하도록 보장합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_1.png)\n\n사후 믿음을 계산하기 위해 칼만 필터는 이전 믿음을 시간을 경과함에 따라 전달하는 모션 모델을 사용합니다. 그런 다음 관측 모델은 로봇 센서에서의 데이터를 통합하여 예측된 믿음을 업데이트하고 이를 사후로 변환합니다. 칼만 필터 알고리즘은 아래에서 요약됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Linearity of the Linear Kalman Filter](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_2.png)\n\n선형 칼만 필터의 \"선형성\"은 알고리즘의 2번째와 5번째 줄에서 가장 명백합니다. 2번째 줄에서 예측된 상태는 이전 상태 μ_t−1과 제어 입력 u_t의 선형 함수입니다. 5번째 줄에서 예측된 관측값 (y = Cμ)은 인수 μ^bar_t의 선형 함수입니다. 이 선형성은 가우시안 특성을 유지하여 필터를 구현하기 쉽게 만듭니다. 그러나 이것은 또한 주요 약점 중 하나를 나타냅니다.\n\n선형 칼만 필터가 실제 문제에 부적합한 이유는 실제 문제가 종종 선형적이지 않기 때문입니다. 이전 글에서 도입했던 간단한 상태 추정의 경우, 상태가 모바일 로봇의 2차원 포즈 (x, y, θ)로 표현되었지만 정확히 선형적이지 않았습니다. 아래에 다시 소개되는 이동 모델에서 확인할 수 있습니다.\n\n![Motion Model](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_4.png)\n\n모델이 쉽게 알아볼 수 있듯이, 삼각 함수가 사용되어 로봇의 좌표를 업데이트하는데 사용되며, 이러한 함수들은 선형이 아닙니다. 이 모델에서 선형 칼만 필터는 어느 순간 발산할 가능성이 있습니다. 이 이유로 선형 칼만 필터를 소개한 후, 대부분의 실제 현상의 비선형성을 고려하기 위해 확장 방법을 고안하는 작업이 즉시 시작되었습니다.\n\n# 확장 칼만 필터\n\n## 비선형성의 문제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비선형성에 관한 문제를 더 자세히 설명하기 위해 다음 애니메이션을 살펴볼 수 있습니다. 첫 번째 애니메이션에서는 선형 칼만 필터의 가정이 성립하는 세계에 있으며, 새로운 상태가 인수에 대해 단순하게 선형인 경우를 살펴봅니다. 시각화를 이해하기 쉽게 하기 위해 1차원 상태 x를 가정합니다. 이 애니메이션은 가우스를 선형 함수 g를 통과시켰을 때 다른 가우스가 되는 과정을 보여줍니다. 이 경우 g = -0.5*x+1입니다.\n\nx의 가우시안 표현에서 시작하지만 비선형 함수 g를 선택하는 경우, 결과 확률 밀도 함수는 더 이상 가우시안이 아닙니다. 새로운 밀도를 계산하기 위한 폐쇄형 방법이 없습니다. 대신 입력 분포에서 점들을 샘플링하고 이를 g를 통과시켜 출력 히스토그램을 구축하여 출력 분포를 만들어야 합니다. 아래에 표시된 출력의 형태에서 확인할 수 있듯이, 이는 가우시안이 아닌 것을 알 수 있습니다. 또한 이 출력 분포는 칼만 필터의 단봉성 가정을 위배하며, 단일 피크를 요구합니다. P(y)의 가우시안 근사는 출력 데이터에 가우시안을 맞추어 얻었습니다. 이는 실제 모델의 비선형성을 다루기 위한 선형 칼만 필터의 한계를 강조합니다.\n\n요약하면 비선형 모델을 다룰 때 출력 밀도는 가우시안이 아니며 폐쇄형으로 계산할 수 없으며 종종 다중 피크를 갖기 때문에 칼만 필터 방정식이 무용지물이 됩니다. 이 문제를 해결하기 위해 확장 칼만 필터(EKF)는 선형성 가정을 버립니다. 대신 상태 전이 확률과 측정 확률은 비선형 함수 g 및 h에 따라 결정됨을 수용합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_6.png)\n\n모든 것이 잘 진행되고 있어요. 칼만 필터의 우아한 방정식을 활용하려면 여전히 우리의 신뢰를 가우시안으로 표현해야 합니다. 이 필수성은 정확한 사후분포를 계산하는 것에서 EKF의 초점을 이동시켜 실제 신뢰의 좋은 가우시안 근사값을 찾게 됩니다. 아래 그림에서 보듯, 몬테칼로를 사용하여 출력 분포를 계산한 후, 그에 대한 가우시안을 fitting하고 필요한 매개변수 μ (뮤)와 Σ (시그마)를 얻을 수 있습니다. 그러나 아직도 가우시안을 닫힌 형태로 계산할 수 없는 문제가 남아 있습니다.\n\n## 테일러 전개를 통한 선형화\n\n이 문제를 해결하기 위해 확장 칼만 필터는 선형화라는 추가 근사값을 적용합니다. 선형화의 핵심 아이디어는 비선형 함수 g를 해당 관심점에서 g에 접하는 접선인 선형 함수로 근사하는 것입니다. 비선형 함수를 선형화하는 다양한 기술 중, EKF가 사용하는 것은 일차 테일러 전개입니다. 함수의 테일러 전개는 함수의 도함수를 하나의 점 a에서 표현된 다항식 항들의 무한 합입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 이미지에서 높은 차수의 테일러 전개가 점 a = 0 주변에서 g를 더 가까운 근사로 제공하는 것을 볼 수 있습니다. 그러나 고차 다항식이 늘어날수록 요구되는 계산도 증가하며, 문제가 빠르게 풀기 어려워집니다. 다행히도 Kalman 필터가 자주 업데이트되는 경우(작은 Δt), 관심점 a의 차이가 매우 작아야합니다. 따라서 우리는 다음 (매우 가까운) 각 지점 a에서의 함수 g의 값을 및 기울기(점 a에서의 미분)를 사용하여 함수 g의 선형 근사를 얻기 위해 1차 다항식(선)을 사용할 수 있습니다. 이 문제는 본질적으로 아래와 같이 간소화됩니다:\n\n\n![2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 동작이 왜 잘 작동하는지 설명하기 위해 g라는 함수를 가정해 볼게요.\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_10.png)\n\n점 a에서 함수 g의 일차 테일러 전개는 아래 그림에서 빨간색으로 표시되어 있어요. 큰 x 값 범위를 관찰하면 좋은 근사치를 제공하지 않음이 명백해요.\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 우리가 사후 분포의 값들에 대한 근사치에만 주의를 기울이기 때문에, 새로운 사후 분포에 대해 그러한 근사치를 매우 짧은 시간 이후에 다시 계산할 것을 알고 있기 때문에, 아래 그래프에서 볼 수 있듯이, 우리의 근사치가 관심 지점 주변에서 매우 좋다는 것을 알 수 있습니다. 이는 첫 번째 차수 테일러 전개가 우리의 목적에 대해 충분한 근사치를 제공하며, 시스템 내부의 비선형성에도 불구하고 확장 칼만 필터를 효과적으로 적용할 수 있도록 합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_12.png)\n\n마지막으로, 아래 이미지는 두 프로세스를 비교하려고 합니다: 첫 번째는 원래의 가우시안으로 시작하여 비선형 함수 g를 통과시키고, 몬테카를로를 사용하여 비가우시안 출력 분포를 얻은 후, 이 출력에 가우시안을 적합시킵니다. 두 번째로, EKF가 사용하는 프로세스는 g를 선형화시키고, 원래의 가우시안을 이 선형 근사치를 통해 통과시킨 후, 선형화를 통해 닫힌 형태로 출력을 직접 얻습니다. 이 비교는 비선형 시스템 다루기에 대한 EKF 접근 방식의 효율성과 실용성을 강조합니다.\n\n명확성을 위해, 아래에서는 출력만 표시됩니다. 보시다시피, EKF 가우시안이 몬테카를로 시뮬레이션에서 적합된 가우시안과 정확히 같지는 않지만, 충분히 가깝습니다. 이 작은 차이는 실제 분포의 닫힌 형태의 추정치를 효율적으로 얻기 위해 지불하는 대가입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 설명한 간단한 예제는 스칼라 경우에 대한 것이었지만, 우리의 상태는 벡터입니다. 따라서 기울기를 구하기 위해 우리는 상태에 대한 g의 편미분을 계산합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_13.png)\n\ng와 그 기울기 g′의 값은 그들의 인자 (u_t와 x_t-1)에 따라 달라지는데요, 이는 우리의 관심 지점입니다 (스칼라 경우의 a와 대조적입니다). u_t의 값에 대해서는 로봇에 제공된 제어 명령을 사용합니다. x_t-1에 대해서는 선형화할 시기에서 가장 가능성이 높은 상태의 값으로 선택할 수 있습니다. 가우시안의 경우, 최대 가능성 값은 이전 시간 단계에서 계산된 사후값의 평균인 μ_t-1로 표시됩니다. 이 선형화는 업데이트 속도가 매우 빠른 필터 (매우 작은 Δt)에 대해 잘 작동하며, 이때 μ_t-1의 값과 우리가 추정하려는 현재 상태 간의 차이가 크지 않을 때 잘 작동합니다.\n\n이제 기울기를 계산했기 때문에 g를 다음과 같이 추정할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![sensor fusion](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_14.png)\n\n가우시안에서, 모션 모델 또는 상태 전이 확률은 아래와 같이 표기됩니다. 여기서 R_t는 보통의 프로세스 노이즈 공분산입니다.\n\n![motion model](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_15.png)\n\n기울기가 숫자인 스칼라 케이스와는 달리, g'(u_t, μ_t-1)으로 알려진 G_t는 행렬입니다. 비선형 함수 g에 대한 상태 x의 일차 편미분값을 모두 포함하는 이 행렬은 야코비안이라고 합니다. 이 야코비안 행렬은 상태의 차원인 n×n의 크기를 가지며, 현재 제어 및 이전 사후 평균에 따라 값이 달라집니다. 따라서 야코비안 값은 시간이 지남에 따라 변합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장 칼만 필터(Extended Kalman Filter)는 함수 h에 의해 표현되는 비선형 관측 모델을 다룹니다. 특히, 타일러 전개(Taylor Expansion)는 새롭게 예측된 믿음 μ^bar_t을 중심으로 진행되며, 이는 h를 선형화하는 시점에서 가장 가능성이 높은 상태입니다.\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_16.png)\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_17.png)\n\n가우시안으로, 측정 모델은 아래와 같이 표기됩니다. 여기서 Q_t는 전통적인 측정 잡음 공분산입니다. 여기서 야코비안 H_t는 관측 모델의 비선형 함수 h에 대한 상태 x에 대한 일차 편미분의 m×n 행렬입니다. 따라서 m은 관측의 차원이고, n은 상태의 차원입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_18.png\" /\u003e\n\n## Extended Kalman Filter Algorithm\n\n요약하면, 아래에 표시된 EKF 알고리즘은 이 기사의 앞부분에 표시된 LKF 알고리즘과 매우 유사하지만, 주요한 차이점은 모션 및 관측 모델의 선형화가 2번 줄과 5번 줄에서 이루어진다는 것입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_19.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 이미지는 EKF 알고리즘을 LKF와 나란히 재진 다음 주요 차이점을 강조합니다. 예측 단계에서 EKF는 선형 시스템 행렬 A 및 B 대신 상태를 시간에 따라 진화시키는 비선형 함수 g를 사용합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter�...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모델 선형화\n\n선형 칼만 필터와 사용된 모션 모델은 정확히 선형은 아니지만 여전히 단순하여 LKF가 처리할 수 있는 간단한 상수 속도 모델이었습니다. 참고로, 아래에 다시 표시해 드립니다. 이 모델은 확장 칼만 필터의 능력을 보여주기 위해 사용될 더 복잡한 변형을 소개하는 기초 역할을 할 것입니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_21.png)\n\n이 방정식들은 다음과 같이 LKF에서 요구하는 선형 시스템 행렬로 변환되었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식입니다.\n\n\n![sensorfusion1](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_22.png)\n\nEKF를 사용하여 동일한 모델을 사용하려면 선형화해야 합니다. 먼저로봇의 상태 형식을 재정의합니다.\n\n![sensorfusion2](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_23.png)\n\n이후 비선형 함수 g를 정의합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_24.png\" /\u003e\n\n다음으로, 우리는 각 상태 변수 x, y, 그리고 θ에 대한 함수 g의 편도함수를 포함하는 야코비안 행렬 G를 정의합니다. 이 행렬은 상태 변수의 변화가 운동 모델에 어떻게 영향을 미치는지를 포착합니다. 야코비안 행렬 G는 EKF 알고리즘에서 공분산 행렬을 업데이트하는 데 사용될 것이며, 우리에게 운동 모델의 비선형성들을 고려하면서도 칼만 필터 프레임워크의 계산 효율성을 유지할 수 있게 해줍니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_25.png\" /\u003e\n\nx 업데이트 방정식의 x, y, 그리고 θ에 대한 도함수를 나타내는 야코비안 행렬 G의 첫 번째 행을 살펴보면, 다음과 같은 것을 볼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 첫 번째 항목은 x에 대한 부분 도함수이기 때문에 1입니다.\n- 두 번째 항목은 x가 y에 의존하지 않음을 나타내는 0입니다.\n- 세 번째 항목은 −vsin(θ)Δt이며, 이것은 x가 θ에 −vsin(θ)Δt항으로 의존함을 보여줍니다. 이것은 θ의 변화가 x 좌표에 미치는 영향을 반영합니다.\n\n다음으로 측정 함수 h가 필요합니다. 이전 Linear Kalman Filter의 구현과 유사하게, 사용할 측정은 로봇의 오도메트리 시스템에서 제공하는 것만 사용할 것입니다. 이 시스템은 바퀴 엔코더에서 계산된 로봇의 추정 자세를 직접 제공합니다. 그 데이터 형식이 우리의 상태 형식과 일치하기 때문에, h(μ^bar_t)는 단순히 μ^bar_t를 반환합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_26.png)\n\n그리고 상태에 대한 측정 함수의 부분 미분을 포함하는 Jacobian 행렬 H는 단위 행렬이며 아래와 같이 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![sensor fusion](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_27.png)\n\n이전 그림에서 EKF와 LKF를 비교한 것처럼, 선형화가 두 알고리즘의 주요 차이점입니다. 관련 기능 및 해당 야코비안을 확인한 후, EKF의 구현은 LKF의 구현을 밀접하게 따릅니다. 아래는 선형화된 속도 모션 모델의 Python 구현입니다. 행렬 g와 야코비안 G를 모두 반환합니다.\n\n```python\ndef velocity_motion_model_linearized_1():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  x = mu[0]\n  y = mu[1]\n  theta = mu[2]\n  \n  v = u[0]       \n  w = u[1]       \n  \n  g = np.array([\n            x + v * np.cos(theta) * delta_t,\n            y + v * np.sin(theta) * delta_t,\n            theta + w * delta_t\n        ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  theta = mu[2]\n  v = u[0]       \n  w = u[1]       \n  \n  G = np.array([\n   [1, 0, -v * np.sin(theta) * delta_t],\n   [0, 1, v * np.cos(theta) * delta_t],\n   [0, 0, 1]\n  ])\n\n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n다음으로, 다음과 같이 간단한 관측 모델을 구현합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef odometry_observation_model_linearized():\n def observation_function_h(mu):\n  return mu\n \n def jacobian_of_h_wrt_state_H(mu):\n  return np.eye(3)\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n마지막으로, 아래에 구현된 Extended Kalman Filter 코드를 확인해보세요. 직전 게시물에서 소개된 Linear Kalman Filter 코드와 얼마나 비슷한지 주목하세요.\n\n```js\nimport numpy as np \n\nfrom rse_motion_models.velocity_motion_models import velocity_motion_model_linearized_1\nfrom rse_observation_models.odometry_observation_models import odometry_observation_model_linearized\n\nclass KalmanFilter:\n\n def __init__(self, initial_state, initial_covariance, proc_noise_std = [0.02, 0.02, 0.01], obs_noise_std = [0.02, 0.02, 0.01]):\n\n  self.mu = initial_state # 초기 상태 추정\n  self.Sigma = initial_covariance # 초기 불확실성\n\n  self.g, self.G = velocity_motion_model_linearized_1() # 사용할 액션 모델\n\n  # 과정 모델 노이즈의 표준 편차\n  self.proc_noise_std = np.array(proc_noise_std)\n  # 과정 노이즈 공분산 (R)\n  self.R = np.diag(self.proc_noise_std ** 2) \n\n  self.h, self.H = odometry_observation_model_linearized() # 사용할 관측 모델\n\n  # 관측 모델 노이즈의 표준 편차\n  self.obs_noise_std = np.array(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.Q = np.diag(self.obs_noise_std ** 2)\n\n def predict(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.mu = self.g(self.mu, u, dt)\n  # 공분산 (Sigma) 예측\n  self.Sigma = self.G(self.mu, u, dt) @ self.Sigma @ self.G(self.mu, u, dt).T + self.R\n\n  return self.mu, self.Sigma\n\n def update(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.Sigma @ self.H(self.mu).T @ np.linalg.inv(self.H(self.mu) @ self.Sigma @ self.H(self.mu).T + self.Q)\n  \n  # 상태 추정 (mu) 업데이트\n  self.mu = self.mu + K @ (z - self.h(self.mu))\n\n  # 공분산 (Sigma) 업데이트\n  I = np.eye(len(K)) \n  self.Sigma = (I - K @ self.H(self.mu)) @ self.Sigma\n\n  return self.mu, self.Sigma\n```\n\n성능:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nExtended Kalman Filter(EKF)의 성능을 평가하기 위해, 동일한 상수 속도 모델과 오도메트리 관측 모델을 사용하는 선형 칼만 필터(LKF)의 성능과 비교할 것입니다. 먼저, 초기 과정 노이즈를 크게 설정하고 관측 노이즈를 매우 낮게 설정할 것입니다. 이 설정은 사실상 필터에게 행동 모델보다는 관측을 신뢰하도록 지시합니다. 예상했듯이, 두 버전의 칼만 필터는 관측을 따라가며 비슷한 성능을 발휘합니다. 관측 모델의 간단함과 선형성을 고려하면 이 결과가 예상된 것입니다.\n\n다음 테스트는 더 어려울 것입니다. 과정 노이즈를 매우 낮게 설정하고 관측 노이즈를 매우 높게 설정할 것입니다. 이 설정은 필터가 대부분의 관측을 무시하고 상태 추정에 운동 모델에 크게 의존하게 만듭니다. 운동 방정식이 같더라도, EKF가 적용한 선형화 때문에 크게 다른 결과를 예상할 것입니다.\n\n예상대로, 차이는 상당합니다. 이러한 노이즈 설정 하에서 LKF는 성능이 저조합니다. 비선형 운동 모델에 완전히 의존할 때, LKF의 결과는 실제 값과 로봇 센서에서 보고된 오도메트리에서 크게 벗어납니다. 반면에, EKF는 관측 데이터에 더 가까이 머물며 훨씬 나은 성능을 발휘합니다. 또한 오른쪽 그림의 큰 타원에서 나타나는 바와 같이 EKF는 높은 불확실성을 정확히 나타냅니다. 반면에, LKF는 낮은 불확실성을보고하는데, 이것은 부정확합니다.\n\n## 대안 운동 모델\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과거 두 필터에서 사용된 모션 모델은 꽤 간단하며 로봇이 헤딩 각도 θ(세타)의 방향으로 직선으로 이동한다고 가정합니다. 보다 정교한 모션 모델이 존재하며, 그 모델이 더 나은 성능을 발휘할 수 있는지 확인하는 것이 중요합니다. 다음에 탐구할 새로운 상수 속도 모션 모델은 로봇을 변환(직선) 및 회전(각도) 속도 v 및 ω를 통해 제어할 수 있도록 합니다. 직선 이동을 가정하는 대신, 이 모델은 아래에 나와 있는 것처럼 반지름이 r인 원 위를 로봇이 이동한다고 가정합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_28.png)\n\n직선 모션 가정과 같이 원형 모션 가정도 근사값일 뿐이며, 시간 간격이 매우 작은 경우에만 유효합니다(움직임이 원이건 직선이건 구분할 수 없을 정도로 아주 작을 때). ω 값이 0에 가까워질수록, 반지름은 아주 크게되어 거의 직선 상에서의 움직임을 나타내게 됩니다. 시간에 따라 상태를 진화시키기 위해 이 모델을 따르는 비선형 함수 g에 해당하는 방정식 벡터는 아래에 나와 있습니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_29.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제이 모델을 선형화하기 위해 g의 자코비안을 계산해야 합니다. 이는 상태에 대한 g의 편도함수에 해당합니다. 우리의 상태는 다음과 같습니다:\n\n![State](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_30.png)\n\n따라서 자코비안 G는 다음과 같이 표현됩니다:\n\n![Jacobian](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_31.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 보았듯이, 관련 행렬을 결정한 후에 구현은 간단합니다. 먼저, 아래 속도 모델 코드를 정의합니다.\n\n```js\ndef velocity_motion_model_linearized_2():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  x = mu[0]\n  y = mu[1]\n  theta = mu[2]\n  \n  v = u[0]       \n  w = u[1]       \n  if w == 0:\n   w = 1e-6   # 직선 이동의 경우 0으로 나누는 것을 피하기 위해\n\n  g = np.array([\n     x + -v/w * np.sin(theta) + v/w * np.sin(theta + w * delta_t),\n     y + v/w * np.cos(theta) - v/w * np.cos(theta + w * delta_t),\n     theta + w * delta_t\n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  theta = mu[2]\n  v = u[0]       \n  w = u[1]       \n  if w == 0:\n   w = 1e-6   # 직선 이동의 경우 0으로 나누는 것을 피하기 위해\n\n  G = np.array([\n   [1, 0, -v / w * np.cos(theta) + v / w * np.cos(theta + w * delta_t)],\n   [0, 1, -v / w * np.sin(theta) + v / w * np.sin(theta + w * delta_t)],\n   [0, 0, 1]\n  ])\n\n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n관측 모델은 이전 예제에서 사용한 것과 정확히 동일하며 완전성을 위해 여기에 다시 제시하겠습니다.\n\n```js\ndef odometry_observation_model_linearized():\n def observation_function_h(mu):\n  return mu\n \n def jacobian_of_h_wrt_state_H(mu):\n  return np.eye(3)\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 두 번째 확장 칼만 필터가 아래에 구현되었습니다. 코드는 이전 것과 정확히 동일하지만 움직임 모델을 정의하는 라인만 예외입니다.\n\n```js\nclass KalmanFilter:\n\n def __init__(self, initial_state, initial_covariance, proc_noise_std = [0.02, 0.02, 0.01], obs_noise_std = [0.02, 0.02, 0.01]):\n\n  self.mu = initial_state # 초기 상태 추정\n  self.Sigma = initial_covariance # 초기 불확실성\n\n  self.g, self.G = velocity_motion_model_linearized_2() # 사용할 액션 모델\n  \n  # 프로세스 또는 액션 모델 노이즈의 표준 편차\n  self.proc_noise_std = np.array(proc_noise_std)\n  # 프로세스 노이즈 공분산 (R)\n  self.R = np.diag(self.proc_noise_std ** 2) \n\n  self.h, self.H = odometry_observation_model_linearized() # 사용할 관측 모델\n\n  # 관측 또는 센서 모델 노이즈의 표준 편차\n  self.obs_noise_std = np.array(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.Q = np.diag(self.obs_noise_std ** 2)\n\n def predict(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.mu = self.g(self.mu, u, dt)\n  # 공분산 (Sigma) 예측\n  self.Sigma = self.G(self.mu, u, dt) @ self.Sigma @ self.G(self.mu, u, dt).T + self.R \n\n  return self.mu, self.Sigma\n\n def update(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.Sigma @ self.H(self.mu).T @ np.linalg.inv(self.H(self.mu) @ self.Sigma @ self.H(self.mu).T + self.Q)\n  \n  # 상태 추정 (mu) 업데이트\n  self.mu = self.mu + K @ (z - self.h(self.mu))\n\n  # 공분산 (Sigma) 업데이트\n  I = np.eye(len(K)) \n  self.Sigma = (I - K @ self.H(self.mu)) @ self.Sigma\n\n  return self.mu, self.Sigma\n```\n\n성능:\n\n이 시점에서 EKF가 LKF보다 우월함이 명확하며, 이제는 더 정교한 움직임 모델이 뚜렷한 향상을 가져오는지를 결정하는 것에 중점을 두고 연구를 계속할 것입니다. 관측 모델이 명백히 움직임 모델보다 우선하는 잡음 설정에서 평가를 수행하지 않을 것이며, 이렇게 하면 모든 필터에 대해 단순히 관측 결과와 일치할 것으로 예상됩니다. 따라서 여기서는 관측을 대부분 무시하고 움직임 모델에 더 집중하는 잡음 설정으로 결과를 제시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n놀랍게도 더 고급 모델이 더 간단한 모델보다 우수한 성능을 내지 못했습니다. 위의 두 개의 도표에서는 순수한 눈으로는 중요한 성능 차이를 알아차리기 힘듭니다. 심지어 보고된 잡음 타원도 거의 같아 보입니다.\n\n이전의 기사에서 얻은 결론 중 하나는 우리도 이곳에서 도출하는 것인데, 어떻게 소위 세련되고 고급스러운 필터를 사용하더라도 잘못된 데이터를 입력하면 잘못된 추정값을 제공할 것이라는 것입니다. 우리의 오도메트리 관측에 문제가 있다는 사실을 조정할 방법이 없네요. 이전의 모든 도표에서 주요 문제가 헤딩인 것처럼 보입니다. 올바른 헤딩 데이터가 없으면 어떤 고급 움직임 모델도 이를 보상할 수 없을 겁니다. 로봇이 헤딩의 대안 소스를 제공한다면 (그리고 다른 유용한 센서 데이터도 제공한다면), 필터를 사용하여 서로 다른 소스를 융합하고 더 나은 추정값을 얻을 수 있습니다. 다음에는 센서 융합을 살펴보겠습니다.\n\n## 센서 융합의 힘\n\n센서 융합에는 다양한 종류가 있으며, 우리가 적용할 유형은 저수준 융합으로 알려져 있습니다. 저수준 데이터 융합의 목표는 함께 더 유익한 여러 소스의 기본 센서 데이터를 결합하는 것입니다. 이 아이디어는 서로 다른 소스의 강점을 활용하여 더 나은 추정값을 만들어내는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 원래 오도메트리 데이터와 관성 측정 장치(IMU)에서 오는 데이터를 어떻게 결합하는지에 대해 연구할 것입니다. IMU는 일반적으로 우리에게 방향, 각속도 및 선형 가속도를 제공할 수 있습니다. 이는 우리가 부정확한 추정이 나쁜 방향 정보 때문인 것으로 의심하기 때문에 매우 유익합니다. IMU는 이러한 종류의 정보에 대해 오도메터보다 정확할 경향이 있기 때문에, IMU 데이터가 오도메트리와 어떻게 융합되어 더 나은 결과를 얻을 수 있는지 알아보겠습니다.\n\n우리가 할 첫 번째 일은 상태와 모델을 확장하는 것입니다. 이전에는 추가 데이터의 좋은 소스가 없었기 때문에 로봇의 자세 이상을 고려하는 것이 별 의미가 없었습니다. 그러나 이제 우리에게 각속도와 선형 가속도를 제공할 수 있는 센서가 있기 때문에, 새로운 정보를 상태에 통합하는 것이 로봇의 상태를 더 잘 추정하는 데 도움이 될 것입니다. 이는 또한 우리의 동작 및 센서 모델을 확장할 것이기 때문에 필수적입니다. 이 기사에서는 새로운 7차원 상태로 시작하는 두 가지 다른 확장을 시도할 것입니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_32.png)\n\n여기서 x, y 및 θ는 친숙한 로봇 자세에 해당합니다. 과거 구현과는 달리, 속도 v와 각속도 ω가 제어 입력에서만 얻어졌던 경우, 이제 이들의 추정치도 상태의 일부로 유지합니다. 상태에는 x 및 y의 선형 가속도도 포함됩니다. 확장된 상태를 고려할 때, 동작 모델도 각 상태 구성 요소를 계산하기 위해 확장되어야 합니다. 가속도를 고려하는 동작 모델은 더 이상 상수 속도 모델이 될 수 없습니다. 따라서 속도는 추정 사이에 변화하고, 가속도는 일정하게 유지된다는 새로운 가속도 일정 모델을 사용할 것입니다. 이 새로운 가속도 상수 모델은 아래 표시된 함수 g와 야코비안 행렬 G에 의해 표현됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_33.png)\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_34.png)\n\n지금부터 조금 복잡해집니다. 이 큰 야코비안 행렬을 계산하는 것은 매우 에러가 발생하기 쉽습니다. 수동으로 시도해보는 것도 재미있지만, 항상 결과를 확인하기 위해 소프트웨어를 사용하는 것이 좋은 생각입니다. 계산이 올바르다고 확신하고 구현한 후에도 해당 구현이 정확한지 확인할 방법이 있어야 합니다. 이 기사에 구현된 모든 야코비안에 대해, 테스트 스크립트가 잡아낸 적어도 하나의 작은 오류가 있었습니다. 야코비안을 계산하고 구현이 올바른지 확인하는 데 소프트웨어를 사용하지 않으면 코드에 버그를 도입할 확률이 높습니다.\n\n상태 및 운동 모델을 성공적으로 확장했습니다. 이제 센서 퓨전을 가능하게 하려면 관찰 모델도 확장해야 합니다. 확장된 상태 및 운동 모델과 결합하여 퓨전을 수행할 수 있도록 두 센서의 데이터를 연결하려 할 것입니다. 새로운 관찰 모델은 아래에 표시된 벡터의 첫 세 요소인 보통의 로봇 자세와 IMU 센서에서 파생된 방향 θ_imu, 각 속도 ω, 그리고 x 및 y 구성 요소에서의 가속도 a_x 및 a_y와 조합된 것입니다. 우리가 선속도 v를 직접 관측하지는 않지만 여전히 상태에 있을 수 있으며, Kalman 필터에 의해 운동 모델을 통해 추론될 것입니다. 이와 같은 관측되지 않는 변수를 숨겨진 또는 잠재 변수라고 부르는 경우가 종종 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 아래에 정의된 자코비안 행렬입니다. 관측값에서 상태 변수로의 직접적인 매핑이 있으므로, 자코비안에서 관측값(행)이 상태 변수(열)에 해당될 때는 1이 있습니다. 자코비안에서 상태 변수 θ에 해당하는 3열에는 두 개의 1이 있음을 주목해 주세요 — odometry에서 θ에 대한 행과 IMU에서 θ에 대한 행이 각각 하나씩 있습니다. 또한, 선형 속도에 해당하는 4열은 직접 관측하지 않기 때문에 모두 0입니다.\n\n구현된 모션 및 관측 모델은 아래에 나와 있습니다. 필터의 구현은 이전 것과 매우 유사하므로 간결함을 위해 생략하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef acceleration_motion_model_linearized_1():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  \n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[0]      \n  w = u[1] \n  \n  g = np.array([\n   x + v * np.cos(theta) * delta_t + 0.5 * a_x * delta_t**2,      \n      y + v * np.sin(theta) * delta_t + 0.5 * a_y * delta_t**2,    \n      theta + w * delta_t,\n      v + a_x * np.cos(theta) * delta_t + a_y * np.sin(theta) * delta_t,\n      w,                                                      \n      a_x,                                                              \n      a_y\n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n\n  G = np.array([[1.0, 0.0, -delta_t * v * np.sin(theta), delta_t  * np.cos(theta), 0.0, 0.5*delta_t**2, 0.0],   \n                   [0.0, 1.0, delta_t * v * np.cos(theta), delta_t * np.sin(theta), 0.0, 0.0, 0.5*delta_t**2],       \n                   [0.0, 0.0, 1.0, 0.0, delta_t, 0.0, 0.0],                                      \n                   [0.0, 0.0, -delta_t * a_x * np.sin(theta) + delta_t * a_y * np.cos(theta), \n                   1.0, 0.0, delta_t * np.cos(theta), delta_t * np.sin(theta)],                  \n                   [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])                                         \n  \n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n```js\ndef odometry_imu_observation_model_with_acceleration_motion_model_linearized_1():\n def observation_function_h(mu):\n  x, y, theta, v, w, ax, ay = mu\n  return np.array([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def jacobian_of_h_wrt_state_H():\n  return np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],    \n                    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],        \n                    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n성능:\n\n더 정교한 모션 모델을 사용하고 오도메트리 데이터와 IMU 데이터를 퓨즈하는 개선된 EKF를 평가하는 시간입니다. 특히, 노이즈 매개변수를 다음과 같이 설정하겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nproc_noise_std = [0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1] # [x, y, theta, v, w, a_x, a_y]\nobs_noise_std = [100.0, 100.0, 1000.0, 6.853891945200942e-06, 1.0966227112321507e-06, 0.0015387262937311438, 0.0015387262937311438] #[x, y, theta, theta_imu, w, a_x, a_y]\n```\n\n이 설정은 우리의 모션 모델에 대해 높은 신뢰를 가지고 있지만 관측 모델에 대해서는 IMU 데이터를 오도메트리 데이터보다 더 신뢰한다고 필터에 알려줍니다. 특히, 우리는 필터에게 오도메트리에서의 헤딩이 미친 것이라고 생각하고 IMU에서의 헤딩이 매우 정확하다고 말하고 있습니다. 이는 IMU가 매우 정밀할 수 있고 오도메트리 데이터가 악명 높게 나쁠 수 있기 때문에 종종 사실입니다. 이 설정은 우리가 예측한 궤적을 수정하는 데 도움이 될 수 있는데, 이전에 본 것처럼 모양은 그리 나쁘지 않지만 방향은 매우 잘못된 경우가 많습니다.\n\n아래 비디오와 이어지는 두 그림에서 볼 수 있듯이 가속도 모델과 센서 퓨전이 포함된 새 필터는 이전 필터보다 훨씬 더 잘 수행됩니다. 특히 IMU로부터 제공된 더 나은 방향성 덕분에 추정 궤적이 초기에 실제 궤적에 훨씬 가까웠음을 볼 수 있습니다. 그러나 마지막에는 지그재그 패턴을 따르기 시작했습니다.\n\n더 나은 결과를 얻기 위한 레시피는 없습니다. 이것이 필터를 설계하는 것을 과학보다는 예술로 만드는 것입니다. 위의 지그재그 궤적을 개선할 수 있는 더 나은 모션 모델로 수정할 수 있을 것이라고 생각할 수 있습니다. 이 결과를 본 것처럼 나도 같은 방식으로 느꼈습니다. 모션 모델을 개선해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모션 모델 개선하기\n\n센서 퓨전은 도움이 되고 있지만, 아직 개선할 부분이 많이 남아 있습니다. 상태 추정을 위한 필터를 설계하려면 매우 교육된 추측을 하고 직관을 따라야 합니다. 앞서 논의한 대로, 최근 얻은 결과는 격려적이며 우리가 모델을 개선해 보아야 한다는 제안이 있습니다. 특히 상태를 모델링할 때 글로벌 선속도만을 고려하고, 가속도와 같이 다른 축을 따른 속도를 고려하지 않는 것으로 보입니다. 우리의 상태 벡터를 확장하여 이를 고려해보겠습니다.\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_37.png)\n\n새로운 상태 벡터에는 로봇의 움직임에 대한 더 자세한 정보를 제공할 x와 y 성분의 속도가 포함되어 있습니다. 이 확장이 필터의 성능에 어떤 영향을 미치는지 알아보겠습니다. g 함수와 해당 야코비안 G을 이용하여 표현된 확장된 모션 모델은 아래와 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_38.png)\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_39.png)\n\nThe observation model function h remains the same, but the Jacobian H now has two columns filled with zeros corresponding to the unobserved state variables v_x and v_y.\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_40.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델과 사용된 잡음 매개변수에 대한 코드는 다음과 같습니다. 여기서 추가적으로 필터 코드는 이전에 소개된 코드와 매우 유사하므로 생략될 것입니다.\n\n```js\ndef acceleration_motion_model_linearized_2():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  \n  x, y, theta, v_x, v_y , w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n  \n  g = np.array([\n      x + v * np.cos(theta) * delta_t + 0.5 * a_x * delta_t**2,  \n      y + v * np.sin(theta) * delta_t + 0.5 * a_y * delta_t**2,  \n      theta + w * delta_t,                                   \n      v * np.cos(theta) + a_x * delta_t,                         \n      v * np.sin(theta) + a_y * delta_t,                         \n      w,                                                         \n      a_x,                                                       \n      a_y                                                        \n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  x, y, theta, v_x, v_y, w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n\n  G = np.array([[1.0, 0.0, -delta_t * v * np.sin(theta), 0.0, 0.0, 0.0, 0.5*delta_t**2, 0.0],   \n                   [0.0, 1.0, delta_t * v * np.cos(theta), 0.0, 0.0, 0.0, 0.0, 0.5*delta_t**2],        \n                   [0.0, 0.0, 1.0, 0.0, 0.0, delta_t, 0.0, 0.0],                                      \n                   [0.0, 0.0, -v * np.sin(theta), 0.0, 0.0, 0.0, delta_t, 0.0],                       \n                   [0.0, 0.0, v * np.cos(theta), 0.0, 0.0, 0.0, 0.0, delta_t],                        \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])                                         \n  \n  return G\n \n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n```js\ndef odometry_imu_observation_model_with_acceleration_motion_model_linearized_2():\n def observation_function_h(mu):\n  x, y, theta, v_x, v_y, w, ax, ay = mu\n  return np.array([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def jacobian_of_h_wrt_state_H():\n  return np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],       \n                    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],            \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n \n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n```js\nproc_noise_std = [0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1] # [x, y, theta, v_x, v_y, w, a_x, a_y]\nobs_noise_std = [100.0, 100.0, 1000.0, 6.853891945200942e-06, 1.0966227112321507e-06, 0.0015387262937311438, 0.0015387262937311438] #[x, y, theta, theta_imu, w, a_x, a_y]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n성능:\n\n새로운 필터는 이전 동영상과 아래 그림에서 명백히 확인할 수 있듯이 인상적인 성과를 보여주었습니다. 예측된 궤적이 이제 센서 퓨전과 더 정교한 모델의 덕분에 실제 지면에 매우 가까워졌습니다. 게다가 그림의 최신 자세에서 타원으로 표시된 최종 불확실성은 이전 필터 버전보다 훨씬 작습니다. 이는 센서 퓨전이 잘 설계된 EKF의 효과를 보여줍니다. 추가로 소음 매개변수를 조정하면 더 많은 개선이 가능하지만, 이는 기사의 길이 때문에 여기서 탐구되지 않을 것입니다.\n\n## 직접 시도해보세요\n\n이전 기사와 마찬가지로, 코드는 전체 코드를 검사하거나 단순히 알고리즘을 실행하고 결과를 실시간으로 확인하려는 사람들을 위해 제공됩니다. 코드를 실행하려면 아래 지시사항을 따르십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 필요한 것은 ROS 2입니다. ROS 2 Humble을 Ubuntu Jammy Jellyfish (22.04)에 설치하는 방법은 여기에서 찾을 수 있습니다. Ubuntu의 다른 버전이나 다른 운영 체제의 경우 공식 ROS 2 문서를 참고하십시오.\n\n데이터를 얻으려면 이 링크에서 ROS 2 가방을 다운로드해야 합니다. 사용하기 전에 파일을 압축 해제해야 합니다.\n\n마지막으로 ROS 2 패키지를 복제하고 빌드해야 합니다. 아래 단계를 따라 진행할 수 있습니다. ros2_ws를 실제 ROS 2 작업 공간으로 교체해야 합니다.\n\n\n# 종속성 설치\nsudo apt install python3-pykdl\n\n# 패키지를 복제하고 빌드\ncd ros2_ws/src\ngit clone https://github.com/carlos-argueta/rse_prob_robotics.git\ncd ..\ncolcon build --symlink-install\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장 칼만 필터를 실행하려면 3개의 다른 터미널을 열어야 합니다.\n\n터미널 1에서 (ros2_ws를 실제 워크스페이스로 교체해주세요) 다음 명령을 실행하여 Rviz를 열고 로봇이 보고 있는 것을 확인하세요.\n\n```js\nsource ~/ros2_ws/install/setup.bash\nros2 launch rse_gaussian_filters rviz_launch.launch.py\n```\n\n터미널 2에서 확장 칼만 필터의 버전에 따라 다음 명령 중 하나를 실행하세요. 먼저 아무 출력도 나오지 않을 것이며, ROS 2 가방(bag)을 실행할 때까지 기다리세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsource ~/ros2_ws/install/setup.bash\n\n# 아래 명령어 중 하나 실행\n\n# 3D 상태, 기본 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v1 \n\n# 3D 상태, 고급 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v2 \n\n# 7D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_7d \n\n# 8D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_8d \n```\n\n터미널 3에서 ROS 2 가방이 추출된 위치로 이동하여 다음 명령어로 재생하십시오. \"Ignoring a topic '/navrelposned', reason: package 'ublox_msgs' not found\"와 같은 경고 메시지를 무시해도 됩니다.\n\n```js\nros2 bag play linkou-2023-12-27-2-med --clock\n```\n\n위 단계를 따르면 확장 칼만 필터를 실행하고 결과를 실시간으로 확인할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실용적인 고려사항 및 마지막으로\n\n이 글은 확장 칼만 필터 (EKF)를 기본적인 선형 칼만 필터 (LKF)의 확장으로 소개했습니다. EKF는 대부분의 실세계 시스템의 비선형성을 근사화를 통해 다룹니다. 이는 사후 평균 주변의 비선형 함수를 일차 테일러 전개를 이용하여 선형화합니다. 선형화된 후, EKF는 LKF와 유사하게 작동합니다.\n\n보여진 대로, 센서 퓨전과 결합된 EKF는 놀라운 결과를 얻을 수 있습니다. 그러나 좋은 필터를 설계하는 것은 어렵고 오류를 범하기 쉽습니다. 종종 과학과 예술 사이의 균형을 요구합니다. 적합한 움직임 및 관측 모델을 찾는 것이 첫 번째 난관입니다. 이동 로봇에 대한 사용 가능한 모델이 있더라도, 다른 시나리오는 좋은 모델이 부족할 수 있습니다. 자코비안 계산 또한 어렵고 실수하기 쉬우며, 소프트웨어 확인이 필요함을 강조합니다.\n\n효과적인 EKF 설계를 위해 여러 다른 고려사항과 결정이 중요합니다. 한 가지 중요한 측면은 올바른 시간 간격(delta_t)을 선택하는 것으로, 효과적인 선형화를 위해 충분히 작아야 합니다. 이 경우, 고정값 대신 동적 delta_t가 사용되었습니다. 필터의 업데이트 단계를 수행할 때 언제, 어떻게 결정하는지도 중요합니다. 특히 서로 다른 속도로 측정 값을 제공하는 다른 센서가 있을 때 (예: IMU는 보통 오도메트리보다 높은 주파수로), 적절한 노이즈 매개 변수 선택도 여러 가지 방법이 가능한 복잡한 작업입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면, 확장 칼만 필터는 특히 센서 퓨전과 함께 사용될 때 우수한 상태 추정 결과를 제공할 수 있습니다. 그러나 견고한 EKF를 설계하려면 상당한 노력과 연습이 필요합니다. 다음 글에서는 Unscented Kalman Filter (UKF)를 소개할 예정입니다. UKF는 EKF보다 여러 장점을 제공합니다. Unscented Transform을 사용하여 비선형 변환의 평균과 공분산을 더 정확하게 캡처합니다. UKF는 Jacobian을 필요로하지 않아 구현을 간단하게 만들어줍니다. 선형화로 인한 근사 오차를 줄이므로 매우 비선형 시스템에 더 효과적입니다. 또한 UKF는 비가우시안 분포를 더 잘 처리하여 견고성과 다양성을 향상시킵니다.\n\n# 독후감\n\n다음은 칼만 필터 패밀리에 대해 학습하기 위해 참고한 훌륭한 자료 목록입니다:\n\n- Optimal State Estimation: Kalman, H∞, and Nonlinear Approaches\n- State Estimation for Robotics\n- Kalman and Bayesian Filters in Python\n- Probabilistic Robotics\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사가 도움이 되셨기를 바랍니다. 피드백이 있으시면 언제든지 댓글을 남겨주세요. 또한, 이후의 주제를 다룬 보다 심도 있는 강좌 시리즈를 시작하려고 합니다. 이 강좌는 비디오, 코딩 프로젝트 등이 포함될 예정이며 유료로 운영될 가능성이 높습니다. 이런 강좌에 관심이 있다면 댓글로 알려주시면 참여 의향을 파악할 수 있습니다.\n\n저와 소통하고 싶다면 LinkedIn에서 저를 찾아보세요: https://www.linkedin.com/in/carlos-argueta\n\n저와 함께 ROS 2를 이용한 로보틱스를 배우고 싶으신가요? 제 라이브 강의에 참여해보세요!","ogImage":{"url":"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png"},"coverImage":"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png","tag":["Tech"],"readingTime":31},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png\" alt=\"Sensor Fusion with the Extended Kalman Filter in ROS2\"\u003e\u003c/p\u003e\n\u003cp\u003e안녕하세요! 이 글은 가우시안 필터를 소개하는 시리즈 중 두 번째 글입니다. 구체적으로, 이 글은 칼만 필터 패밀리에 대한 세 번째 세부 소개입니다. 이미 칼만 필터에 대해 익숙하지 않다면, 계속하기 전에 첫 번째 글을 읽기를 권장합니다. 다음 글에서는 언센티드 칼만 필터를 소개할 예정입니다. 이 글의 결과를 재현하는 데 사용된 데이터와 코드는 이 글의 끝 부분에 찾을 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e이전 글에서 소개된 대로, 성공적인 로봇 시스템은 유용한 작업을 수행하기 위해 물리적 세계를 인식하고 조작할 수 있어야 합니다. 이를 달성하기 위해 로봇은 환경의 중요한 불확실성을 고려해야 합니다. 현대 로봇 공학에서 가장 기본적인 문제 중 하나는 상태 추정입니다. 상태 추정은 로봇과 환경(랜드마크 및 기타 객체의 위치 등)의 가장 확률적인 상태(예: 위치, 방향, 속도)를 불확실한(잡음이 있는) 및 아마도 불완전한 정보를 기반으로 결정하는 것을 포함합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e칼만 필터는 불확실한 환경에서 상태 추정에 대응합니다. 필터는 상태의 각 요소 (예: x, y 좌표, 헤딩)을 가우시안 확률 변수로 모델링합니다. 가우시안은 상태 x에 대한 확률 밀도를 벡터 μ (뮤)와 공분산 행렬 Σ (시그마)만 사용하여 표현할 수 있게 합니다. 이 매개변수화에서 우리의 상태 x는 기대값 μ에 의해 표현되며, Σ는 제어, 이동 및 관측 잡음으로 인한 상태의 내재적 불확실성을 포착합니다.\u003c/p\u003e\n\u003cp\u003e칼만 필터가 사용하는 베이지안 프레임워크에서는 전체 상태를 믿음(belief)이라고 합니다. 가우시안 (또는 정규분포)을 사용하는 장점은 그들의 수학적 성질에 있습니다. 이 성질은 칼만 필터 방정식을 단순화합니다. 가우시안 믿음이 선형 변환을 겪을 때의 특징 중요한데, 가우시안 믿음이 선형 변환을 겪으면 결과는 여전히 가우시안 확률 변수로 유지됩니다. 이 성질은 칼만 필터의 방정식이 우아하고 다루기 쉬운 상태를 유지하도록 보장합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e사후 믿음을 계산하기 위해 칼만 필터는 이전 믿음을 시간을 경과함에 따라 전달하는 모션 모델을 사용합니다. 그런 다음 관측 모델은 로봇 센서에서의 데이터를 통합하여 예측된 믿음을 업데이트하고 이를 사후로 변환합니다. 칼만 필터 알고리즘은 아래에서 요약됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_2.png\" alt=\"Linearity of the Linear Kalman Filter\"\u003e\u003c/p\u003e\n\u003cp\u003e선형 칼만 필터의 \"선형성\"은 알고리즘의 2번째와 5번째 줄에서 가장 명백합니다. 2번째 줄에서 예측된 상태는 이전 상태 μ_t−1과 제어 입력 u_t의 선형 함수입니다. 5번째 줄에서 예측된 관측값 (y = Cμ)은 인수 μ^bar_t의 선형 함수입니다. 이 선형성은 가우시안 특성을 유지하여 필터를 구현하기 쉽게 만듭니다. 그러나 이것은 또한 주요 약점 중 하나를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e선형 칼만 필터가 실제 문제에 부적합한 이유는 실제 문제가 종종 선형적이지 않기 때문입니다. 이전 글에서 도입했던 간단한 상태 추정의 경우, 상태가 모바일 로봇의 2차원 포즈 (x, y, θ)로 표현되었지만 정확히 선형적이지 않았습니다. 아래에 다시 소개되는 이동 모델에서 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_3.png\" alt=\"Motion Model\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e모델이 쉽게 알아볼 수 있듯이, 삼각 함수가 사용되어 로봇의 좌표를 업데이트하는데 사용되며, 이러한 함수들은 선형이 아닙니다. 이 모델에서 선형 칼만 필터는 어느 순간 발산할 가능성이 있습니다. 이 이유로 선형 칼만 필터를 소개한 후, 대부분의 실제 현상의 비선형성을 고려하기 위해 확장 방법을 고안하는 작업이 즉시 시작되었습니다.\u003c/p\u003e\n\u003ch1\u003e확장 칼만 필터\u003c/h1\u003e\n\u003ch2\u003e비선형성의 문제\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e비선형성에 관한 문제를 더 자세히 설명하기 위해 다음 애니메이션을 살펴볼 수 있습니다. 첫 번째 애니메이션에서는 선형 칼만 필터의 가정이 성립하는 세계에 있으며, 새로운 상태가 인수에 대해 단순하게 선형인 경우를 살펴봅니다. 시각화를 이해하기 쉽게 하기 위해 1차원 상태 x를 가정합니다. 이 애니메이션은 가우스를 선형 함수 g를 통과시켰을 때 다른 가우스가 되는 과정을 보여줍니다. 이 경우 g = -0.5*x+1입니다.\u003c/p\u003e\n\u003cp\u003ex의 가우시안 표현에서 시작하지만 비선형 함수 g를 선택하는 경우, 결과 확률 밀도 함수는 더 이상 가우시안이 아닙니다. 새로운 밀도를 계산하기 위한 폐쇄형 방법이 없습니다. 대신 입력 분포에서 점들을 샘플링하고 이를 g를 통과시켜 출력 히스토그램을 구축하여 출력 분포를 만들어야 합니다. 아래에 표시된 출력의 형태에서 확인할 수 있듯이, 이는 가우시안이 아닌 것을 알 수 있습니다. 또한 이 출력 분포는 칼만 필터의 단봉성 가정을 위배하며, 단일 피크를 요구합니다. P(y)의 가우시안 근사는 출력 데이터에 가우시안을 맞추어 얻었습니다. 이는 실제 모델의 비선형성을 다루기 위한 선형 칼만 필터의 한계를 강조합니다.\u003c/p\u003e\n\u003cp\u003e요약하면 비선형 모델을 다룰 때 출력 밀도는 가우시안이 아니며 폐쇄형으로 계산할 수 없으며 종종 다중 피크를 갖기 때문에 칼만 필터 방정식이 무용지물이 됩니다. 이 문제를 해결하기 위해 확장 칼만 필터(EKF)는 선형성 가정을 버립니다. 대신 상태 전이 확률과 측정 확률은 비선형 함수 g 및 h에 따라 결정됨을 수용합니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_5.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_6.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cp\u003e모든 것이 잘 진행되고 있어요. 칼만 필터의 우아한 방정식을 활용하려면 여전히 우리의 신뢰를 가우시안으로 표현해야 합니다. 이 필수성은 정확한 사후분포를 계산하는 것에서 EKF의 초점을 이동시켜 실제 신뢰의 좋은 가우시안 근사값을 찾게 됩니다. 아래 그림에서 보듯, 몬테칼로를 사용하여 출력 분포를 계산한 후, 그에 대한 가우시안을 fitting하고 필요한 매개변수 μ (뮤)와 Σ (시그마)를 얻을 수 있습니다. 그러나 아직도 가우시안을 닫힌 형태로 계산할 수 없는 문제가 남아 있습니다.\u003c/p\u003e\n\u003ch2\u003e테일러 전개를 통한 선형화\u003c/h2\u003e\n\u003cp\u003e이 문제를 해결하기 위해 확장 칼만 필터는 선형화라는 추가 근사값을 적용합니다. 선형화의 핵심 아이디어는 비선형 함수 g를 해당 관심점에서 g에 접하는 접선인 선형 함수로 근사하는 것입니다. 비선형 함수를 선형화하는 다양한 기술 중, EKF가 사용하는 것은 일차 테일러 전개입니다. 함수의 테일러 전개는 함수의 도함수를 하나의 점 a에서 표현된 다항식 항들의 무한 합입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 이미지에서 높은 차수의 테일러 전개가 점 a = 0 주변에서 g를 더 가까운 근사로 제공하는 것을 볼 수 있습니다. 그러나 고차 다항식이 늘어날수록 요구되는 계산도 증가하며, 문제가 빠르게 풀기 어려워집니다. 다행히도 Kalman 필터가 자주 업데이트되는 경우(작은 Δt), 관심점 a의 차이가 매우 작아야합니다. 따라서 우리는 다음 (매우 가까운) 각 지점 a에서의 함수 g의 값을 및 기울기(점 a에서의 미분)를 사용하여 함수 g의 선형 근사를 얻기 위해 1차 다항식(선)을 사용할 수 있습니다. 이 문제는 본질적으로 아래와 같이 간소화됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png\" alt=\"2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 동작이 왜 잘 작동하는지 설명하기 위해 g라는 함수를 가정해 볼게요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_10.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cp\u003e점 a에서 함수 g의 일차 테일러 전개는 아래 그림에서 빨간색으로 표시되어 있어요. 큰 x 값 범위를 관찰하면 좋은 근사치를 제공하지 않음이 명백해요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_11.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러나 우리가 사후 분포의 값들에 대한 근사치에만 주의를 기울이기 때문에, 새로운 사후 분포에 대해 그러한 근사치를 매우 짧은 시간 이후에 다시 계산할 것을 알고 있기 때문에, 아래 그래프에서 볼 수 있듯이, 우리의 근사치가 관심 지점 주변에서 매우 좋다는 것을 알 수 있습니다. 이는 첫 번째 차수 테일러 전개가 우리의 목적에 대해 충분한 근사치를 제공하며, 시스템 내부의 비선형성에도 불구하고 확장 칼만 필터를 효과적으로 적용할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_12.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e마지막으로, 아래 이미지는 두 프로세스를 비교하려고 합니다: 첫 번째는 원래의 가우시안으로 시작하여 비선형 함수 g를 통과시키고, 몬테카를로를 사용하여 비가우시안 출력 분포를 얻은 후, 이 출력에 가우시안을 적합시킵니다. 두 번째로, EKF가 사용하는 프로세스는 g를 선형화시키고, 원래의 가우시안을 이 선형 근사치를 통해 통과시킨 후, 선형화를 통해 닫힌 형태로 출력을 직접 얻습니다. 이 비교는 비선형 시스템 다루기에 대한 EKF 접근 방식의 효율성과 실용성을 강조합니다.\u003c/p\u003e\n\u003cp\u003e명확성을 위해, 아래에서는 출력만 표시됩니다. 보시다시피, EKF 가우시안이 몬테카를로 시뮬레이션에서 적합된 가우시안과 정확히 같지는 않지만, 충분히 가깝습니다. 이 작은 차이는 실제 분포의 닫힌 형태의 추정치를 효율적으로 얻기 위해 지불하는 대가입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위에서 설명한 간단한 예제는 스칼라 경우에 대한 것이었지만, 우리의 상태는 벡터입니다. 따라서 기울기를 구하기 위해 우리는 상태에 대한 g의 편미분을 계산합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_13.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eg와 그 기울기 g′의 값은 그들의 인자 (u_t와 x_t-1)에 따라 달라지는데요, 이는 우리의 관심 지점입니다 (스칼라 경우의 a와 대조적입니다). u_t의 값에 대해서는 로봇에 제공된 제어 명령을 사용합니다. x_t-1에 대해서는 선형화할 시기에서 가장 가능성이 높은 상태의 값으로 선택할 수 있습니다. 가우시안의 경우, 최대 가능성 값은 이전 시간 단계에서 계산된 사후값의 평균인 μ_t-1로 표시됩니다. 이 선형화는 업데이트 속도가 매우 빠른 필터 (매우 작은 Δt)에 대해 잘 작동하며, 이때 μ_t-1의 값과 우리가 추정하려는 현재 상태 간의 차이가 크지 않을 때 잘 작동합니다.\u003c/p\u003e\n\u003cp\u003e이제 기울기를 계산했기 때문에 g를 다음과 같이 추정할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_14.png\" alt=\"sensor fusion\"\u003e\u003c/p\u003e\n\u003cp\u003e가우시안에서, 모션 모델 또는 상태 전이 확률은 아래와 같이 표기됩니다. 여기서 R_t는 보통의 프로세스 노이즈 공분산입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_15.png\" alt=\"motion model\"\u003e\u003c/p\u003e\n\u003cp\u003e기울기가 숫자인 스칼라 케이스와는 달리, g'(u_t, μ_t-1)으로 알려진 G_t는 행렬입니다. 비선형 함수 g에 대한 상태 x의 일차 편미분값을 모두 포함하는 이 행렬은 야코비안이라고 합니다. 이 야코비안 행렬은 상태의 차원인 n×n의 크기를 가지며, 현재 제어 및 이전 사후 평균에 따라 값이 달라집니다. 따라서 야코비안 값은 시간이 지남에 따라 변합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e확장 칼만 필터(Extended Kalman Filter)는 함수 h에 의해 표현되는 비선형 관측 모델을 다룹니다. 특히, 타일러 전개(Taylor Expansion)는 새롭게 예측된 믿음 μ^bar_t을 중심으로 진행되며, 이는 h를 선형화하는 시점에서 가장 가능성이 높은 상태입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_16.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_17.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e가우시안으로, 측정 모델은 아래와 같이 표기됩니다. 여기서 Q_t는 전통적인 측정 잡음 공분산입니다. 여기서 야코비안 H_t는 관측 모델의 비선형 함수 h에 대한 상태 x에 대한 일차 편미분의 m×n 행렬입니다. 따라서 m은 관측의 차원이고, n은 상태의 차원입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_18.png\"\u003e\n\u003ch2\u003eExtended Kalman Filter Algorithm\u003c/h2\u003e\n\u003cp\u003e요약하면, 아래에 표시된 EKF 알고리즘은 이 기사의 앞부분에 표시된 LKF 알고리즘과 매우 유사하지만, 주요한 차이점은 모션 및 관측 모델의 선형화가 2번 줄과 5번 줄에서 이루어진다는 것입니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_19.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래 이미지는 EKF 알고리즘을 LKF와 나란히 재진 다음 주요 차이점을 강조합니다. 예측 단계에서 EKF는 선형 시스템 행렬 A 및 B 대신 상태를 시간에 따라 진화시키는 비선형 함수 g를 사용합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter�...\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e모델 선형화\u003c/h2\u003e\n\u003cp\u003e선형 칼만 필터와 사용된 모션 모델은 정확히 선형은 아니지만 여전히 단순하여 LKF가 처리할 수 있는 간단한 상수 속도 모델이었습니다. 참고로, 아래에 다시 표시해 드립니다. 이 모델은 확장 칼만 필터의 능력을 보여주기 위해 사용될 더 복잡한 변형을 소개하는 기초 역할을 할 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_21.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 방정식들은 다음과 같이 LKF에서 요구하는 선형 시스템 행렬로 변환되었습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 Markdown 형식입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_22.png\" alt=\"sensorfusion1\"\u003e\u003c/p\u003e\n\u003cp\u003eEKF를 사용하여 동일한 모델을 사용하려면 선형화해야 합니다. 먼저로봇의 상태 형식을 재정의합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_23.png\" alt=\"sensorfusion2\"\u003e\u003c/p\u003e\n\u003cp\u003e이후 비선형 함수 g를 정의합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_24.png\"\u003e\n\u003cp\u003e다음으로, 우리는 각 상태 변수 x, y, 그리고 θ에 대한 함수 g의 편도함수를 포함하는 야코비안 행렬 G를 정의합니다. 이 행렬은 상태 변수의 변화가 운동 모델에 어떻게 영향을 미치는지를 포착합니다. 야코비안 행렬 G는 EKF 알고리즘에서 공분산 행렬을 업데이트하는 데 사용될 것이며, 우리에게 운동 모델의 비선형성들을 고려하면서도 칼만 필터 프레임워크의 계산 효율성을 유지할 수 있게 해줍니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_25.png\"\u003e\n\u003cp\u003ex 업데이트 방정식의 x, y, 그리고 θ에 대한 도함수를 나타내는 야코비안 행렬 G의 첫 번째 행을 살펴보면, 다음과 같은 것을 볼 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e첫 번째 항목은 x에 대한 부분 도함수이기 때문에 1입니다.\u003c/li\u003e\n\u003cli\u003e두 번째 항목은 x가 y에 의존하지 않음을 나타내는 0입니다.\u003c/li\u003e\n\u003cli\u003e세 번째 항목은 −vsin(θ)Δt이며, 이것은 x가 θ에 −vsin(θ)Δt항으로 의존함을 보여줍니다. 이것은 θ의 변화가 x 좌표에 미치는 영향을 반영합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e다음으로 측정 함수 h가 필요합니다. 이전 Linear Kalman Filter의 구현과 유사하게, 사용할 측정은 로봇의 오도메트리 시스템에서 제공하는 것만 사용할 것입니다. 이 시스템은 바퀴 엔코더에서 계산된 로봇의 추정 자세를 직접 제공합니다. 그 데이터 형식이 우리의 상태 형식과 일치하기 때문에, h(μ^bar_t)는 단순히 μ^bar_t를 반환합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_26.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e그리고 상태에 대한 측정 함수의 부분 미분을 포함하는 Jacobian 행렬 H는 단위 행렬이며 아래와 같이 표시됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_27.png\" alt=\"sensor fusion\"\u003e\u003c/p\u003e\n\u003cp\u003e이전 그림에서 EKF와 LKF를 비교한 것처럼, 선형화가 두 알고리즘의 주요 차이점입니다. 관련 기능 및 해당 야코비안을 확인한 후, EKF의 구현은 LKF의 구현을 밀접하게 따릅니다. 아래는 선형화된 속도 모션 모델의 Python 구현입니다. 행렬 g와 야코비안 G를 모두 반환합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003evelocity_motion_model_linearized_1\u003c/span\u003e():\n\n \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003estate_transition_function_g\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003emu = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\n  x = mu[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n  y = mu[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n  theta = mu[\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]\n  \n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]       \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]       \n  \n  g = np.array([\n            x + v * np.cos(theta) * delta_t,\n            y + v * np.sin(theta) * delta_t,\n            theta + w * delta_t\n        ])\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e g\n\n \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_g_wrt_state_G\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003emu = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\n  theta = mu[\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]\n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]       \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]       \n  \n  G = np.array([\n   [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, -v * np.sin(theta) * delta_t],\n   [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, v * np.cos(theta) * delta_t],\n   [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n  ])\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e G\n\n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e state_transition_function_g, jacobian_of_g_wrt_state_G\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음으로, 다음과 같이 간단한 관측 모델을 구현합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eodometry_observation_model_linearized\u003c/span\u003e():\n def \u003cspan class=\"hljs-title function_\"\u003eobservation_function_h\u003c/span\u003e(mu):\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e mu\n \n def \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_h_wrt_state_H\u003c/span\u003e(mu):\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003eeye\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n\n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e observation_function_h, jacobian_of_h_wrt_state_H\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로, 아래에 구현된 Extended Kalman Filter 코드를 확인해보세요. 직전 게시물에서 소개된 Linear Kalman Filter 코드와 얼마나 비슷한지 주목하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np \n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e rse_motion_models.\u003cspan class=\"hljs-property\"\u003evelocity_motion_models\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e velocity_motion_model_linearized_1\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e rse_observation_models.\u003cspan class=\"hljs-property\"\u003eodometry_observation_models\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e odometry_observation_model_linearized\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eKalmanFilter\u003c/span\u003e:\n\n def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, initial_state, initial_covariance, proc_noise_std = [\u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e], obs_noise_std = [\u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e]):\n\n  self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e = initial_state # 초기 상태 추정\n  self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e = initial_covariance # 초기 불확실성\n\n  self.\u003cspan class=\"hljs-property\"\u003eg\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eG\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003evelocity_motion_model_linearized_1\u003c/span\u003e() # 사용할 액션 모델\n\n  # 과정 모델 노이즈의 표준 편차\n  self.\u003cspan class=\"hljs-property\"\u003eproc_noise_std\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(proc_noise_std)\n  # 과정 노이즈 공분산 (R)\n  self.\u003cspan class=\"hljs-property\"\u003eR\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003ediag\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eproc_noise_std\u003c/span\u003e ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e) \n\n  self.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eH\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003eodometry_observation_model_linearized\u003c/span\u003e() # 사용할 관측 모델\n\n  # 관측 모델 노이즈의 표준 편차\n  self.\u003cspan class=\"hljs-property\"\u003eobs_noise_std\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.\u003cspan class=\"hljs-property\"\u003eQ\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003ediag\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eobs_noise_std\u003c/span\u003e ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n\n def \u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003eg\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, u, dt)\n  # 공분산 (\u003cspan class=\"hljs-title class_\"\u003eSigma\u003c/span\u003e) 예측\n  self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003eG\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, u, dt) @ self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e @ self.\u003cspan class=\"hljs-title function_\"\u003eG\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, u, dt).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e + self.\u003cspan class=\"hljs-property\"\u003eR\u003c/span\u003e\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e\n\n def \u003cspan class=\"hljs-title function_\"\u003eupdate\u003c/span\u003e(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e @ self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e @ np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003einv\u003c/span\u003e(self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e) @ self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e @ self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e + self.\u003cspan class=\"hljs-property\"\u003eQ\u003c/span\u003e)\n  \n  # 상태 추정 (mu) 업데이트\n  self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e + K @ (z - self.\u003cspan class=\"hljs-title function_\"\u003eh\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e))\n\n  # 공분산 (\u003cspan class=\"hljs-title class_\"\u003eSigma\u003c/span\u003e) 업데이트\n  I = np.\u003cspan class=\"hljs-title function_\"\u003eeye\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(K)) \n  self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e = (I - K @ self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e)) @ self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e성능:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eExtended Kalman Filter(EKF)의 성능을 평가하기 위해, 동일한 상수 속도 모델과 오도메트리 관측 모델을 사용하는 선형 칼만 필터(LKF)의 성능과 비교할 것입니다. 먼저, 초기 과정 노이즈를 크게 설정하고 관측 노이즈를 매우 낮게 설정할 것입니다. 이 설정은 사실상 필터에게 행동 모델보다는 관측을 신뢰하도록 지시합니다. 예상했듯이, 두 버전의 칼만 필터는 관측을 따라가며 비슷한 성능을 발휘합니다. 관측 모델의 간단함과 선형성을 고려하면 이 결과가 예상된 것입니다.\u003c/p\u003e\n\u003cp\u003e다음 테스트는 더 어려울 것입니다. 과정 노이즈를 매우 낮게 설정하고 관측 노이즈를 매우 높게 설정할 것입니다. 이 설정은 필터가 대부분의 관측을 무시하고 상태 추정에 운동 모델에 크게 의존하게 만듭니다. 운동 방정식이 같더라도, EKF가 적용한 선형화 때문에 크게 다른 결과를 예상할 것입니다.\u003c/p\u003e\n\u003cp\u003e예상대로, 차이는 상당합니다. 이러한 노이즈 설정 하에서 LKF는 성능이 저조합니다. 비선형 운동 모델에 완전히 의존할 때, LKF의 결과는 실제 값과 로봇 센서에서 보고된 오도메트리에서 크게 벗어납니다. 반면에, EKF는 관측 데이터에 더 가까이 머물며 훨씬 나은 성능을 발휘합니다. 또한 오른쪽 그림의 큰 타원에서 나타나는 바와 같이 EKF는 높은 불확실성을 정확히 나타냅니다. 반면에, LKF는 낮은 불확실성을보고하는데, 이것은 부정확합니다.\u003c/p\u003e\n\u003ch2\u003e대안 운동 모델\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e과거 두 필터에서 사용된 모션 모델은 꽤 간단하며 로봇이 헤딩 각도 θ(세타)의 방향으로 직선으로 이동한다고 가정합니다. 보다 정교한 모션 모델이 존재하며, 그 모델이 더 나은 성능을 발휘할 수 있는지 확인하는 것이 중요합니다. 다음에 탐구할 새로운 상수 속도 모션 모델은 로봇을 변환(직선) 및 회전(각도) 속도 v 및 ω를 통해 제어할 수 있도록 합니다. 직선 이동을 가정하는 대신, 이 모델은 아래에 나와 있는 것처럼 반지름이 r인 원 위를 로봇이 이동한다고 가정합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_28.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e직선 모션 가정과 같이 원형 모션 가정도 근사값일 뿐이며, 시간 간격이 매우 작은 경우에만 유효합니다(움직임이 원이건 직선이건 구분할 수 없을 정도로 아주 작을 때). ω 값이 0에 가까워질수록, 반지름은 아주 크게되어 거의 직선 상에서의 움직임을 나타내게 됩니다. 시간에 따라 상태를 진화시키기 위해 이 모델을 따르는 비선형 함수 g에 해당하는 방정식 벡터는 아래에 나와 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_29.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제이 모델을 선형화하기 위해 g의 자코비안을 계산해야 합니다. 이는 상태에 대한 g의 편도함수에 해당합니다. 우리의 상태는 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_30.png\" alt=\"State\"\u003e\u003c/p\u003e\n\u003cp\u003e따라서 자코비안 G는 다음과 같이 표현됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_31.png\" alt=\"Jacobian\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위에서 보았듯이, 관련 행렬을 결정한 후에 구현은 간단합니다. 먼저, 아래 속도 모델 코드를 정의합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003evelocity_motion_model_linearized_2\u003c/span\u003e():\n\n def \u003cspan class=\"hljs-title function_\"\u003estate_transition_function_g\u003c/span\u003e(mu = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e):\n  x = mu[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n  y = mu[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n  theta = mu[\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]\n  \n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]       \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]       \n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e w == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n   w = \u003cspan class=\"hljs-number\"\u003e1e-6\u003c/span\u003e   # 직선 이동의 경우 \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e으로 나누는 것을 피하기 위해\n\n  g = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([\n     x + -v/w * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta) + v/w * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta + w * delta_t),\n     y + v/w * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta) - v/w * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta + w * delta_t),\n     theta + w * delta_t\n  ])\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e g\n\n def \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_g_wrt_state_G\u003c/span\u003e(mu = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e):\n  theta = mu[\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]\n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]       \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]       \n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e w == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n   w = \u003cspan class=\"hljs-number\"\u003e1e-6\u003c/span\u003e   # 직선 이동의 경우 \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e으로 나누는 것을 피하기 위해\n\n  G = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([\n   [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, -v / w * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta) + v / w * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta + w * delta_t)],\n   [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, -v / w * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta) + v / w * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta + w * delta_t)],\n   [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n  ])\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e G\n\n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e state_transition_function_g, jacobian_of_g_wrt_state_G\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e관측 모델은 이전 예제에서 사용한 것과 정확히 동일하며 완전성을 위해 여기에 다시 제시하겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eodometry_observation_model_linearized\u003c/span\u003e():\n def \u003cspan class=\"hljs-title function_\"\u003eobservation_function_h\u003c/span\u003e(mu):\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e mu\n \n def \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_h_wrt_state_H\u003c/span\u003e(mu):\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003eeye\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n\n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e observation_function_h, jacobian_of_h_wrt_state_H\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e마침내 두 번째 확장 칼만 필터가 아래에 구현되었습니다. 코드는 이전 것과 정확히 동일하지만 움직임 모델을 정의하는 라인만 예외입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eKalmanFilter\u003c/span\u003e:\n\n def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, initial_state, initial_covariance, proc_noise_std = [\u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e], obs_noise_std = [\u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e]):\n\n  self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e = initial_state # 초기 상태 추정\n  self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e = initial_covariance # 초기 불확실성\n\n  self.\u003cspan class=\"hljs-property\"\u003eg\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eG\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003evelocity_motion_model_linearized_2\u003c/span\u003e() # 사용할 액션 모델\n  \n  # 프로세스 또는 액션 모델 노이즈의 표준 편차\n  self.\u003cspan class=\"hljs-property\"\u003eproc_noise_std\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(proc_noise_std)\n  # 프로세스 노이즈 공분산 (R)\n  self.\u003cspan class=\"hljs-property\"\u003eR\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003ediag\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eproc_noise_std\u003c/span\u003e ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e) \n\n  self.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eH\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003eodometry_observation_model_linearized\u003c/span\u003e() # 사용할 관측 모델\n\n  # 관측 또는 센서 모델 노이즈의 표준 편차\n  self.\u003cspan class=\"hljs-property\"\u003eobs_noise_std\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.\u003cspan class=\"hljs-property\"\u003eQ\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003ediag\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eobs_noise_std\u003c/span\u003e ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n\n def \u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003eg\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, u, dt)\n  # 공분산 (\u003cspan class=\"hljs-title class_\"\u003eSigma\u003c/span\u003e) 예측\n  self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003eG\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, u, dt) @ self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e @ self.\u003cspan class=\"hljs-title function_\"\u003eG\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, u, dt).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e + self.\u003cspan class=\"hljs-property\"\u003eR\u003c/span\u003e \n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e\n\n def \u003cspan class=\"hljs-title function_\"\u003eupdate\u003c/span\u003e(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e @ self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e @ np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003einv\u003c/span\u003e(self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e) @ self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e @ self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e + self.\u003cspan class=\"hljs-property\"\u003eQ\u003c/span\u003e)\n  \n  # 상태 추정 (mu) 업데이트\n  self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e + K @ (z - self.\u003cspan class=\"hljs-title function_\"\u003eh\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e))\n\n  # 공분산 (\u003cspan class=\"hljs-title class_\"\u003eSigma\u003c/span\u003e) 업데이트\n  I = np.\u003cspan class=\"hljs-title function_\"\u003eeye\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(K)) \n  self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e = (I - K @ self.\u003cspan class=\"hljs-title function_\"\u003eH\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e)) @ self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003emu\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eSigma\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e성능:\u003c/p\u003e\n\u003cp\u003e이 시점에서 EKF가 LKF보다 우월함이 명확하며, 이제는 더 정교한 움직임 모델이 뚜렷한 향상을 가져오는지를 결정하는 것에 중점을 두고 연구를 계속할 것입니다. 관측 모델이 명백히 움직임 모델보다 우선하는 잡음 설정에서 평가를 수행하지 않을 것이며, 이렇게 하면 모든 필터에 대해 단순히 관측 결과와 일치할 것으로 예상됩니다. 따라서 여기서는 관측을 대부분 무시하고 움직임 모델에 더 집중하는 잡음 설정으로 결과를 제시합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e놀랍게도 더 고급 모델이 더 간단한 모델보다 우수한 성능을 내지 못했습니다. 위의 두 개의 도표에서는 순수한 눈으로는 중요한 성능 차이를 알아차리기 힘듭니다. 심지어 보고된 잡음 타원도 거의 같아 보입니다.\u003c/p\u003e\n\u003cp\u003e이전의 기사에서 얻은 결론 중 하나는 우리도 이곳에서 도출하는 것인데, 어떻게 소위 세련되고 고급스러운 필터를 사용하더라도 잘못된 데이터를 입력하면 잘못된 추정값을 제공할 것이라는 것입니다. 우리의 오도메트리 관측에 문제가 있다는 사실을 조정할 방법이 없네요. 이전의 모든 도표에서 주요 문제가 헤딩인 것처럼 보입니다. 올바른 헤딩 데이터가 없으면 어떤 고급 움직임 모델도 이를 보상할 수 없을 겁니다. 로봇이 헤딩의 대안 소스를 제공한다면 (그리고 다른 유용한 센서 데이터도 제공한다면), 필터를 사용하여 서로 다른 소스를 융합하고 더 나은 추정값을 얻을 수 있습니다. 다음에는 센서 융합을 살펴보겠습니다.\u003c/p\u003e\n\u003ch2\u003e센서 융합의 힘\u003c/h2\u003e\n\u003cp\u003e센서 융합에는 다양한 종류가 있으며, 우리가 적용할 유형은 저수준 융합으로 알려져 있습니다. 저수준 데이터 융합의 목표는 함께 더 유익한 여러 소스의 기본 센서 데이터를 결합하는 것입니다. 이 아이디어는 서로 다른 소스의 강점을 활용하여 더 나은 추정값을 만들어내는 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 기사에서는 원래 오도메트리 데이터와 관성 측정 장치(IMU)에서 오는 데이터를 어떻게 결합하는지에 대해 연구할 것입니다. IMU는 일반적으로 우리에게 방향, 각속도 및 선형 가속도를 제공할 수 있습니다. 이는 우리가 부정확한 추정이 나쁜 방향 정보 때문인 것으로 의심하기 때문에 매우 유익합니다. IMU는 이러한 종류의 정보에 대해 오도메터보다 정확할 경향이 있기 때문에, IMU 데이터가 오도메트리와 어떻게 융합되어 더 나은 결과를 얻을 수 있는지 알아보겠습니다.\u003c/p\u003e\n\u003cp\u003e우리가 할 첫 번째 일은 상태와 모델을 확장하는 것입니다. 이전에는 추가 데이터의 좋은 소스가 없었기 때문에 로봇의 자세 이상을 고려하는 것이 별 의미가 없었습니다. 그러나 이제 우리에게 각속도와 선형 가속도를 제공할 수 있는 센서가 있기 때문에, 새로운 정보를 상태에 통합하는 것이 로봇의 상태를 더 잘 추정하는 데 도움이 될 것입니다. 이는 또한 우리의 동작 및 센서 모델을 확장할 것이기 때문에 필수적입니다. 이 기사에서는 새로운 7차원 상태로 시작하는 두 가지 다른 확장을 시도할 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_32.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 x, y 및 θ는 친숙한 로봇 자세에 해당합니다. 과거 구현과는 달리, 속도 v와 각속도 ω가 제어 입력에서만 얻어졌던 경우, 이제 이들의 추정치도 상태의 일부로 유지합니다. 상태에는 x 및 y의 선형 가속도도 포함됩니다. 확장된 상태를 고려할 때, 동작 모델도 각 상태 구성 요소를 계산하기 위해 확장되어야 합니다. 가속도를 고려하는 동작 모델은 더 이상 상수 속도 모델이 될 수 없습니다. 따라서 속도는 추정 사이에 변화하고, 가속도는 일정하게 유지된다는 새로운 가속도 일정 모델을 사용할 것입니다. 이 새로운 가속도 상수 모델은 아래 표시된 함수 g와 야코비안 행렬 G에 의해 표현됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_33.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_34.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e지금부터 조금 복잡해집니다. 이 큰 야코비안 행렬을 계산하는 것은 매우 에러가 발생하기 쉽습니다. 수동으로 시도해보는 것도 재미있지만, 항상 결과를 확인하기 위해 소프트웨어를 사용하는 것이 좋은 생각입니다. 계산이 올바르다고 확신하고 구현한 후에도 해당 구현이 정확한지 확인할 방법이 있어야 합니다. 이 기사에 구현된 모든 야코비안에 대해, 테스트 스크립트가 잡아낸 적어도 하나의 작은 오류가 있었습니다. 야코비안을 계산하고 구현이 올바른지 확인하는 데 소프트웨어를 사용하지 않으면 코드에 버그를 도입할 확률이 높습니다.\u003c/p\u003e\n\u003cp\u003e상태 및 운동 모델을 성공적으로 확장했습니다. 이제 센서 퓨전을 가능하게 하려면 관찰 모델도 확장해야 합니다. 확장된 상태 및 운동 모델과 결합하여 퓨전을 수행할 수 있도록 두 센서의 데이터를 연결하려 할 것입니다. 새로운 관찰 모델은 아래에 표시된 벡터의 첫 세 요소인 보통의 로봇 자세와 IMU 센서에서 파생된 방향 θ_imu, 각 속도 ω, 그리고 x 및 y 구성 요소에서의 가속도 a_x 및 a_y와 조합된 것입니다. 우리가 선속도 v를 직접 관측하지는 않지만 여전히 상태에 있을 수 있으며, Kalman 필터에 의해 운동 모델을 통해 추론될 것입니다. 이와 같은 관측되지 않는 변수를 숨겨진 또는 잠재 변수라고 부르는 경우가 종종 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음은 아래에 정의된 자코비안 행렬입니다. 관측값에서 상태 변수로의 직접적인 매핑이 있으므로, 자코비안에서 관측값(행)이 상태 변수(열)에 해당될 때는 1이 있습니다. 자코비안에서 상태 변수 θ에 해당하는 3열에는 두 개의 1이 있음을 주목해 주세요 — odometry에서 θ에 대한 행과 IMU에서 θ에 대한 행이 각각 하나씩 있습니다. 또한, 선형 속도에 해당하는 4열은 직접 관측하지 않기 때문에 모두 0입니다.\u003c/p\u003e\n\u003cp\u003e구현된 모션 및 관측 모델은 아래에 나와 있습니다. 필터의 구현은 이전 것과 매우 유사하므로 간결함을 위해 생략하겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eacceleration_motion_model_linearized_1\u003c/span\u003e():\n\n def \u003cspan class=\"hljs-title function_\"\u003estate_transition_function_g\u003c/span\u003e(mu = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e):\n  \n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]      \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e] \n  \n  g = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([\n   x + v * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta) * delta_t + \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e * a_x * delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,      \n      y + v * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta) * delta_t + \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e * a_y * delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,    \n      theta + w * delta_t,\n      v + a_x * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta) * delta_t + a_y * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta) * delta_t,\n      w,                                                      \n      a_x,                                                              \n      a_y\n  ])\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e g\n\n def \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_g_wrt_state_G\u003c/span\u003e(mu = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e):\n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]       \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]       \n\n  G = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, -delta_t * v * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta), delta_t  * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta), \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e*delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],   \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, delta_t * v * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta), delta_t * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta), \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e*delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e],       \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, delta_t, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],                                      \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, -delta_t * a_x * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta) + delta_t * a_y * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta), \n                   \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, delta_t * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta), delta_t * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta)],                  \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],                                          \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],                                          \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e]])                                         \n  \n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e G\n\n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e state_transition_function_g, jacobian_of_g_wrt_state_G\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eodometry_imu_observation_model_with_acceleration_motion_model_linearized_1\u003c/span\u003e():\n def \u003cspan class=\"hljs-title function_\"\u003eobservation_function_h\u003c/span\u003e(mu):\n  x, y, theta, v, w, ax, ay = mu\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_h_wrt_state_H\u003c/span\u003e():\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],    \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],        \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e]])\n\n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e observation_function_h, jacobian_of_h_wrt_state_H\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e성능:\u003c/p\u003e\n\u003cp\u003e더 정교한 모션 모델을 사용하고 오도메트리 데이터와 IMU 데이터를 퓨즈하는 개선된 EKF를 평가하는 시간입니다. 특히, 노이즈 매개변수를 다음과 같이 설정하겠습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eproc_noise_std = [\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.05\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e] # [x, y, theta, v, w, a_x, a_y]\nobs_noise_std = [\u003cspan class=\"hljs-number\"\u003e100.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e100.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1000.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e6.853891945200942e-06\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0966227112321507e-06\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0015387262937311438\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0015387262937311438\u003c/span\u003e] #[x, y, theta, theta_imu, w, a_x, a_y]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 설정은 우리의 모션 모델에 대해 높은 신뢰를 가지고 있지만 관측 모델에 대해서는 IMU 데이터를 오도메트리 데이터보다 더 신뢰한다고 필터에 알려줍니다. 특히, 우리는 필터에게 오도메트리에서의 헤딩이 미친 것이라고 생각하고 IMU에서의 헤딩이 매우 정확하다고 말하고 있습니다. 이는 IMU가 매우 정밀할 수 있고 오도메트리 데이터가 악명 높게 나쁠 수 있기 때문에 종종 사실입니다. 이 설정은 우리가 예측한 궤적을 수정하는 데 도움이 될 수 있는데, 이전에 본 것처럼 모양은 그리 나쁘지 않지만 방향은 매우 잘못된 경우가 많습니다.\u003c/p\u003e\n\u003cp\u003e아래 비디오와 이어지는 두 그림에서 볼 수 있듯이 가속도 모델과 센서 퓨전이 포함된 새 필터는 이전 필터보다 훨씬 더 잘 수행됩니다. 특히 IMU로부터 제공된 더 나은 방향성 덕분에 추정 궤적이 초기에 실제 궤적에 훨씬 가까웠음을 볼 수 있습니다. 그러나 마지막에는 지그재그 패턴을 따르기 시작했습니다.\u003c/p\u003e\n\u003cp\u003e더 나은 결과를 얻기 위한 레시피는 없습니다. 이것이 필터를 설계하는 것을 과학보다는 예술로 만드는 것입니다. 위의 지그재그 궤적을 개선할 수 있는 더 나은 모션 모델로 수정할 수 있을 것이라고 생각할 수 있습니다. 이 결과를 본 것처럼 나도 같은 방식으로 느꼈습니다. 모션 모델을 개선해 보겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e모션 모델 개선하기\u003c/h2\u003e\n\u003cp\u003e센서 퓨전은 도움이 되고 있지만, 아직 개선할 부분이 많이 남아 있습니다. 상태 추정을 위한 필터를 설계하려면 매우 교육된 추측을 하고 직관을 따라야 합니다. 앞서 논의한 대로, 최근 얻은 결과는 격려적이며 우리가 모델을 개선해 보아야 한다는 제안이 있습니다. 특히 상태를 모델링할 때 글로벌 선속도만을 고려하고, 가속도와 같이 다른 축을 따른 속도를 고려하지 않는 것으로 보입니다. 우리의 상태 벡터를 확장하여 이를 고려해보겠습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_37.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e새로운 상태 벡터에는 로봇의 움직임에 대한 더 자세한 정보를 제공할 x와 y 성분의 속도가 포함되어 있습니다. 이 확장이 필터의 성능에 어떤 영향을 미치는지 알아보겠습니다. g 함수와 해당 야코비안 G을 이용하여 표현된 확장된 모션 모델은 아래와 같습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_38.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_39.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003eThe observation model function h remains the same, but the Jacobian H now has two columns filled with zeros corresponding to the unobserved state variables v_x and v_y.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_40.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e모델과 사용된 잡음 매개변수에 대한 코드는 다음과 같습니다. 여기서 추가적으로 필터 코드는 이전에 소개된 코드와 매우 유사하므로 생략될 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eacceleration_motion_model_linearized_2\u003c/span\u003e():\n\n def \u003cspan class=\"hljs-title function_\"\u003estate_transition_function_g\u003c/span\u003e(mu = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e):\n  \n  x, y, theta, v_x, v_y , w, a_x, a_y = mu\n\n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]       \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]       \n  \n  g = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([\n      x + v * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta) * delta_t + \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e * a_x * delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \n      y + v * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta) * delta_t + \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e * a_y * delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \n      theta + w * delta_t,                                   \n      v * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta) + a_x * delta_t,                         \n      v * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta) + a_y * delta_t,                         \n      w,                                                         \n      a_x,                                                       \n      a_y                                                        \n  ])\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e g\n\n def \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_g_wrt_state_G\u003c/span\u003e(mu = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, u = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, delta_t = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e):\n  x, y, theta, v_x, v_y, w, a_x, a_y = mu\n\n  v = u[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]       \n  w = u[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]       \n\n  G = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, -delta_t * v * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta), \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e*delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],   \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, delta_t * v * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta), \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e*delta_t**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e],        \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, delta_t, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],                                      \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, -v * np.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(theta), \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, delta_t, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],                       \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, v * np.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e(theta), \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, delta_t],                        \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],                                          \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],                                          \n                   [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e]])                                         \n  \n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e G\n \n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e state_transition_function_g, jacobian_of_g_wrt_state_G\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eodometry_imu_observation_model_with_acceleration_motion_model_linearized_2\u003c/span\u003e():\n def \u003cspan class=\"hljs-title function_\"\u003eobservation_function_h\u003c/span\u003e(mu):\n  x, y, theta, v_x, v_y, w, ax, ay = mu\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def \u003cspan class=\"hljs-title function_\"\u003ejacobian_of_h_wrt_state_H\u003c/span\u003e():\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],       \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],            \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e],         \n                    [\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e]])\n \n \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e observation_function_h, jacobian_of_h_wrt_state_H\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eproc_noise_std = [\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.05\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e] # [x, y, theta, v_x, v_y, w, a_x, a_y]\nobs_noise_std = [\u003cspan class=\"hljs-number\"\u003e100.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e100.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1000.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e6.853891945200942e-06\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0966227112321507e-06\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0015387262937311438\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0015387262937311438\u003c/span\u003e] #[x, y, theta, theta_imu, w, a_x, a_y]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e성능:\u003c/p\u003e\n\u003cp\u003e새로운 필터는 이전 동영상과 아래 그림에서 명백히 확인할 수 있듯이 인상적인 성과를 보여주었습니다. 예측된 궤적이 이제 센서 퓨전과 더 정교한 모델의 덕분에 실제 지면에 매우 가까워졌습니다. 게다가 그림의 최신 자세에서 타원으로 표시된 최종 불확실성은 이전 필터 버전보다 훨씬 작습니다. 이는 센서 퓨전이 잘 설계된 EKF의 효과를 보여줍니다. 추가로 소음 매개변수를 조정하면 더 많은 개선이 가능하지만, 이는 기사의 길이 때문에 여기서 탐구되지 않을 것입니다.\u003c/p\u003e\n\u003ch2\u003e직접 시도해보세요\u003c/h2\u003e\n\u003cp\u003e이전 기사와 마찬가지로, 코드는 전체 코드를 검사하거나 단순히 알고리즘을 실행하고 결과를 실시간으로 확인하려는 사람들을 위해 제공됩니다. 코드를 실행하려면 아래 지시사항을 따르십시오.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e먼저 필요한 것은 ROS 2입니다. ROS 2 Humble을 Ubuntu Jammy Jellyfish (22.04)에 설치하는 방법은 여기에서 찾을 수 있습니다. Ubuntu의 다른 버전이나 다른 운영 체제의 경우 공식 ROS 2 문서를 참고하십시오.\u003c/p\u003e\n\u003cp\u003e데이터를 얻으려면 이 링크에서 ROS 2 가방을 다운로드해야 합니다. 사용하기 전에 파일을 압축 해제해야 합니다.\u003c/p\u003e\n\u003cp\u003e마지막으로 ROS 2 패키지를 복제하고 빌드해야 합니다. 아래 단계를 따라 진행할 수 있습니다. ros2_ws를 실제 ROS 2 작업 공간으로 교체해야 합니다.\u003c/p\u003e\n\u003ch1\u003e종속성 설치\u003c/h1\u003e\n\u003cp\u003esudo apt install python3-pykdl\u003c/p\u003e\n\u003ch1\u003e패키지를 복제하고 빌드\u003c/h1\u003e\n\u003cp\u003ecd ros2_ws/src\ngit clone \u003ca href=\"https://github.com/carlos-argueta/rse_prob_robotics.git\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/carlos-argueta/rse_prob_robotics.git\u003c/a\u003e\ncd ..\ncolcon build --symlink-install\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e확장 칼만 필터를 실행하려면 3개의 다른 터미널을 열어야 합니다.\u003c/p\u003e\n\u003cp\u003e터미널 1에서 (ros2_ws를 실제 워크스페이스로 교체해주세요) 다음 명령을 실행하여 Rviz를 열고 로봇이 보고 있는 것을 확인하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esource ~\u003cspan class=\"hljs-regexp\"\u003e/ros2_ws/i\u003c/span\u003enstall/setup.\u003cspan class=\"hljs-property\"\u003ebash\u003c/span\u003e\nros2 launch rse_gaussian_filters rviz_launch.\u003cspan class=\"hljs-property\"\u003elaunch\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e터미널 2에서 확장 칼만 필터의 버전에 따라 다음 명령 중 하나를 실행하세요. 먼저 아무 출력도 나오지 않을 것이며, ROS 2 가방(bag)을 실행할 때까지 기다리세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esource ~\u003cspan class=\"hljs-regexp\"\u003e/ros2_ws/i\u003c/span\u003enstall/setup.\u003cspan class=\"hljs-property\"\u003ebash\u003c/span\u003e\n\n# 아래 명령어 중 하나 실행\n\n# 3D 상태, 기본 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v1 \n\n# 3D 상태, 고급 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v2 \n\n# 7D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_7d \n\n# 8D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_8d \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e터미널 3에서 ROS 2 가방이 추출된 위치로 이동하여 다음 명령어로 재생하십시오. \"Ignoring a topic '/navrelposned', reason: package 'ublox_msgs' not found\"와 같은 경고 메시지를 무시해도 됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eros2 bag play linkou-\u003cspan class=\"hljs-number\"\u003e2023\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e27\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e-med --clock\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위 단계를 따르면 확장 칼만 필터를 실행하고 결과를 실시간으로 확인할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e실용적인 고려사항 및 마지막으로\u003c/h1\u003e\n\u003cp\u003e이 글은 확장 칼만 필터 (EKF)를 기본적인 선형 칼만 필터 (LKF)의 확장으로 소개했습니다. EKF는 대부분의 실세계 시스템의 비선형성을 근사화를 통해 다룹니다. 이는 사후 평균 주변의 비선형 함수를 일차 테일러 전개를 이용하여 선형화합니다. 선형화된 후, EKF는 LKF와 유사하게 작동합니다.\u003c/p\u003e\n\u003cp\u003e보여진 대로, 센서 퓨전과 결합된 EKF는 놀라운 결과를 얻을 수 있습니다. 그러나 좋은 필터를 설계하는 것은 어렵고 오류를 범하기 쉽습니다. 종종 과학과 예술 사이의 균형을 요구합니다. 적합한 움직임 및 관측 모델을 찾는 것이 첫 번째 난관입니다. 이동 로봇에 대한 사용 가능한 모델이 있더라도, 다른 시나리오는 좋은 모델이 부족할 수 있습니다. 자코비안 계산 또한 어렵고 실수하기 쉬우며, 소프트웨어 확인이 필요함을 강조합니다.\u003c/p\u003e\n\u003cp\u003e효과적인 EKF 설계를 위해 여러 다른 고려사항과 결정이 중요합니다. 한 가지 중요한 측면은 올바른 시간 간격(delta_t)을 선택하는 것으로, 효과적인 선형화를 위해 충분히 작아야 합니다. 이 경우, 고정값 대신 동적 delta_t가 사용되었습니다. 필터의 업데이트 단계를 수행할 때 언제, 어떻게 결정하는지도 중요합니다. 특히 서로 다른 속도로 측정 값을 제공하는 다른 센서가 있을 때 (예: IMU는 보통 오도메트리보다 높은 주파수로), 적절한 노이즈 매개 변수 선택도 여러 가지 방법이 가능한 복잡한 작업입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e요약하자면, 확장 칼만 필터는 특히 센서 퓨전과 함께 사용될 때 우수한 상태 추정 결과를 제공할 수 있습니다. 그러나 견고한 EKF를 설계하려면 상당한 노력과 연습이 필요합니다. 다음 글에서는 Unscented Kalman Filter (UKF)를 소개할 예정입니다. UKF는 EKF보다 여러 장점을 제공합니다. Unscented Transform을 사용하여 비선형 변환의 평균과 공분산을 더 정확하게 캡처합니다. UKF는 Jacobian을 필요로하지 않아 구현을 간단하게 만들어줍니다. 선형화로 인한 근사 오차를 줄이므로 매우 비선형 시스템에 더 효과적입니다. 또한 UKF는 비가우시안 분포를 더 잘 처리하여 견고성과 다양성을 향상시킵니다.\u003c/p\u003e\n\u003ch1\u003e독후감\u003c/h1\u003e\n\u003cp\u003e다음은 칼만 필터 패밀리에 대해 학습하기 위해 참고한 훌륭한 자료 목록입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOptimal State Estimation: Kalman, H∞, and Nonlinear Approaches\u003c/li\u003e\n\u003cli\u003eState Estimation for Robotics\u003c/li\u003e\n\u003cli\u003eKalman and Bayesian Filters in Python\u003c/li\u003e\n\u003cli\u003eProbabilistic Robotics\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 기사가 도움이 되셨기를 바랍니다. 피드백이 있으시면 언제든지 댓글을 남겨주세요. 또한, 이후의 주제를 다룬 보다 심도 있는 강좌 시리즈를 시작하려고 합니다. 이 강좌는 비디오, 코딩 프로젝트 등이 포함될 예정이며 유료로 운영될 가능성이 높습니다. 이런 강좌에 관심이 있다면 댓글로 알려주시면 참여 의향을 파악할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e저와 소통하고 싶다면 LinkedIn에서 저를 찾아보세요: \u003ca href=\"https://www.linkedin.com/in/carlos-argueta\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://www.linkedin.com/in/carlos-argueta\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e저와 함께 ROS 2를 이용한 로보틱스를 배우고 싶으신가요? 제 라이브 강의에 참여해보세요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>