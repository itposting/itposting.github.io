---
title: "GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드"
description: ""
coverImage: "/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png"
date: 2024-06-22 21:29
ogImage: 
  url: /assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png
tag: Tech
originalTitle: "Large Language Models, GPT-3: Language Models are Few-Shot Learners"
link: "https://medium.com/towards-data-science/large-language-models-gpt-3-language-models-are-few-shot-learners-6e1261a1b466"
---


## 메타러닌 학습 프레임워크 내에서 대규모에서 타이타닉 규모로 GPT를 효율적으로 확장하는 방법

![그림](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png)

# 소개

GPT는 최근 많은 인기를 얻고 있는 언어 모델 가족입니다. 데이터 과학 커뮤니티의 관심은 GPT-3가 2020년에 출시되면서 급속하게 집중되었습니다. GPT-2가 등장한 후, 거의 누구도 1년 안에 175B의 매개변수를 포함한 GPT의 타이타닉 버전이 출현할 것으로 상상하지 못했습니다! 이는 이전 버전과 비교했을 때 2차적으로 두 배 많은 양입니다.

<div class="content-ad"></div>

GPT-3의 엄청난 용량 덕분에 코드 완성, 글 작성, 콘텐츠 생성, 가상 어시스턴트 등 다양한 일상 시나리오에서 사용할 수 있었습니다. 이러한 작업의 품질이 항상 완벽하지는 않지만, GPT-3이 달성한 전반적인 진전은 정말 놀랍습니다!

이 기사에서는 GPT-3의 주요 세부 사항과 GPT-2 창조자들로부터 영감을 받은 유용한 아이디어를 자세히 살펴볼 것입니다. 탐구하는 동안, 공식 GPT-3 논문을 참조하겠습니다. GPT-3의 대부분의 설정은 GPT-2에서 직접 파생된 데이터 수집, 구조 선택 및 사전 훈련 과정을 포함합니다. 그래서 대부분의 시간을 GPT-3의 새로운 측면에 집중할 것입니다.

# 메타-러닝 프레임워크

GPT-3 창조자들은 GPT-2에서 사용된 학습 방법에 대해 매우 관심을 가졌습니다: 일반적인 사전 훈련 + 미세 조정 프레임워크 대신, 저자들은 크고 다양한 데이터 세트를 수집하고 텍스트 입력에 작업 목표를 통합했습니다. 이 방법론은 여러 가지 이유로 편리했습니다:

<div class="content-ad"></div>

- 미세 조정 단계를 제거함으로써 이제는 개별 하위 작업을 위해 여러 대규모 레이블된 데이터 세트가 더 이상 필요하지 않습니다.
- 다른 작업에 대해 하나의 모델 버전을 여러 개 사용하는 대신 하나만 사용할 수 있습니다.
- 모델은 사람이 하는 것과 더 유사한 방식으로 작동합니다. 대부분의 경우 사람들은 주어진 작업을 완전히 이해하려면 언어 예제가 전혀 필요하지 않거나 몇 개만 필요합니다. 추론 중에 모델은 해당 예제를 텍스트 형식으로 받을 수 있습니다. 그 결과로 이 측면은 인간과 상호 작용하는 AI 애플리케이션을 개발하는 데 더 나은 전망을 제공합니다.
- 모델은 한 번만 단일 데이터 세트에서 훈련됩니다. 미세 조정 + 미세 조정 패러다임과 달리, 모델은 완전히 다른 데이터 분포를 가질 수 있던 두 가지 다른 데이터 세트에서 훈련되어야 했다는 점으로 잠재적인 일반화 문제를 야기할 수 있었습니다.

공식적으로, 설명된 프레임워크를 메타 학습이라고 합니다. 논문은 공식적인 정의를 제공합니다:

학습 패러다임을 더 자세히 설명하기 위해 내부 및 외부 루프 용어가 소개됩니다. 기본적으로 내부 루프는 훈련 중에 단일 순방향 패스에 해당하며 외부 루프는 모든 내부 루프를 나타냅니다.

훈련 과정 동안 모델은 다른 텍스트 예제에서 유사한 작업을 받을 수 있습니다. 예를 들어, 모델은 서로 다른 배치에서 다음과 같은 예제를 볼 수 있습니다:

<div class="content-ad"></div>

- "Good"은 "excellent"의 동의어입니다.
- "Computer"은 "laptop"의 동의어입니다.
- "House"는 "building"의 동의어입니다.

이 예시들은 모델이 어떻게 동의어를 이해하는지를 돕는데 도움이 되며, 특정 단어의 동의어를 찾을 때 유용할 수 있습니다. 특정 작업 내에서 비슷한 언어 지식을 습득하도록 도와주는 예시의 조합을 "컨텍스트 학습"이라고 합니다.

<img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_1.png" />

## n-shot 학습

<div class="content-ad"></div>

추론 중에 수행되는 모델의 쿼리는 작업 예제를 추가로 포함할 수 있습니다. 작업 데모가 쿼리의 목적을 더 잘 이해하는 데 중요한 역할을 한다는 것을 알 수 있었습니다. 제공된 작업 예제의 수(샷)에 따라 아래 표에 요약된 세 가지 유형의 학습이 존재합니다:

| 샷 수 | 학습 유형 |
|-------|------------|
| 0     | Zero-shot  |
| 1     | One-shot   |
| 2+    | Few-shot   |

대부분의 경우(항상은 아니지만) 제공된 예제의 수가 모델이 올바른 답변을 제공하는 능력과 긍정적으로 상관 관계가 있음을 알 수 있습니다. 저자들은 다른 크기의 모델을 세 가지 n-shot 설정 중 하나에 사용하여 연구를 완료했습니다. 결과에서 용량이 증가함에 따라 모델이 문맥 학습에 더 능숙해진다는 것을 보여줍니다. 성능 차이가 증가하는 선 그래프로 이를 시연합니다. 적은 수, 한 개, 영 샷 설정 간의 성능 차이가 모델 크기와 함께 더 커짐을 보여줍니다.

<div class="content-ad"></div>

# 아키텍처

이 논문은 GPT-3의 아키텍처 설정을 정확히 설명하고 있습니다.

## 데이터셋

초기에 저자들은 GPT-3를 훈련하기 위해 Common Crawl 데이터셋을 사용하고자 했습니다. 이 굉장히 큰 데이터셋은 다양한 주제의 데이터를 담고 있습니다. 그러나 초기의 원본 데이터셋은 데이터 품질에 문제가 있어 처음에는 필터링되고 중복이 제거되었습니다. 최종 데이터셋을 더욱 다양하게 만들기 위해 아래 다이어그램에 표시된 네 가지 다른 작은 데이터셋과 연결되었습니다:

<div class="content-ad"></div>

<img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_4.png" />

# 훈련 세부 정보

- 옵티마이저: Adam (β₁ = 0.9, β₂ = 0.999, ε = 1e-6).
- 폭주 그래디언트 문제를 방지하기 위해 1.0에서 그래디언트 클리핑이 사용됩니다.
- 학습 속도 조정을 위해 코사인 감쇠와 선형 웜업의 조합이 사용됩니다.
- 배치 크기는 학습 중 32K에서 3.2M 토큰으로 점진적으로 증가합니다.
- 0.1의 가중치 감쇠가 정규화자로 사용됩니다.
- 더 나은 계산 효율성을 위해 모든 시퀀스의 길이가 2048로 설정됩니다. 단일 시퀀스 내의 다른 문서는 구분자 토큰으로 분리됩니다.

## 빔 탐색

<div class="content-ad"></div>

GPT-3는 자기회귀 모델입니다. 이것은 과거의 예측된 단어에 대한 정보를 사용하여 미래의 다음 단어를 예측하는 데 사용합니다.

탐욕 알고리즘은 자기회귀 모델에서 텍스트 시퀀스를 구성하기 위한 가장 단순한 방법 중 하나입니다. 각 반복에서 모델이 가장 가능성이 높은 단어를 선택하도록 강제하고, 이를 다음 단어의 입력으로 사용합니다. 그러나 현재 반복에서 가장 가능성이 높은 단어를 선택하는 것이 로그 우도 최적화에 대한 최선의 방법은 아닙니다!

이미지 링크: ![이미지](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_5.png)

낮은 확률로 현재 단어를 선택하면 나머지 예측된 단어들의 확률이 높아질 수 있는 상황이 발생할 수 있습니다. 한편, 지역 단어를 가장 높은 확률로 선택하는 것은 그 다음 단어들도 높은 확률에 해당한다는 것을 보장하지는 않습니다. 탐욕 전략이 최적으로 작동하지 않는 경우를 보여주는 예시는 아래 다이어그램에 나와 있습니다:

<div class="content-ad"></div>


![Image](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_6.png)

가능한 해결책은 모든 옵션 중에서 가장 가능성이 높은 시퀀스를 찾는 것입니다. 그러나 이 접근 방식은 가능한 시퀀스의 조합이 무수히 많기 때문에 매우 효율적이지 않습니다.

빔 서치(Beam Search)는 탐욕 알고리즘과 모든 가능한 조합을 탐색하는 것 사이의 좋은 절충안입니다. 각 반복에서 빔 서치는 가장 가능성이 높은 토큰을 여러 개 선택하고 현재 가장 가능성이 높은 시퀀스 집합을 유지합니다. 새로운 더 가능성 있는 시퀀스가 형성될 때마다, 해당 시퀀스 중 가장 가능성이 낮은 것을 대체합니다. 알고리즘의 끝에는 집합에서 가장 가능성이 높은 시퀀스가 반환됩니다.

![Image](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_7.png)


<div class="content-ad"></div>

빔 서치는 최상의 검색 전략을 보장하지는 않지만 실제로는 근사치가 매우 잘 동작합니다. 그 이유로 GPT-3에서 사용됩니다.

# 단점

GPT-3는 인간과 유사한 긴 텍스트 조각을 생성하는 놀라운 능력을 가졌지만 몇 가지 단점이 있습니다:

- 텍스트 생성 중 GPT-3이 내린 결정은 일반적으로 해석하기 어려워 분석하기 어렵습니다.
- GPT-3은 모델로 항상 방지할 수 없는 해로운 방식으로 사용될 수 있습니다.
- GPT-3는 학습 데이터 세트에 편향이 있어 때때로 공정성 측면에서 취약할 수 있습니다. 특히, 성별 평등, 종교 또는 인종과 같은 민감한 도메인에 관련된 경우입니다.
- 이전의 GPT-2 보다 GPT-3는 훈련에 수십 배나 더 많은 에너지(수천 페타플랍 / 일)가 필요한데, 이는 친환경적이지 않습니다. 동시에, GPT-3 개발자들은 이 모델이 추론 중에 매우 효율적이기 때문에 평균 소비량이 여전히 낮다는 점으로 이 측면을 정당화합니다.

<div class="content-ad"></div>

# 결론

GPT-3는 상상할 수 없는 175B개의 훈련 가능한 매개변수를 보유하여 이전 모델들을 강력히 이기는 몇 가지 최고의 기준들에서 엄청난 인기를 얻었습니다! 그 당시에 GPT-3 결과는 때때로 사람이 생성한 텍스트인지 GPT-3가 생성한 것인지 구별하기가 어려울 정도로 좋았습니다.

GPT-3의 몇 가지 단점과 제한사항에도 불구하고, 이는 연구자들에게 미래에 대한 새로운 탐구와 잠재적 개선의 가능성을 여는 문이 되었습니다.

# 자료들

<div class="content-ad"></div>

- 언어 모델은 소수 샷 학습자들입니다.

모든 이미지는 특별히 언급되지 않는 한 작성자가 찍은 것입니다.