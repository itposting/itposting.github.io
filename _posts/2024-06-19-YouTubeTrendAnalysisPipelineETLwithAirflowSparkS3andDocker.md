---
title: "ìœ íŠœë¸Œ íŠ¸ë Œë“œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ Airflow, Spark, S3 ë° Dockerë¥¼ ì´ìš©í•œ ETL"
description: ""
coverImage: "/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png"
date: 2024-06-19 09:37
ogImage: 
  url: /assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png
tag: Tech
originalTitle: "YouTube Trend Analysis Pipeline: ETL with Airflow, Spark, S3 and Docker"
link: "https://medium.com/@swathireddythokala16/youtube-trend-analysis-pipeline-etl-with-airflow-spark-s3-and-docker-85a7d76992eb"
---


ì´ ê¸°ì‚¬ì—ì„œëŠ” Apache Airflowì™€ PySparkë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ETL (ì¶”ì¶œ, ë³€í™˜, ë¡œë“œ) íŒŒì´í”„ë¼ì¸ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ì´ íŒŒì´í”„ë¼ì¸ì€ YouTube Data APIì—ì„œ íŠ¸ë Œë“œ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•œ í›„ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ S3ì— ì €ì¥í•  ê²ƒì…ë‹ˆë‹¤.

Twitter APIë¥¼ ì‚¬ìš©í•œ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì£¼ëŠ” Darshil Parmarì˜ YouTube ë¹„ë””ì˜¤ë¥¼ ì‹œì²­í•œ í›„, ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ì— ë„ì „í•˜ê¸°ë¡œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Twitter APIì˜ ê°€ê²© ì •ì±… ë³€ê²½ìœ¼ë¡œ ì¸í•´, ì‹œì²­ìê°€ YouTube Data APIë¥¼ ëŒ€ì²´ë¡œ ì œì•ˆí–ˆê³  ì´ê²ƒì´ ì œ í¥ë¯¸ë¥¼ ìê·¹í–ˆìŠµë‹ˆë‹¤.

í”„ë¡œì íŠ¸ì— ëŒì…í•˜ê¸° ì „ì— ë‘ ê°€ì§€ í•„ìˆ˜ ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤:

1. Youtube Data API í‚¤ íšë“

<div class="content-ad"></div>

- Google Developers Consoleì„ ë°©ë¬¸í•´ ì£¼ì„¸ìš”.
- ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.
- "YouTube Data API"ë¥¼ ê²€ìƒ‰í•˜ê³  í™œì„±í™”í•´ ì£¼ì„¸ìš”.
- ìƒˆ ìê²© ì¦ëª…ì„ ìƒì„±í•˜ê³  í”„ë¡œì íŠ¸ì—ì„œ ë‚˜ì¤‘ì— ì‚¬ìš©í•  API í‚¤ë¥¼ ë³µì‚¬í•´ ì£¼ì„¸ìš”.

ìì„¸í•œ ì§€ì¹¨ì€ YouTube Data API ì‹œì‘ ê°€ì´ë“œë¥¼ ì°¸ì¡°í•´ ì£¼ì„¸ìš”.

2. AWS ì•¡ì„¸ìŠ¤ í‚¤ ID ë° ë¹„ë°€ ì•¡ì„¸ìŠ¤ í‚¤ íšë“

- AWS Management Consoleì— ë¡œê·¸ì¸í•´ ì£¼ì„¸ìš”.
- IAM(Identity and Access Management) ì„¹ì…˜ìœ¼ë¡œ ì´ë™í•˜ê³  ìƒˆ ì‚¬ìš©ìë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.
- í•„ìš”í•œ S3 ì•¡ì„¸ìŠ¤ ì •ì±…ì„ ë¶€ì—¬í•˜ê³  ì•¡ì„¸ìŠ¤ í‚¤ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.
- í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  ì•¡ì„¸ìŠ¤ í‚¤ IDì™€ ë¹„ë°€ ì•¡ì„¸ìŠ¤ í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ì €ì¥í•´ ì£¼ì„¸ìš”.

<div class="content-ad"></div>

ì´ì œ ì‹¤ì œ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤! ì¤€ë¹„ëë‚˜ìš” ì—¬ëŸ¬ë¶„!!

![YouTube Trend Analysis Pipeline ETL with Airflow, Spark, S3, and Docker](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png)

ì´ ê¸€ì€ 4ê°€ì§€ ì£¼ìš” ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ìš”:

- ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜ ë° ì„¤ì •
- Youtube Data APIì—ì„œ ë°ì´í„° ì¶”ì¶œ
- PySparkë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë³€í™˜
- AWS S3ë¡œ ë°ì´í„° ë¡œë“œ

<div class="content-ad"></div>

# 1. ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜ ë° ì„¤ì •:

- VS Code â€” [VS Code ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜](https://code.visualstudio.com/).
- Docker Desktop â€” [Docker Desktop ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜](https://www.docker.com/products/docker-desktop).
- (ì„ íƒì‚¬í•­) Windows Subsystem for Linux (WSL) â€” ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì— ì‚¬ìš©ë˜ëŠ” Apache Airflow ë° PySparkì™€ ê°™ì€ ë§ì€ ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ Unix ê³„ì—´ ì‹œìŠ¤í…œì„ ìœ„í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë„êµ¬ë¥¼ Windowsì—ì„œ ì‚¬ìš©í•  ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” í˜¸í™˜ì„± ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ WSLì„ í†µí•´ ë„¤ì´í‹°ë¸Œ Linux í™˜ê²½ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ` ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ PowerShellì„ ì—½ë‹ˆë‹¤.
  - ` ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”: wsl --install.
  - ` ëª…ë ¹ì— ë”°ë¼ WSLì„ ì„¤ì¹˜í•˜ê³  Microsoft Storeì—ì„œ Linux ë°°í¬íŒ(ì˜ˆ: Ubuntu)ì„ ì„ íƒí•˜ì„¸ìš”.
  - ` Linux ë°°í¬íŒì— ì‚¬ìš©ì ì´ë¦„ ë° ì•”í˜¸ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

ì´ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” ë° WSLì´ ë°˜ë“œì‹œ í•„ìš”í•œ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. Docker Desktopì€ Windowsì—ì„œ ë„¤ì´í‹°ë¸Œë¡œ ì‹¤í–‰ë  ìˆ˜ ìˆìœ¼ë©° Docker ìì²´ê°€ ê´€ë¦¬í•˜ëŠ” ê°€ë²¼ìš´ Linux ê°€ìƒ ë¨¸ì‹ (VM)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Docker Desktopê³¼ í•¨ê»˜ WSLì„ ì‚¬ìš©í•˜ë©´ Windowsì—ì„œ ì§ì ‘ Linux ëª…ë ¹ ë° ì‘ì—…ì„ ì‹¤í–‰í•  ìˆ˜ ìˆì–´ ë³´ë‹¤ ë„¤ì´í‹°ë¸Œí•œ ê°œë°œ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.

ì´ì œ ì„¤ì •ì„ ì‹œì‘í•´ ë´…ì‹œë‹¤.

<div class="content-ad"></div>

ë¶€ë¶„ 1 â€” ë„ì»¤ ì´ë¯¸ì§€ ë§Œë“¤ê¸°

- í”„ë¡œì íŠ¸ìš© ìƒˆ í´ë”ë¥¼ ë§Œë“¤ê³  "Airflow-Project"ë¡œ ì´ë¦„ì„ ì§€ì–´ì£¼ì„¸ìš”.
- í•´ë‹¹ í´ë”ì—ì„œ ëª…ë ¹ í”„ë¡¬í”„íŠ¸ë¥¼ ì—½ë‹ˆë‹¤.
- ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ ì•„ë˜ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
code .
```

- ì´ ëª…ë ¹ì€ VS Codeì—ì„œ í•´ë‹¹ í´ë”ë¥¼ í”„ë¡œì íŠ¸ë¡œ ì—½ë‹ˆë‹¤.
- VS Codeì—ì„œ "dockerfile"ì´ë¼ëŠ” ìƒˆ íŒŒì¼ì„ ë§Œë“¤ê³  ì•„ë˜ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”:

<div class="content-ad"></div>

```js
FROM apache/airflow:latest

# ì‹œìŠ¤í…œ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ ë£¨íŠ¸ ì‚¬ìš©ìë¡œ ì „í™˜í•©ë‹ˆë‹¤
USER root

# git, OpenJDKë¥¼ ì„¤ì¹˜í•˜ê³  apt ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤
RUN apt-get update && \
    apt-get -y install git default-jdk && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ airflow ì‚¬ìš©ìë¡œ ì „í™˜í•©ë‹ˆë‹¤
USER airflow

# í•„ìš”í•œ Python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤
RUN pip install --no-cache-dir pyspark pandas google-api-python-client emoji boto3
```

ì´ Docker íŒŒì¼ì€ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ìš”.

- íŒŒì¼ì„ ë§ˆìš°ìŠ¤ ì˜¤ë¥¸ìª½ ë²„íŠ¼ìœ¼ë¡œ í´ë¦­í•˜ê³  VS Codeì—ì„œ "ì´ë¯¸ì§€ ë¹Œë“œ" ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”. ì´ë¦„ì„ ì…ë ¥í•˜ë¼ëŠ” í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ "airflow-project"ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì´ ëª…ë ¹ì€ Docker ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ docker-compose.yml íŒŒì¼ì„ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ë„ë¡ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

(ì¬ë¯¸ìˆëŠ” ì‚¬ì‹¤: íŒŒì¼ì—ì„œ Python ì„¤ì¹˜ê°€ ì—†ëŠ” ì´ìœ  ê¶ê¸ˆí•˜ì‹ ê°€ìš”? ì‹¤ì œë¡œ Dockerfileì—ì„œ ì‚¬ìš©ëœ ê¸°ë³¸ ì´ë¯¸ì§€ì¸ apache/airflow:latestì—ëŠ” Pythonì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆì–´ìš”. ì™œëƒí•˜ë©´ Airflow ìì²´ê°€ Pythonìœ¼ë¡œ ì‘ì„±ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì£¼ë¡œ ì›Œí¬í”Œë¡œ ë° ì‘ì—… ì •ì˜ì— Pythonì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Dockerfileì—ì„œ ë³„ë„ë¡œ Pythonì„ ì„¤ì¹˜í•  í•„ìš”ê°€ ì—†ë‹µë‹ˆë‹¤!)

<div class="content-ad"></div>

íŒŒíŠ¸ 2 â€” ë„ì»¤ ì»´í¬ì¦ˆ íŒŒì¼ ìƒì„±í•˜ê¸°

ë„ì»¤ ì»´í¬ì¦ˆë¥¼ ì‚¬ìš©í•˜ë©´ ë©€í‹° ì»¨í…Œì´ë„ˆ ë„ì»¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¨ì¼ ëª…ë ¹ìœ¼ë¡œ ì—¬ëŸ¬ ë„ì»¤ ì»¨í…Œì´ë„ˆë¥¼ ì •ì˜í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©° ê° ì„œë¹„ìŠ¤ì˜ í™˜ê²½ ë³€ìˆ˜, ë³¼ë¥¨, í¬íŠ¸ ë° ê¸°íƒ€ ì„¤ì •ì„ ëª…í™•í•˜ê³  ì¡°ì§ì ì¸ ë°©ì‹ìœ¼ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„ì»¤ ì»´í¬ì¦ˆë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¨ì¼ ëª…ë ¹ì–´ì¸ docker-compose up ë˜ëŠ” docker-compose downì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ë¥¼ ì‰½ê²Œ ì‹œì‘, ì¤‘ì§€ ë° ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- "docker-compose.yml" íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ì½”ë“œë¥¼ íŒŒì¼ì— ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤:

```js
version: '3'
services:

  airflowproject:
    image: airflow-project:latest
    environment:
      - AWS_ACCESS_KEY_ID=your-aws-access-key
      - AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key
      - YOUTUBE_API_KEY=your-youtube-api-key
    volumes:
      - ./airflow:/opt/airflow
    ports:
      - "8080:8080"
    command: airflow standalone
```

<div class="content-ad"></div>

- ì´ì œ íŒŒì¼ì„ ë§ˆìš°ìŠ¤ ì˜¤ë¥¸ìª½ ë²„íŠ¼ìœ¼ë¡œ í´ë¦­í•œ í›„ VS Codeì—ì„œ 'Compose Up' ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”. í™˜ê²½ì„ ì„¤ì •í•˜ê¸° ìœ„í•´ í´ë¦­í•˜ì„¸ìš”.
- ê¹œì§ ë†€ë„ ì¼ì´ ë²Œì–´ì¡Œì–´ìš”! ì´ ì‘ì—…ì„ ì™„ë£Œí•œ í›„ì—ëŠ” VS Code í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì— "airflow"ë¼ëŠ” ìƒˆ í´ë”ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Docker ë°ìŠ¤í¬í†±ì„ ì—´ì–´ì„œ ëª¨ë“  ê²ƒì´ ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œëœ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ í™”ë©´ì´ í‘œì‹œë©ë‹ˆë‹¤.

![ì´ë¯¸ì§€](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_1.png)

- ì´ì œ Airflow í”„ë¡œì íŠ¸ë¥¼ í´ë¦­í•˜ì—¬ Airflowê°€ 8080 í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì„ì„ ë‚˜íƒ€ë‚´ëŠ” ë¡œê·¸ê°€ í‘œì‹œë˜ëŠ” í™”ë©´ì„ ì—½ë‹ˆë‹¤.

<div class="content-ad"></div>


![ì´ë¯¸ì§€](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_2.png)

- í¬íŠ¸ë¥¼ í´ë¦­í•˜ë©´ Airflow ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤. ì´ ë§í¬ë¥¼ ì²˜ìŒ ì—´ì–´ë³´ëŠ” ê²½ìš° ìê²© ì¦ëª…ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
- ì‚¬ìš©ì ì´ë¦„ì€ "admin"ì´ê³  ë¹„ë°€ë²ˆí˜¸ëŠ” compose up ëª…ë ¹ì„ ì‹¤í–‰í•œ í›„ ìƒì„±ëœ Airflow í´ë” ë‚´ì˜ "standalone_admin_password.txt" íŒŒì¼ì— ìˆìŠµë‹ˆë‹¤.

![ì´ë¯¸ì§€](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_3.png)

- ë¡œê·¸ì¸ í˜ì´ì§€ì—ì„œ ìê²© ì¦ëª…ì„ ì…ë ¥í•œ í›„, ë¡œì»¬ í˜¸ìŠ¤íŠ¸ì—ì„œ Airflowê°€ ì‹¤í–‰ ì¤‘ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤:


<div class="content-ad"></div>


<img src="/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_4.png" />

ë‹¹ì‹ ì˜ í™˜ê²½ ì„¤ì • ì™„ë£Œì…ë‹ˆë‹¤! íœ´â€•!!

# 2. YouTube ë°ì´í„° APIì—ì„œ ë°ì´í„° ì¶”ì¶œí•˜ê¸°:

<div class="content-ad"></div>

- Airflow í´ë” ì•„ë˜ì— "dags"ë¼ëŠ” ì´ë¦„ì˜ í´ë”ë¥¼ ë§Œë“¤ê³ , dags í´ë” ì•„ë˜ì— "youtube_etl_dag.py"ë¼ëŠ” íŒŒì´ì¬ íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.
- ì´ì œ "youtube_etl_dag.py" íŒŒì¼ì— ë‹¤ìŒì„ importí•˜ì„¸ìš”.

```js
import logging
import os
import re
import shutil
from datetime import datetime, timedelta

import boto3
import emoji
import pandas as pd
from googleapiclient.discovery import build
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, udf
from pyspark.sql.types import (DateType, IntegerType, LongType, StringType,
                               StructField, StructType)

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
```

- ì´ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” ë° ìœ„ì˜ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤(ì½”ë“œ ì‘ì„±ì„ ì‹œì‘í•˜ë©´ ëª¨ë‘ ìœ ìš©í•´ì§‘ë‹ˆë‹¤)
- VS Codeì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë“  ì¢…ì†ì„±ì´ ë„ì»¤ì— ì„¤ì¹˜ë˜ì–´ ìˆì§€ë§Œ ë¡œì»¬ ë¨¸ì‹ ì—ëŠ” ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì´ë¯€ë¡œ ì‹ ê²½ ì“°ì§€ ë§ˆì‹­ì‹œì˜¤.
- Airflowì—ì„œ êµ¬ë¬¸ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ í™”ë©´ ìƒë‹¨ì— í‘œì‹œë˜ê³ , ë…¼ë¦¬ ì˜¤ë¥˜/ì˜ˆì™¸ëŠ” Airflow ë¡œê·¸ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
# DAGì™€ ê¸°ë³¸ ì¸ìˆ˜ ì •ì˜
default_args = {
    'owner': 'airflow',  # DAG ì†Œìœ ì
    'depends_on_past': False,  # ê³¼ê±° DAG ì‹¤í–‰ì— ì¢…ì†í•˜ëŠ”ì§€ ì—¬ë¶€
    'email_on_failure': False,  # ì‹¤íŒ¨ ì‹œ ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    'email_on_retry': False,  # ì¬ì‹œë„ ì‹œ ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    'retries': 1,  # ì¬ì‹œë„ íšŸìˆ˜
    'retry_delay': timedelta(minutes=5),  # ì¬ì‹œë„ ê°„ì˜ ì§€ì—° ì‹œê°„
     'start_date': datetime(2023, 6, 10, 0, 0, 0),  # ë§¤ì¼ ìì •(00:00) UTCì— ì‹¤í–‰
}

dag = DAG(
    'youtube_etl_dag',  # DAG ì‹ë³„ì
    default_args=default_args,  # ê¸°ë³¸ ì¸ìˆ˜ í• ë‹¹
    description='ê°„ë‹¨í•œ ETL DAG',  # DAG ì„¤ëª…
    schedule_interval=timedelta(days=1),  # ì¼ë³„ ìŠ¤ì¼€ì¤„ ê°„ê²©
    catchup=False,  # ëˆ„ë½ëœ DAG ì‹¤í–‰ì„ ë³µêµ¬í•˜ì§€ ì•ŠìŒ
)
```  

<div class="content-ad"></div>

ë§¤ì¼ ìì •(0ì‹œ)ì— ì‹¤í–‰ë˜ëŠ” DAGì¸ 'youtube_etl_dag'ì„ ì •ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ DAGì€ Airflowì—ì„œ ê´€ë¦¬ ë° íŠ¸ë¦¬ê±°ë˜ë©°, VS Codeì—ì„œ ë³„ë„ë¡œ ì‹¤í–‰í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. Python íŒŒì¼ì„ ì—…ë°ì´íŠ¸í•˜ë©´ Airflowì—ì„œ ìë™ìœ¼ë¡œ ë³€ê²½ ì‚¬í•­ì„ ê°ì§€í•˜ê³  ë°˜ì˜í•  ê²ƒì…ë‹ˆë‹¤.

í˜„ì¬ Airflowì—ëŠ” DAGì´ í‘œì‹œë˜ì§€ë§Œ ì•„ì§ ì •ì˜ëœ ì‘ì—…ì´ ì—†ì–´ì„œ ì–´ë–¤ ì‘ì—…ë„ í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. DAGë¥¼ ê¸°ëŠ¥ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ ë°ì´í„° ì¶”ì¶œ ì‘ì—…ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤.

```js
# YouTube APIì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ Python callable í•¨ìˆ˜
def extract_data(**kwargs):
    api_key = kwargs['api_key']
    region_codes = kwargs['region_codes']
    category_ids = kwargs['category_ids']
    
    df_trending_videos = fetch_data(api_key, region_codes, category_ids)
    current_date = datetime.now().strftime("%Y%m%d")
    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'
    # DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥
    df_trending_videos.to_csv(output_path, index=False)

def fetch_data(api_key, region_codes, category_ids):
    """
    YouTube APIì—ì„œ ì—¬ëŸ¬ ë‚˜ë¼ì™€ ì¹´í…Œê³ ë¦¬ì˜ ì¸ê¸° ë™ì˜ìƒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    # ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    video_data = []

    # YouTube API ì„œë¹„ìŠ¤ ë¹Œë“œ
    youtube = build('youtube', 'v3', developerKey=api_key)

    for region_code in region_codes:
        for category_id in category_ids:
            # ê° ì§€ì—­ ë° ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ next_page_tokenì„ Noneìœ¼ë¡œ ì´ˆê¸°í™”
            next_page_token = None
            while True:
                # ì¸ê¸° ë™ì˜ìƒì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ YouTube APIì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
                request = youtube.videos().list(
                    part='snippet,contentDetails,statistics',
                    chart='mostPopular',
                    regionCode=region_code,
                    videoCategoryId=category_id,
                    maxResults=50,
                    pageToken=next_page_token
                )
                response = request.execute()
                videos = response['items']

                # ê° ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ê³  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
                for video in videos:
                    video_info = {
                        'region_code': region_code,
                        'category_id': category_id,
                        'video_id': video['id'],
                        'title': video['snippet']['title'],
                        'published_at': video['snippet']['publishedAt'],
                        'view_count': int(video['statistics'].get('viewCount', 0)),
                        'like_count': int(video['statistics'].get('likeCount', 0)),
                        'comment_count': int(video['statistics'].get('commentCount', 0)),
                        'channel_title': video['snippet']['channelTitle']
                    }
                    video_data.append(video_info)

                # ê²°ê³¼ì˜ ë” ë§ì€ í˜ì´ì§€ê°€ ìˆëŠ” ê²½ìš° ë‹¤ìŒ í˜ì´ì§€ í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break

    return pd.DataFrame(video_data)

# DAGë¥¼ ìœ„í•œ ë°ì´í„° ì¶”ì¶œ ì‘ì—… ì •ì˜
extract_task = PythonOperator(
    task_id='extract_data_from_youtube_api',
    python_callable=extract_data,
    op_kwargs={
        'api_key': os.getenv('YOUTUBE_API_KEY'),
        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],
        'category_ids': ['1', '2', '10', '15', '20', '22', '23']
    },
    dag=dag,
)

extract_task # ì´ ì‘ì—…ì„ ì‹¤í–‰í•˜ë„ë¡ DAGë¥¼ ì„¤ì •í•¨
```

ì´ ì½”ë“œì—ì„œ ë‘ ê°€ì§€ ì£¼ìš” ì‘ì—…ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤:

<div class="content-ad"></div>

- DAGì— extract_taskë¼ëŠ” ì‘ì—…ì„ ë§Œë“¤ê³  ìˆìŠµë‹ˆë‹¤.
- extract_taskì—ì„œ í˜¸ì¶œë˜ëŠ” callable í•¨ìˆ˜ì¸ extract_dataë¥¼ ì •ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” YouTube Data APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ "Youtube_Trending_Data_Raw"ë¡œ ì‹œì‘í•˜ëŠ” CSV íŒŒì¼ì— pandas DataFrameì„ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

YouTube Data API ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ APIì˜ ë‹¤ë¥¸ ë¶€ë¶„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì— ëŒ€í•´ ìì„¸íˆ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” íŠ¸ë Œë”© ë¹„ë””ì˜¤ ë°ì´í„°ì— ê´€ì‹¬ì´ ìˆìœ¼ë¯€ë¡œ APIì˜ í•´ë‹¹ ë¶€ë¶„ì— ì§‘ì¤‘í•  ê²ƒì…ë‹ˆë‹¤. next_page_tokenì€ ëª¨ë“  í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

ì½”ë“œë¥¼ ìˆ˜ì •í•œ í›„ Airflow í˜ì´ì§€ì— ë³€ê²½ ì‚¬í•­ì´ ë°˜ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. DAGë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ ì™¼ìª½ ìƒë‹¨ì— ìˆëŠ” ì‹¤í–‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ê·¸ë˜í”„ì—ì„œ ì‘ì—… ìƒíƒœ (ëŒ€ê¸°, ì‹¤í–‰ ì¤‘, ì„±ê³µ ë“±)ëŠ” ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. DAGê°€ ì‹¤í–‰ ì¤‘ì¼ ë•Œ ë¡œê·¸ë¥¼ ë³´ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

<img src="/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_5.png" />

<div class="content-ad"></div>

ëŸ° ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  íŒŒì¼ì— ì €ì¥í•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. ì‘ì—…ì˜ ê° ë‹¨ê³„ì—ì„œ ê·¸ë˜í”„ ìƒ‰ìƒì´ ë³€ê²½ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì„ ê±°ì—ìš”. ë©‹ì§€ì£ ? :)

ì‘ì—… ìƒíƒœê°€ ì„±ê³µì„ ë‚˜íƒ€ë‚´ëŠ” ë…¹ìƒ‰ìœ¼ë¡œ ë³€í•˜ë©´, ìƒˆ íŒŒì¼ì¸ "Youtube-Trending-Data-Raw"ê°€ ìƒê¸´ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.

ìš°ë¦¬ì˜ Raw ë°ì´í„°ëŠ” ì´ë ‡ê²Œ ìƒê²¼ì–´ìš”:

![ì´ë¯¸ì§€](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_6.png)

<div class="content-ad"></div>

ì´ì œ ì¶”ì¶œ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‘ì—…ìœ¼ë¡œ ë„˜ì–´ê°€ ë´…ì‹œë‹¤!

## 3. PySparkë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë³€í™˜í•˜ê¸°:

ì›ì‹œ ë°ì´í„° íŒŒì¼ì„ ì‚´í´ë³´ë©´ ë°ì´í„°ì— ë§ì€ í•´ì‹œíƒœê·¸ì™€ ì´ëª¨ì§€ê°€ ìˆëŠ”ë°, ì´ëŠ” ìš°ë¦¬ í”„ë¡œì íŠ¸ì—ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ì •ë¦¬í•˜ì—¬ ì¶”ê°€ ë¶„ì„ì— ìœ ìš©í•˜ë„ë¡ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.

ì´ ì‘ì—…ì— PySparkë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. PySparkëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ë³€í™˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ê°€ íŠ¹íˆ í¬ì§€ ì•Šê¸° ë•Œë¬¸ì— Pandasë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì „ì— PySparkë¥¼ ì‚¬ìš©í•œ ì ì´ ìˆì–´ ì´ë²ˆì—ë„ PySparkë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤. ìµœê·¼ PySparkë¥¼ ê³µë¶€í•˜ê³  ìˆìœ¼ë©°, ì´ë¡ ì„ ê³µë¶€í•˜ëŠ” ê²ƒë³´ë‹¤ ì‹¤ì œ êµ¬í˜„ì´ ë” í¥ë¯¸ë¡­ë‹¤ê³  ëŠë‚ë‹ˆë‹¤.

<div class="content-ad"></div>


# Python callable function to extract data from YouTube API
def extract_data(**kwargs):
    api_key = kwargs['api_key']
    region_codes = kwargs['region_codes']
    category_ids = kwargs['category_ids']
    
    df_trending_videos = fetch_data(api_key, region_codes, category_ids)
    current_date = datetime.now().strftime("%Y%m%d")
    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'
    # Save DataFrame to CSV file
    df_trending_videos.to_csv(output_path, index=False)

def fetch_data(api_key, region_codes, category_ids):
    """
    Fetches trending video data for multiple countries and categories from YouTube API.
    Returns a pandas data frame containing video data.
    """
    video_data = []

    # Build YouTube API service
    youtube = build('youtube', 'v3', developerKey=api_key)

    for region_code in region_codes:
        for category_id in category_ids:
            # Initialize the next_page_token to None for each region and category
            next_page_token = None
            while True:
                # Make a request to the YouTube API to fetch trending videos
                request = youtube.videos().list(
                    part='snippet,contentDetails,statistics',
                    chart='mostPopular',
                    regionCode=region_code,
                    videoCategoryId=category_id,
                    maxResults=50,
                    pageToken=next_page_token
                )
                response = request.execute()
                videos = response['items']

                # Process each video and collect data
                for video in videos:
                    video_info = {
                        'region_code': region_code,
                        'category_id': category_id,
                        'video_id': video['id'],
                        'title': video['snippet']['title'],
                        'published_at': video['snippet']['publishedAt'],
                        'view_count': video['statistics'].get('viewCount', 0),
                        'like_count': video['statistics'].get('likeCount', 0),
                        'comment_count': video['statistics'].get('commentCount', 0),
                        'channel_title': video['snippet']['channelTitle']
                    }
                    video_data.append(video_info)

                # Get the next page token, if there are more pages of results
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break

    return pd.DataFrame(video_data)

def preprocess_data_pyspark_job():
    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()
    current_date = datetime.now().strftime("%Y%m%d")
    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'
    df = spark.read.csv(output_path, header=True)
    
    # Define UDF to remove hashtag data, emojis
    def clean_text(text):
     if text is not None:
        # Remove emojis
        text = emoji.demojize(text, delimiters=('', ''))
        
        # Remove hashtag data
        if text.startswith('#'):
            text = text.replace('#', '').strip()
        else:
            split_text = text.split('#')
            text = split_text[0].strip()
        
        # Remove extra double quotes and backslashes
        text = text.replace('\\"', '')  # Remove escaped quotes
        text = re.sub(r'\"+', '', text)  # Remove remaining double quotes
        text = text.replace('\\', '')  # Remove backslashes
        
        return text.strip()  # Strip any leading or trailing whitespace

     return text
    # Register UDF
    clean_text_udf = udf(clean_text, StringType())

    # Clean the data
    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \
                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \
                   .withColumn('published_at', to_date(col('published_at'))) \
                   .withColumn('view_count', col('view_count').cast(LongType())) \
                   .withColumn('like_count', col('like_count').cast(LongType())) \
                   .withColumn('comment_count', col('comment_count').cast(LongType())) \
                   .dropna(subset=['video_id'])
    
    # Generate the filename based on the current date
    current_date = datetime.now().strftime("%Y%m%d")
    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'
    
    # Write cleaned DataFrame to the specified path
    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   


# Define extract task for the DAG
extract_task = PythonOperator(
    task_id='extract_data_from_youtube_api',
    python_callable=extract_data,
    op_kwargs={
        'api_key': os.getenv('YOUTUBE_API_KEY'),
        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],
        'category_ids': ['1', '2', '10', '15', '20', '22', '23']
    },
    dag=dag,
)

# Define preprocessing task for the DAG
preprocess_data_pyspark_task= PythonOperator(
    task_id='preprocess_data_pyspark_task',
    python_callable=preprocess_data_pyspark_job,
    dag=dag
)

extract_task >> preprocess_data_pyspark_task


ì—¬ê¸°ì„œëŠ” ì´ ì½”ë“œê°€ í•˜ëŠ” ì¼ì„ ì„¤ëª…í•´ ë“œë ¸ìŠµë‹ˆë‹¤.

- "preprocess_data_pyspark_task"ë¼ëŠ” ì‘ì—…ì„ ë§Œë“­ë‹ˆë‹¤.
- ì´ ì‘ì—…ì€ preprocess_data_pyspark_job í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
- preprocess_data_pyspark_job í•¨ìˆ˜ëŠ” ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
- ê·¸ë¦¬ê³  ì •ë¦¬ëœ ë°ì´í„°ëŠ” "Transformed_Youtube_Data_currentDate"ë¼ëŠ” í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.
- ì´ í´ë” ì•ˆì—ëŠ” ì •ë¦¬ëœ ë°ì´í„°ê°€ ë‹´ê¸´ "part-" ì ‘ë‘ì‚¬ê°€ ë¶™ì€ ìƒˆ CSV íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

ë§Œì•½ Airflowë¥¼ ë³´ì‹ ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì²« ë²ˆì§¸ ì‘ì—…ì— ìƒˆë¡œìš´ ì‘ì—…ì´ ì¶”ê°€ëœ ê²ƒì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

<div class="content-ad"></div>

ì•„ë˜ëŠ” ìš°ë¦¬ê°€ ë³€í™˜í•œ ë°ì´í„°ì˜ ëª¨ìŠµì…ë‹ˆë‹¤:

![Transformed Data](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_8.png)

ì´ ì‘ì—…ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ìµœì¢… ì‘ì—…ìœ¼ë¡œ ë„˜ì–´ê°ˆ ì°¨ë¡€ì…ë‹ˆë‹¤.

<div class="content-ad"></div>

# 4. S3ë¡œ ë°ì´í„° ë¡œë“œí•˜ê¸°:

ì´ ì‘ì—…ì„ ì‹œì‘í•˜ê¸° ì „ì— ì²˜ìŒì— ì„¤ì •í•œ IAM ì‚¬ìš©ìë¥¼ ì‚¬ìš©í•˜ì—¬ S3 ë²„í‚·ì„ ìƒì„±í•˜ê³  ë²„í‚· ì´ë¦„ì„ ë©”ëª¨í•´ì£¼ì„¸ìš”.

ìš°ë¦¬ì˜ ìµœì¢… ì½”ë“œì…ë‹ˆë‹¤!

```js
import logging
import os
import re
import shutil
from datetime import datetime, timedelta

import boto3
import emoji
import pandas as pd
from googleapiclient.discovery import build
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, udf
from pyspark.sql.types import (DateType, IntegerType, LongType, StringType,
                               StructField, StructType)

from airflow import DAG
from airflow.operators.python_operator import PythonOperator

# DAG ë° ê¸°ë³¸ ì¸ì ì •ì˜
default_args = {
    'owner': 'airflow',  # DAG ì†Œìœ ì
    'depends_on_past': False,  # ì´ì „ DAG ì‹¤í–‰ì— ì˜ì¡´ ì—¬ë¶€
    'email_on_failure': False,  # ì‹¤íŒ¨ ì‹œ ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    'email_on_retry': False,  # ì¬ì‹œë„ ì‹œ ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    'retries': 1,  # ì¬ì‹œë„ íšŸìˆ˜
    'retry_delay': timedelta(minutes=5),  # ì¬ì‹œë„ ì‚¬ì´ ê°„ê²©
    'start_date': datetime(2023, 6, 10, 0, 0, 0),  # ë§¤ì¼ ìì •(00:00) UTCì— ì‹¤í–‰
}

# DAG ì •ì˜
dag = DAG(
    'youtube_etl_dag',  # DAG ì‹ë³„ì
    default_args=default_args,  # ê¸°ë³¸ ì¸ìˆ˜ í• ë‹¹
    description='ê°„ë‹¨í•œ ETL DAG',  # DAG ì„¤ëª…
    schedule_interval=timedelta(days=1),  # ìŠ¤ì¼€ì¤„ ê°„ê²©: ë§¤ì¼
    catchup=False,  # ëˆ„ë½ëœ DAG ì‹¤í–‰ì„ ë³µêµ¬í•˜ì§€ ì•ŠìŒ
)

# YouTube APIì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” Python ìœ í˜•ì˜ í•¨ìˆ˜
def extract_data(**kwargs):
    api_key = kwargs['api_key']
    region_codes = kwargs['region_codes']
    category_ids = kwargs['category_ids']
    
    df_trending_videos = fetch_data(api_key, region_codes, category_ids)
    current_date = datetime.now().strftime("%Y%m%d")
    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'
    # DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥
    df_trending_videos.to_csv(output_path, index=False)

# YouTube APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def fetch_data(api_key, region_codes, category_ids):
    """
    YouTube APIì—ì„œ ì—¬ëŸ¬ êµ­ê°€ ë° ì¹´í…Œê³ ë¦¬ì˜ íŠ¸ë Œë“œ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ë¹„ë””ì˜¤ ë°ì´í„°ê°€ í¬í•¨ëœ Pandas ë°ì´í„° í”„ë ˆì„ ë°˜í™˜.
    """
    # ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ë³´ê´€í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    video_data = []

    # YouTube API ì„œë¹„ìŠ¤ ë¹Œë“œ
    youtube = build('youtube', 'v3', developerKey=api_key)

    for region_code in region_codes:
        for category_id in category_ids:
            # ê° ì§€ì—­ ë° ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ next_page_tokenì„ Noneìœ¼ë¡œ ì´ˆê¸°í™”
            next_page_token = None
            while True:
                # YouTube APIì— íŠ¸ë Œë“œ ë¹„ë””ì˜¤ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ìš”ì²­
                request = youtube.videos().list(
                    part='snippet,contentDetails,statistics',
                    chart='mostPopular',
                    regionCode=region_code,
                    videoCategoryId=category_id,
                    maxResults=50,
                    pageToken=next_page_token
                )
                response = request.execute()
                videos = response['items']

                # ê° ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ë°ì´í„° ìˆ˜ì§‘
                for video in videos:
                    video_info = {
                        'region_code': region_code,
                        'category_id': category_id,
                        'video_id': video['id'],
                        'title': video['snippet']['title'],
                        'published_at': video['snippet']['publishedAt'],
                        'view_count': video['statistics'].get('viewCount', 0),
                        'like_count': video['statistics'].get('likeCount', 0),
                        'comment_count': video['statistics'].get('commentCount', 0),
                        'channel_title': video['snippet']['channelTitle']
                    }
                    video_data.append(video_info)

                # ê²°ê³¼ì˜ ì¶”ê°€ í˜ì´ì§€ê°€ ìˆëŠ” ê²½ìš° ë‹¤ìŒ í˜ì´ì§€ í† í° ê°€ì ¸ì˜¤ê¸°
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break

    return pd.DataFrame(video_data)

# PySpark ì‘ì—… ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_data_pyspark_job():
    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()
    current_date = datetime.now().strftime("%Y%m%d")
    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'
    df = spark.read.csv(output_path, header=True)
    
    # í•´ì‹œíƒœê·¸ ë°ì´í„°, ì´ëª¨ì§€ ì œê±°ë¥¼ ìœ„í•œ UDF ì •ì˜
    def clean_text(text):
     if text is not None:
        # ì´ëª¨ì§€ ì œê±°
        text = emoji.demojize(text, delimiters=('', ''))
        
        # í•´ì‹œíƒœê·¸ ë° ì´í›„ ëª¨ë“  ê²ƒ ì œê±°
        if text.startswith('#'):
            text = text.replace('#', '').strip()
        else:
            split_text = text.split('#')
            text = split_text[0].strip()
        
        # ì¶”ê°€ ì´ì¤‘ ì¸ìš©ë¶€í˜¸ì™€ ë°±ìŠ¬ë˜ì‹œ ì œê±°
        text = text.replace('\\"', '')  # ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ ì œê±°
        text = re.sub(r'\"+', '', text)  # ë‚¨ì€ ì´ì¤‘ ì¸ìš©ë¶€í˜¸ ì œê±°
        text = text.replace('\\', '')  # ë°±ìŠ¬ë˜ì‹œ ì œê±°
        
        return text.strip()  # ì„ í–‰ ë˜ëŠ” í›„í–‰ ê³µë°± ì œê±°

     return text
    # UDF ë“±ë¡
    clean_text_udf = udf(clean_text, StringType())

    # ë°ì´í„° ì •ë¦¬
    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \
                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \
                   .withColumn('published_at', to_date(col('published_at'))) \
                   .withColumn('view_count', col('view_count').cast(LongType())) \
                   .withColumn('like_count', col('like_count').cast(LongType())) \
                   .withColumn('comment_count', col('comment_count').cast(LongType())) \
                   .dropna(subset=['video_id'])
    
    # í˜„ì¬ ë‚ ì§œë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ ì´ë¦„ ìƒì„±
    current_date = datetime.now().strftime("%Y%m%d")
    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'
    
    # ì •ë¦¬ëœ DataFrameì„ ì§€ì •ëœ ê²½ë¡œì— ì‘ì„±
    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   

# S3ë¡œ ë°ì´í„° ì—…ë¡œë“œ í•¨ìˆ˜
def load_data_to_s3(**kwargs):
    bucket_name = kwargs['bucket_name']
    today = datetime.now().strftime('%Y/%m/%d')
    prefix = f"processed-data/{today}"
    current_date = datetime.now().strftime("%Y%m%d")
    local_dir_path  = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'
    upload_to_s3(bucket_name, prefix, local_dir_path)

def upload_to_s3(bucket_name, prefix, local_dir_path):
    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')

    s3_client = boto3.client(
        's3',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key
    )

    for root, dirs, files in os.walk(local_dir_path):
         for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                s3_key = f"{prefix}/{file}"
                logging.info(f"Uploading {file_path} to s3://{bucket_name}/{s3_key}")
                s3_client.upload_file(file_path, bucket_name, s3_key)

# DAGì˜ ì¶”ì¶œ ì‘ì—… ì •ì˜
extract_task = PythonOperator(
    task_id='extract_data_from_youtube_api',
    python_callable=extract_data,
    op_kwargs={
        'api_key': os.getenv('YOUTUBE_API_KEY'),
        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],
        'category_ids': ['1', '2', '10', '15', '20', '22', '23']
    },
    dag=dag,
)

# DAGì˜ ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—… ì •ì˜
preprocess_data_pyspark_task= PythonOperator(
    task_id='preprocess_data_pyspark_task',
    python_callable=preprocess_data_pyspark_job,
    dag=dag
)



<div class="content-ad"></div>

ì´ì œ ì €í¬ê°€ ë§Œë“  ìµœì¢… ì‘ì—…ì¸ load_data_to_s3_taskë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ load_data_to_s3 í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ íŒŒì¼ì„ S3 ë²„í‚·ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. ì—…ë¡œë“œê°€ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ S3 ë²„í‚·ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.

ë§ˆì¹¨ë‚´ ìš°ë¦¬ì˜ AirflowëŠ” ì´ë ‡ê²Œ ìƒê²¼ìŠµë‹ˆë‹¤!

![Airflow](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_9.png)

ì´ì œ ì´ ë°ì´í„°ë¥¼ Tableauë‚˜ ë‹¤ë¥¸ BI ë„êµ¬ì— ì—°ê²°í•˜ì—¬ í¥ë¯¸ë¡œìš´ ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“¤ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì‹œê°í™”í•´ ë³´ì„¸ìš”!

<div class="content-ad"></div>

í•¨ê»˜ ì´ íŒŒì´í”„ë¼ì¸ì„ ë”°ë¼ ì˜¤ë©´ì„œ ìƒˆë¡œìš´ ê¸°ìˆ  ëª‡ ê°€ì§€ë¥¼ ë°°ì› ìœ¼ë©´ ì¢‹ê² ì–´ìš”! ğŸš€ ì„±ê³µì ìœ¼ë¡œ ì—¬ê¸°ê¹Œì§€ ì™”ë‹¤ë©´ ì¶•í•˜í•´ìš”! ğŸ‰ ì´ ìƒˆë¡­ê²Œ ì–»ì€ ì§€ì‹ì´ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì—ì„œì˜ í–¥í›„ ëª¨í—˜ì— í° ë„ì›€ì´ ë˜ê¸¸ ë°”ë˜ìš”!

ì´ í”„ë¡œì íŠ¸ì˜ Github ì €ì¥ì†Œë¥¼ ì²¨ë¶€í•©ë‹ˆë‹¤:

ë§Œì•½ ì´ ê¸€ì„ ì¢‹ì•„í•˜ì…¨ë‹¤ë©´, ê³µìœ í•˜ê³ , ì¢‹ì•„ìš”ë¥¼ ëˆŒëŸ¬ì£¼ì‹œê³ , ì•„ë˜ì— ëŒ“ê¸€ì„ ë‚¨ê²¨ì£¼ì‹œê³  êµ¬ë…í•´ì£¼ì„¸ìš”. ğŸ‰ğŸ‘ğŸ“

ì»¤íŠ¼ì„ ë‹«ìŠµë‹ˆë‹¤! ğŸ­