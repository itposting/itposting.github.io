---
title: "Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM"
description: ""
coverImage: "/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png"
date: 2024-06-19 01:21
ogImage: 
  url: /assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png
tag: Tech
originalTitle: "Llama-3 inferences on IntelÂ® Coreâ„¢ Ultra 5: DirectML and ONNX vs. IPEX-LLM"
link: "https://medium.com/@GenerationAI/llama-3-inferences-on-intel-core-ultra-5-directml-and-onnx-vs-ipex-llm-418e7220817d"
---


ì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Intelì€ ONNX + DirectMLì„ ìœ„í•œ í•˜ë“œì›¨ì–´ ê°€ì†í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ëª‡ ê°€ì§€ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

Microsoftì€ PyTorchë¥¼ ìœ„í•œ DirectML ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ 16ë¹„íŠ¸ì™€ 32ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì ì—ì„œë§Œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆì œì—ì„œ ì´ˆë‹¹ í† í° ìˆ˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í¬í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

```js
conda create --name pytdml python=3.10 -y
conda activate pytdml
pip install torch-directml
git clone https://github.com/luweigen/DirectML
cd DirectML/PyTorch/llm
pip install -r requirements.txt
python app.py --precision float16 --model_repo "meta-llama/Meta-Llama-3-8B-Instruct" --stream_every_n=143
```

<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png" />

<div class="content-ad"></div>

<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png" />

ì´ì „ ì‹¤í—˜ì—ì„œ ğŸ¤—Transformers + IPEX-LLMì´ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— ì´ ì„¤ì •ì—ì„œëŠ” 16ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì  ì¶”ë¡ ë§Œ ë¹„êµí•  ê²ƒì…ë‹ˆë‹¤.

í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ipex-llm-llama3.pyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

```js
# Wei Lu(mailwlu@gmail.com)ì— ì˜í•´ ìˆ˜ì •ë¨
# 2016ë…„ The BigDL Authorsì— ì €ì‘ê¶Œ ì†í•¨
#
# Apache ë¼ì´ì„ ìŠ¤, ë²„ì „ 2.0ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ ë¶€ì—¬
# ì´ íŒŒì¼ì„ ë¼ì´ì„ ìŠ¤ì™€ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
# ë¼ì´ì„ ìŠ¤ ì‚¬ë³¸ì€ ë‹¤ìŒì—ì„œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# ì ìš©ë˜ëŠ” ë²•ë¥ ì— ë”°ë¼ í•„ìš”í•˜ê±°ë‚˜ ì„œë©´ìœ¼ë¡œ í•©ì˜ë˜ê±°ë‚˜, ì†Œí”„íŠ¸ì›¨ì–´ê°€
# "ìˆëŠ” ê·¸ëŒ€ë¡œ" ë°°í¬ë©ë‹ˆë‹¤. ì¡°ê±´ì´ë‚˜ ë³´ì¦ ì—†ì´
# ëª…ì‹œ ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ, ê¹Œì§€ë„ ì–´ë–¤ ì¢…ë¥˜ì˜ ì¡°ê±´ë„ ë³´ì¦ ì—†ì´,
# ëª…ì‹œì  ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ. ì–¸ì–´ íŠ¹ì • ê¶Œí•œê³¼ ê´€ë ¨í•´ì•¼ í•©ë‹ˆë‹¤.
# ê¶Œí•œ ë° ì œí•œ ì‚¬í•­
#

import torch
import time
import argparse

from ipex_llm.transformers import AutoModelForCausalLM
from transformers import AutoTokenizer

# ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸ ì¡°ì •ì€ ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3
ê¸°ë³¸_ì‹œìŠ¤í…œ_í”„ë¡¬í”„íŠ¸ = """\
"""

def get_prompt(user_input: str, chat_history: list[tuple[str, str]],
               system_prompt: str) -> str:
    prompt_texts = [f'<|begin_of_text|>']

    if system_prompt != '':
        prompt_texts.append(f'<|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>')

    for history_input, history_response in chat_history:
        prompt_texts.append(f'<|start_header_id|>user<|end_header_id|>\n\n{history_input.strip()}<|eot_id|>')
        prompt_texts.append(f'<|start_header_id|>assistant<|end_header_id|>\n\n{history_response.strip()}<|eot_id|>')

    prompt_texts.append(f'<|start_header_id|>user<|end_header_id|>\n\n{user_input.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n')
    return ''.join(prompt_texts)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Llama3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ `generate()` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì˜ˆì¸¡')
    parser.add_argument('--repo-id-or-model-path', type=str, default="meta-llama/Meta-Llama-3-70B-Instruct",
                        help='Meta-Llama-3 (ì˜ˆ: `meta-llama/Meta-Llama-3-70B-Instruct`)ë¥¼ ë‹¤ìš´ë¡œë“œí•  Huggingface ì €ì¥ì†Œ ID'
                             'ë˜ëŠ” Huggingface ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ëŒ€í•œ ê²½ë¡œ')
    parser.add_argument('--prompt', type=str, default="OpenVINO is",
                        help='ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸')
    parser.add_argument('--n-predict', type=int, default=128,
                        help='ì˜ˆì¸¡í•  ìµœëŒ€ í† í° ìˆ˜')
    parser.add_argument('--bit', type=str, default="4",
                        help='4ë¡œ ì„¤ì •í•˜ë©´ 4ë¹„íŠ¸ë¡œ ë¡œë“œí•˜ê±°ë‚˜ off ë˜ëŠ” load_in_low_bit ì˜µì…˜ì€ sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k')

    args = parser.parse_args()
    model_path = args.repo_id_or_model_path

    if args.bit == "4":
        # 4ë¹„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ,
        # ëª¨ë¸ì˜ ê´€ë ¨ ë ˆì´ì–´ë¥¼ INT4 í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        # iGPUë¥¼ ì‚¬ìš©í•˜ëŠ” Windows ì‚¬ìš©ìì˜ ê²½ìš°, LLMì„ ì‹¤í–‰í•  ë•Œ `cpu_embedding=True`ë¥¼ from_pretrained í•¨ìˆ˜ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ ë©”ëª¨ë¦¬ ì§‘ì•½ì ì¸ ì„ë² ë”© ë ˆì´ì–´ê°€ iGPU ëŒ€ì‹  CPUë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        model = AutoModelForCausalLM.from_pretrained(model_path,
                                                    load_in_4bit=True,
                                                    optimize_model=True,
                                                    trust_remote_code=True,
                                                    use_cache=True)
    elif args.bit == "off" or args.bit == "32":
        model = AutoModelForCausalLM.from_pretrained(model_path,
                                                    optimize_model=True,
                                                    trust_remote_code=True,
                                                    use_cache=True)
    else:
        model = AutoModelForCausalLM.from_pretrained(model_path,
                                                    load_in_low_bit=args.bit,
                                                    optimize_model=True,
                                                    trust_remote_code=True,
                                                    use_cache=True)

    if args.bit == "32":
        model = model.to('xpu')
    else:
        model = model.half().to('xpu')

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    # ì—¬ê¸°ì„œ ì¢…ê²°ìëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm
    ì¢…ê²°ì = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>"),
    ]

    # ì˜ˆì¸¡ëœ í† í° ìƒì„±
    with torch.inference_mode():
        prompt = get_prompt(args.prompt, [], system_prompt=DEFAULT_SYSTEM_PROMPT)
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to('xpu')
        # ipex_llm ëª¨ë¸ì€ ì›Œë°ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ì¶”ë¡  ì‹œê°„ì„ ì •í™•í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        output = model.generate(input_ids,
                                eos_token_id=terminators,
                                max_new_tokens=20)

        # ì¶”ë¡  ì‹œì‘
        st = time.time()
        output = model.generate(input_ids,
                                eos_token_id=terminators,
                                max_new_tokens=args.n_predict)
        torch.xpu.synchronize()
        end = time.time()
        output = output.cpu()
        output_str = tokenizer.decode(output[0], skip_special_tokens=False)
        print(f'ì¶”ë¡  ì‹œê°„: {end-st} s, í† í°: {len(output[0])}, t/s:{len(output[0])/(end-st)}')
        print('-'*20, 'í”„ë¡¬í”„íŠ¸', '-'*20)
        print(prompt)
        print('-'*20, 'ì¶œë ¥ (skip_special_tokens=False)', '-'*20)
        print(output_str)
```

<div class="content-ad"></div>

```js
python ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=fp16
```

![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png)

![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png)

DirectMLì€ ë‚®ì€ ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ëŠ” ONNX Runtimeì˜ Execution Providerê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í…ŒìŠ¤íŠ¸ëŠ” ì´ python APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

<div class="content-ad"></div>


```js
pip install onnxruntime-genai --pre
pip install onnxruntime-genai-directml --pre
pip install torch transformers onnx onnxruntime
conda install conda-forge::vs2015_runtime
```

ë§ˆì§€ë§‰ ì¤„ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.

ë¼ë§ˆ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.

```js
python -m onnxruntime_genai.models.builder -m meta-llama/Meta-Llama-3-8B-Instruct -o llama-3-8B-Instruct-int4-onnx-directml -p int4 -e dml -c ..\.cache\huggingface\hub\
```

<div class="content-ad"></div>

ë³€í™˜ëœ ëª¨ë¸ì´ ğŸ¤— í—ˆë¸Œì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ genai-llama3.pyì…ë‹ˆë‹¤.

```python
import time
import argparse
import onnxruntime_genai as og

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Predict Tokens using onnxruntime_genai')
    parser.add_argument('--model-path', type=str, default="llama-3-8B-Instruct-int4-onnx-directml",
                        help='model path')
    parser.add_argument('--prompt', type=str, default="OpenVINO is",
                        help='Prompt to infer')
    parser.add_argument(
        '--max-length',
        type=int,
        default=143,
        help='max lengths'
    )
    args = parser.parse_args()

    model = og.Model(args.model_path)
    tokenizer = og.Tokenizer(model)
    
    # Set the max length to something sensible by default,
    # since otherwise it will be set to the entire context length
    search_options = {}
    search_options['max_length'] = args.max_length

    chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

    text = args.prompt
    if not text:
        print("ì˜¤ë¥˜, ì…ë ¥ì´ ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        exit

    prompt = f'{chat_template.format(input=text)}'

    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)
    params.input_ids = input_tokens

    st =  time.time()
    output = model.generate(params)
    out_txt = tokenizer.decode(output[0])

    sec = time.time() - st
    cnt = len(output[0])

    print("ìƒì„± ê²°ê³¼:", out_txt)
    print(f'ì¶”ë¡  ì‹œê°„: {sec} ì´ˆ, í† í° ìˆ˜: {cnt}, ì´ˆë‹¹ í† í° ìˆ˜:{cnt/sec}')
```

<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png" />


<div class="content-ad"></div>


![ì´ë¯¸ì§€](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png)

ğŸ¤—Transformers + IPEX-LLMê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆì–´ìš”.

```js
python ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=4
```

![ì´ë¯¸ì§€](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png)


<div class="content-ad"></div>


![ì´ë¯¸ì§€1](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png)

ê²°ê³¼ë¥¼ í•¨ê»˜ ì‚´í´ë´…ì‹œë‹¤.

![ì´ë¯¸ì§€2](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png)

ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ì´ëŸ¬í•œ í¥ë¯¸ë¡œìš´ ì°¨ì´ê°€ ìƒê²¼ëŠ”ì§€ì— ëŒ€í•œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.
