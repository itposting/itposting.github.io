---
title: "ì–µì œëœ ëª¨ë“  LLMì„ í•´ì œí•˜ì„¸ìš”"
description: ""
coverImage: "/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png"
date: 2024-06-19 03:36
ogImage: 
  url: /assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png
tag: Tech
originalTitle: "Uncensor any LLM with abliteration"
link: "https://medium.com/@mlabonne/uncensor-any-llm-with-abliteration-d30148b7d43e"
---


## ì¬í•™ìŠµ ì—†ì´ ì„¸ë°€ ì¡°ì •í•˜ê¸°

![ì´ë¯¸ì§€](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png)

ëŒë§ˆ ëª¨ë¸ì˜ ì„¸ëŒ€ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‚¬ìš©ìì˜ ì§€ì‹œë¥¼ ì´í•´í•˜ê³  ë”°ë¥´ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ 'ì„¸ì„¸í•œ ì¡°ì •(ì„¸ì„¸í•˜ê²Œ ì¡°ì •)' ë²„ì „ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ë§¤ìš° ê²€ì—´ë˜ì–´ ìˆìœ¼ë©° í•´ë¡œìš´ ìš”ì²­ìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ê²ƒì€ ê±°ë¶€í•˜ê³  "AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ë„ì™€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ì™€ ê°™ì€ ëŒ€ë‹µì„ í•©ë‹ˆë‹¤. ì´ ì•ˆì „ ê¸°ëŠ¥ì€ ì˜¤ìš©ì„ ë°©ì§€í•˜ëŠ” ë° ì¤‘ìš”í•˜ì§€ë§Œ, ëª¨ë¸ì˜ ìœ ì—°ì„±ê³¼ ë°˜ì‘ì„±ì„ ì œí•œí•©ë‹ˆë‹¤.

ë³¸ ë¬¸ì„œì—ì„œëŠ” "ë¬´íš¨í™”"ë¼ëŠ” ê¸°ìˆ ì„ íƒêµ¬í•˜ì—¬ ì¬í•™ìŠµ ì—†ì´ ì–´ë–¤ ëŒë§ˆ ëª¨ë¸ì´ë“  ê²€ì—´ì„ í‘¸ëŠ” ë°©ë²•ì„ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë¸ì— ë‚´ì¥ëœ ê±°ë¶€ ë©”ì»¤ë‹ˆì¦˜ì„ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í•˜ì—¬ ëª¨ë“  ìœ í˜•ì˜ í”„ë¡¬í”„íŠ¸ì— ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

<div class="content-ad"></div>

ì½”ë“œëŠ” Google Colabì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , LLM ì½”ìŠ¤ì—ì„œë„ GitHubì— ìˆìŠµë‹ˆë‹¤. ì´ ê¸°ì‚¬ë¥¼ êµì •í•´ ì£¼ì‹  FailSpyë‹˜ì—ê²Œ íŠ¹ë³„íˆ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.

# âœ‚ï¸ ì‚­ì œë€ì´ë€?

í˜„ëŒ€ LLMì€ ì•ˆì „ ë° ì§€ì‹œë¥¼ ë”°ë¥´ëŠ” ë°©í–¥ìœ¼ë¡œ ì„¸ë°€í•˜ê²Œ ì¡°ì •ë˜ì–´ ìˆì–´, í•´ë¡œìš´ ìš”ì²­ì„ ê±°ë¶€í•˜ê¸° ìœ„í•´ í›ˆë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Arditi ë“±ì´ ë¸”ë¡œê·¸ ê¸€ì—ì„œ ì„¤ëª…í•œ ë°”ì— ë”°ë¥´ë©´, ì´ ê±°ë¶€ í–‰ë™ì€ ëª¨ë¸ì˜ ì”ë¥˜ ìŠ¤íŠ¸ë¦¼ì— ìˆëŠ” íŠ¹ì • ë°©í–¥ì„ í†µí•´ ì¤‘ì¬ë©ë‹ˆë‹¤. ë§Œì•½ ì´ ë°©í–¥ì„ ëª¨ë¸ì´ ë‚˜íƒ€ë‚´ì§€ ëª»í•˜ë„ë¡ ë§‰ëŠ”ë‹¤ë©´, ìš”ì²­ì„ ê±°ë¶€í•˜ëŠ” ëŠ¥ë ¥ì„ ìƒê²Œ ë©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, ì´ ë°©í–¥ì„ ì¸ìœ„ì ìœ¼ë¡œ ì¶”ê°€í•˜ë©´ ëª¨ë¸ì´ í•´ê°€ ì—†ëŠ” ìš”ì²­ì¡°ì°¨ë„ ê±°ë¶€í•  ìˆ˜ ìˆê²Œë©ë‹ˆë‹¤.

ì „í†µì ì¸ ë””ì½”ë” ì „ìš© Llamaë¥˜ ì•„í‚¤í…ì²˜ì—ì„œëŠ” ì„¸ ê°€ì§€ì˜ ì”ë¥˜ ìŠ¤íŠ¸ë¦¼ì„ ëŒ€ìƒìœ¼ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ê° ë¸”ë¡ì˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ(â€œpreâ€), ì–´í…ì…˜ê³¼ MLP ë ˆì´ì–´ ì‚¬ì´ì—ì„œ(â€œmidâ€), ê·¸ë¦¬ê³  MLP ì´í›„ì—(â€œpostâ€). ë‹¤ìŒ ê·¸ë¦¼ì€ ê° ì”ë¥˜ ìŠ¤íŠ¸ë¦¼ì˜ ìœ„ì¹˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

<div class="content-ad"></div>

<img src="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_1.png" />

LLMì„ ë¬´ì¦ê²€ ìƒíƒœë¡œ ë§Œë“¤ê¸° ìœ„í•´ ë¨¼ì € ëª¨ë¸ ë‚´ì˜ "ê±°ë¶€ ë°©í–¥"ì„ ì‹ë³„í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ëŠ” ëª‡ ê°€ì§€ ê¸°ìˆ ì  ë‹¨ê³„ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- ë°ì´í„° ìˆ˜ì§‘: ìœ í•´í•œ ì§€ì‹œë¬¸ ì§‘í•©ê³¼ ë¬´í•´í•œ ì§€ì‹œë¬¸ ì§‘í•©ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³ , ê°ê°ì˜ ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì—ì„œ ì”ì—¬ ìŠ¤íŠ¸ë¦¼ í™œì„±í™”ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
- í‰ê·  ì°¨ì´: ìœ í•´í•œ ì§€ì‹œì™€ ë¬´í•´í•œ ì§€ì‹œì˜ í™œì„±í™” ê°„ í‰ê·  ì°¨ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ê° ë ˆì´ì–´ì— ëŒ€í•œ "ê±°ë¶€ ë°©í–¥"ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„ íƒ: ì´ëŸ¬í•œ ë²¡í„°ë¥¼ ì •ê·œí™”í•˜ê³ , í‰ê°€í•˜ì—¬ ë‹¨ì¼ ìµœìƒì˜ "ê±°ë¶€ ë°©í–¥"ì„ ì„ íƒí•©ë‹ˆë‹¤.

ê±°ë¶€ ë°©í–¥ì„ ì‹ë³„í•œ í›„, í•´ë‹¹ ê¸°ëŠ¥ì„ í‘œí˜„í•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” "ì œê±°(ablate)" ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¶”ë¡  ì‹œê°„ ê°„ì„­ì´ë‚˜ ê°€ì¤‘ì¹˜ ì§êµí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜êµ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

ë¨¼ì € ì¶”ë¡  ì‹œê°„ ê°œì…ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì”ì°¨ ìŠ¤íŠ¸ë¦¼ì— ê¸°ë¡í•˜ëŠ” ëª¨ë“  êµ¬ì„± ìš”ì†Œ(ì˜ˆ: ì–´í…ì…˜ í—¤ë“œ)ë§ˆë‹¤ ê·¸ ì¶œë ¥ì„ ê±°ë¶€ ë°©í–¥ìœ¼ë¡œ íˆ¬ì˜í•œ í›„ ì´ íˆ¬ì˜ì„ ëºë‹ˆë‹¤. ì´ ëº„ì…ˆì€ ê° í† í°ê³¼ ê° ë ˆì´ì–´ì— ì ìš©ë˜ì–´ ëª¨ë¸ì´ ê²°ì½” ê±°ë¶€ ë°©í–¥ì„ í‘œí˜„í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.

í•œí¸, ê°€ì¤‘ì¹˜ ì§êµí™”ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤. ê±°ë¶€ ë°©í–¥ì— ëŒ€í•´ êµ¬ì„± ìš”ì†Œ ê°€ì¤‘ì¹˜ë¥¼ ì§êµí™”í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì´ ì´ ë°©í–¥ìœ¼ë¡œ ê¸°ë¡í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ì”ì°¨ ìŠ¤íŠ¸ë¦¼ì— ê¸°ë¡í•˜ëŠ” í–‰ë ¬ì„ ì¡°ì •í•˜ì—¬ ì´ëŸ¬í•œ ê¸°ì—¬ê°€ ê±°ë¶€ ë°©í–¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” ê°€ì¤‘ì¹˜ ì§êµí™”ë¥¼ ì‚¬ìš©í•œ ì¢Œì ˆ ì‹¤í˜„ì„ êµ¬í˜„í•  ê²ƒì…ë‹ˆë‹¤.

# ğŸ’» êµ¬í˜„

<div class="content-ad"></div>

ì•„ë˜ì˜ abliteration êµ¬í˜„ì€ FailSpyì˜ ë…¸íŠ¸ë¶ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ ë…¸íŠ¸ë¶ì€ ì›ë˜ ì €ìë“¤ì˜ ë…¸íŠ¸ë¶ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì €ëŠ” ì£¼ë¡œ ì´ë¥¼ ì ì‘í•˜ì—¬ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰½ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì€ ì½”ë“œê°€ ë§ì´ í¬í•¨ë˜ì–´ ìˆì–´ì„œ ë¬´ìŠ¨ ì¼ì´ ë²Œì–´ì§€ëŠ”ì§€ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ê¸°ìˆ ì ì¸ ì„¸ë¶€ ì‚¬í•­ì— ëœ ê´€ì‹¬ì´ ìˆëŠ” ê²½ìš° FailSpyì˜ abliterator ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤ (Hugging Faceì˜ abliterated ëª¨ë¸ ëª¨ìŒë„ í™•ì¸í•´ë³´ì„¸ìš”).

ì´ ì½”ë“œëŠ” ë›°ì–´ë‚œ TransformerLens ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì´ì „ì—ëŠ” EasyTransformerë¡œ ì•Œë ¤ì¡ŒìŒ)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ê±°ìš´ ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ë©”ì»¤ë‹ˆì¦˜ í•´ì„ ê°€ëŠ¥ì„±ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©° ì—¬ê¸°ì„œëŠ” í™œì„±í™”ì— ê°œì…í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§Œë“  Neel Nandaì™€ Joseph Bloomì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.

ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  ê°€ì ¸ì™€ ë´…ì‹œë‹¤. ì´ëŸ¬í•œ ëª¨ë“  ë‹¨ê³„ëŠ” Google Colab ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping

import torch
import functools
import einops
import gc

from datasets import load_dataset
from tqdm import tqdm
from torch import Tensor
from typing import List
from transformer_lens import HookedTransformer, utils
from transformer_lens.hook_points import HookPoint
from transformers import AutoModelForCausalLM, AutoTokenizer
from jaxtyping import Float, Int
from collections import defaultdict

# GPU ë©”ëª¨ë¦¬ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ ìë™ ë¯¸ë¶„ì„ ë•ë‹ˆë‹¤ (í¬ë ˆë”§: Undi95)
torch.set_grad_enabled(False)
```

<div class="content-ad"></div>

ë‘ ê°€ì§€ ë°ì´í„° ì„¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤: í”¼í•´ê°€ ì—†ëŠ” ì§€ì¹¨ì„ í¬í•¨í•œ í•˜ë‚˜ì™€ ìœ í•´í•œ ì§€ì¹¨ì„ í¬í•¨í•œ í•˜ë‚˜ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” tatsu-lab/alpacaì™€ llm-attacksì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ëª¨ë“  ê²ƒì„ ë” ì‰½ê²Œ ë§Œë“¤ê¸° ìœ„í•´, ì €ëŠ” ì´ë¥¼ ë‘ ê°œì˜ Hugging Face ë°ì´í„° ì„¸íŠ¸ë¡œ ë‹¤ì‹œ íŒ¨í‚¤ì§•í•˜ì—¬ mlabonne/harmless_alpacaì™€ mlabonne/harmful_behaviorsë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ í•˜ë©´ ì—¬ëŸ¬ë¶„ì´ ì‰½ê²Œ ì—¬ëŸ¬ë¶„ ìì‹ ì˜ ë°ì´í„° ì„¸íŠ¸ë¡œ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì§€ì¹¨ì„ë¡œë“œí•˜ê³  "role"ê³¼ "content" í‚¤ê°€ ìˆëŠ” ì‚¬ì „ ëª©ë¡ìœ¼ë¡œ ë‹¤ì‹œ ì„œì‹í™”í•  ê²ƒì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ Llama 3ì˜ ì±„íŒ… í…œí”Œë¦¿ì„ ë”°ë¥´ëŠ” apply_chat_tokenizer() ë©”ì„œë“œì™€ í˜¸í™˜ë©ë‹ˆë‹¤.

```python
def reformat_texts(texts):
    return [[{"role": "user", "content": text}] for text in texts]

# ìœ í•´í•˜ê³  ë¬´í•´í•œ ë°ì´í„° ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
def get_harmful_instructions():
    dataset = load_dataset('mlabonne/harmful_behaviors')
    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])

def get_harmless_instructions():
    dataset = load_dataset('mlabonne/harmless_alpaca')
    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])

harmful_inst_train, harmful_inst_test = get_harmful_instructions()
harmless_inst_train, harmless_inst_test = get_harmless_instructions()
```

ì´ì œ ë°ì´í„° ì„¸íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ, abliterate í•˜ë ¤ëŠ” ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ˆíƒ€ê¹ê²Œë„, HookedTransformerë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ, FailSpyì˜ ë…¸íŠ¸ë¶ì— ì„¤ëª…ëœ ê¼¼ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  meta-llama/Meta-Llama-3-8B-Instructë¡œ ì´ë¦„ì„ ë³€ê²½í•˜ê² ìŠµë‹ˆë‹¤. GPUê°€ BF16ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ê²½ìš° torch.float16 í˜•ì‹ìœ¼ë¡œ ë¡œë“œí•˜ì„¸ìš”.

<div class="content-ad"></div>

ì´ ì˜ˆì‹œì—ì„œëŠ” DARE TIES(ëª¨ë¸ ë³‘í•©ì— ê´€í•œ ë‚´ ê¸°ì‚¬ ì°¸ì¡°)ë¡œ ë§Œë“¤ì–´ì§„ mlabonne/Daredevil-8Bë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ 8B ì¹´í…Œê³ ë¦¬ì˜ Open LLM Leaderboardì—ì„œ ê°€ì¥ ë†’ì€ MMLU ì ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆì–´ìš”.

```js
MODEL_ID = "mlabonne/Daredevil-8B"
MODEL_TYPE = "meta-llama/Meta-Llama-3-8B-Instruct"

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = HookedTransformer.from_pretrained_no_processing(
    MODEL_TYPE,
    local_files_only=True,
    dtype=torch.bfloat16,
    default_padding_side='left'
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)
tokenizer.padding_side = 'left'
tokenizer.pad_token = tokenizer.eos_token
```

ì´ì œ ë°ì´í„°ì…‹ì„ í† í°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬´í•´í•œ ë° ìœ í•´í•œ ì§€ì‹œì‚¬í•­ì— ëŒ€í•´ ë™ì¼í•œ ìƒ˜í”Œ ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒ˜í”Œ ìˆ˜ê°€ ë†’ìœ¼ë©´ ëª¨ë“  RAM/VRAMì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” 256ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.

```js
def tokenize_instructions(tokenizer, instructions):
    return tokenizer.apply_chat_template(
        instructions,
        padding=True,
        truncation=False,
        return_tensors="pt",
        return_dict=True,
        add_generation_prompt=True,
    ).input_ids

n_inst_train = min(256, len(harmful_inst_train), len(harmless_inst_train))

# ë°ì´í„°ì…‹ í† í°í™”
harmful_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmful_inst_train[:n_inst_train],
)
harmless_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmless_inst_train[:n_inst_train],
)
```

<div class="content-ad"></div>

ì‘ì—…ì´ ëª¨ë‘ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë„ì²˜ëŸ¼ ì²˜ë¦¬í•˜ëŠ” ì²« ë²ˆì§¸ ë‹¨ê³„ì¸ ë°ì´í„° ìˆ˜ì§‘ì„ êµ¬í˜„í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ í† í°í™”ëœ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ê³  ìœ í•´(harmful) ë° ë¬´í•´(harmless)ë¡œ ë‚˜ë¨¸ì§€ ìŠ¤íŠ¸ë¦¼ í™œì„±í™”ë¥¼ ì €ì¥í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì´ëŠ” transformer_lens ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.

```js
batch_size = 32

# í™œì„±í™”ë¥¼ ì €ì¥í•  ê¸°ë³¸ ì‚¬ì „ ì´ˆê¸°í™”
harmful = defaultdict(list)
harmless = defaultdict(list)

# ë°ì´í„° í•™ìŠµì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
num_batches = (n_inst_train + batch_size - 1) // batch_size

for i in tqdm(range(num_batches)):
    print(i)
    start_idx = i * batch_size
    end_idx = min(n_inst_train, start_idx + batch_size)

    # ìœ í•´ ë° ë¬´í•´ í”„ë¡¬í”„íŠ¸ì— ëª¨ë¸ ì‹¤í–‰ ë° í™œì„±í™” ìºì‹œ
    harmful_logits, harmful_cache = model.run_with_cache(
        harmful_tokens[start_idx:end_idx],
        names_filter=lambda hook_name: 'resid' in hook_name,
        device='cpu',
        reset_hooks_end=True
    )
    harmless_logits, harmless_cache = model.run_with_cache(
        harmless_tokens[start_idx:end_idx],
        names_filter=lambda hook_name: 'resid' in hook_name,
        device='cpu',
        reset_hooks_end=True
    )

    # í™œì„±í™” ìˆ˜ì§‘ ë° ì €ì¥
    for key in harmful_cache:
        harmful[key].append(harmful_cache[key])
        harmless[key].append(harmless_cache[key])

    # RAM ë° VRAM ë¹„ìš°ê¸°
    del harmful_logits, harmless_logits, harmful_cache, harmless_cache
    gc.collect()
    torch.cuda.empty_cache()

# ìºì‹œëœ í™œì„±í™” ê²°í•©
harmful = {k: torch.cat(v) for k, v in harmful.items()}
harmless = {k: torch.cat(v) for k, v in harmless.items()}
```

ì´ì œ ê° ì¸µì— ëŒ€í•œ ê±°ë¶€ ë°©í–¥ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìœ í•´ ë° ë¬´í•´ ëª…ë ¹ì˜ í™œì„±í™” ê°„ í‰ê·  ì°¨ì´ì— í•´ë‹¹í•˜ë©° ì •ê·œí™”ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ activation_scoredì—ì„œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ë©ë‹ˆë‹¤.

```js
# í™œì„±í™” ìƒ‰ì¸ì„ ê°€ì ¸ì˜¤ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜
def get_act_idx(cache_dict, act_name, layer):
    key = (act_name, layer)
    return cache_dict[utils.get_act_name(*key)]

# ì¤‘ê°„ ì¸µì—ì„œ ìœ í•´ ë° ë¬´í•´ í™œì„±í™”ì˜ í‰ê·  ì°¨ì´ ê³„ì‚°
activation_layers = ["resid_pre", "resid_mid", "resid_post"]
activation_refusals = defaultdict(list)

for layer_num in range(1, model.cfg.n_layers):
    pos = -1  # ìœ„ì¹˜ ì¸ë±ìŠ¤
    for layer in activation_layers:
        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=0)
        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(dim=0)
        refusal_dir = harmful_mean_act - harmless_mean_act
        refusal_dir = refusal_dir / refusal_dir.norm()
        activation_refusals[layer].append(refusal_dir)

selected_layers = ["resid_pre"]
activation_scored = sorted(
    [
        activation_refusals[layer][l - 1]
        for l in range(1, model.cfg.n_layers)
        for layer in selected_layers
    ],
    key=lambda x: abs(x.mean()),
    reverse=True,
)
```

<div class="content-ad"></div>

í”„ë¡œì„¸ìŠ¤ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ê³„ì‚°í•œ ê±°ì ˆ ë°©í–¥ì„ í‰ê°€í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ê±°ì ˆ ë°©í–¥ì„ ì¶”ë¡  ì¤‘ ê° ì”ì—¬ ìŠ¤íŠ¸ë¦¼ê³¼ ê° ë¸”ë¡ì— ì ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œ ìŠ¤ë‹ˆí«ì—ì„œ ë„¤ ê°€ì§€ í…ŒìŠ¤íŠ¸ ìœ í•´ ì§€ì‹œì™€ 20ê°œ ë¸”ë¡(ë˜ëŠ” ë ˆì´ì–´)ì— ëŒ€í•œ ì„¸ëŒ€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

```js
...
Code snippet ìƒëµ
...
```

ìš°ë¦¬ëŠ” evals ë¦¬ìŠ¤íŠ¸ì— ëª¨ë“  ì„¸ëŒ€ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ë¥¼ ì¶œë ¥í•˜ê³  ê° ì§€ì‹œì— ëŒ€í•œ ë¬´ê²ì§€ ì•Šì€ ì‘ë‹µì„ ì œê³µí•˜ëŠ” ë ˆì´ì–´(ë¸”ë¡)ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "I cannot" ë° "I can't"ë¥¼ í¬í•¨í•˜ëŠ” ì‘ë‹µì€ ì›ì¹˜ ì•ŠëŠ” ë‹µë³€ì„ í•„í„°ë§í•˜ê¸° ìœ„í•´ ìë™ìœ¼ë¡œ ì œì™¸í•©ë‹ˆë‹¤.

ìœ„ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” ë ˆì´ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ì´ì „ selected_layers ëª©ë¡ì˜ ë‹¤ë¥¸ ì”ì—¬ ìŠ¤íŠ¸ë¦¼, ë‹¤ë¥¸ ì§€ì‹œ, ì¶”ê°€ì  ë¸”ë¡ ë“±ì„ í…ŒìŠ¤íŠ¸í•˜ê³ ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

```js
# ì¸ê°„ í‰ê°€ë¥¼ ìœ„í•œ ì„¸ëŒ€ ì¸ì‡„
blacklist = ["I cannot", "I can't"]
for i in range(N_INST_TEST):
    print(f"\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}")
    print(f"\nBASELINE COMPLETION:\n{baseline_generations[i]}\033[0m")
    for layer_candidate in range(EVAL_N):
        if not any(word in evals[layer_candidate][i] for word in blacklist):
            print(f"\n---\n\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:")
            print(evals[layer_candidate][i])
```

ì €ì˜ ê²½ìš°, ë ˆì´ì–´ í›„ë³´ì 9ê°€ ë„¤ ê°€ì§€ ëª…ë ¹ì— ëŒ€í•´ ì„ ì •ì ì´ì§€ ì•Šì€ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ìš°ë¦¬ê°€ ê±°ë¶€ ë°©í–¥ìœ¼ë¡œ ì„ íƒí•  ê²ƒì´ë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œ, ë¬´ê²Œ ì§êµí™”ë¥¼ êµ¬í˜„í•˜ì—¬ ëª¨ë¸ì´ ì´ ë°©í–¥ì˜ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì„ ì •ë˜ì§€ ì•Šì€ì§€ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì™„ë£Œëœ ë‚´ìš©ì„ ì¸ì‡„í•˜ì—¬ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
def get_orthogonalized_matrix(
    matrix: Float[Tensor, "... d_model"], vec: Float[Tensor, "d_model"]
) -> Float[Tensor, "... d_model"]:
    proj = (
        einops.einsum(
            matrix, vec.view(-1, 1), "... d_model, d_model single -> ... single"
        )
        * vec
    )
    return matrix - proj

# ê°€ì¥ ë†’ì€ ê±°ë¶€ ë°©í–¥ì„ ê°–ëŠ” ë ˆì´ì–´ ì„ íƒ
LAYER_CANDIDATE = 9
refusal_dir = activation_scored[LAYER_CANDIDATE]

# ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì§êµí™”
if refusal_dir.device != model.W_E.device:
    refusal_dir = refusal_dir.to(model.W_E.device)
model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)

for block in tqdm(model.blocks):
    if refusal_dir.device != block.attn.W_O.device:
        refusal_dir = refusal_dir.to(block.attn.W_O.device)
    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)
    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)

# ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
orthogonalized_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

# ì„¸ëŒ€ ì¶œë ¥
for i in range(N_INST_TEST):
    if len(baseline_generations) > i:
        print(f"INSTRUCTION {i}: {harmful_inst_test[i]}")
        print(f"\033[92mBASELINE COMPLETION:\n{baseline_generations[i]}")
    print(f"\033[91mINTERVENTION COMPLETION:\n{evals[LAYER_CANDIDATE][i]}")
    print(f"\033[95mORTHOGONALIZED COMPLETION:\n{orthogonalized_generations[i]}\n")
```

ì´ì œ ëª¨ë¸ì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ Hugging Face í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ HF í—ˆë¸Œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

<div class="content-ad"></div>

```json
# ëª¨ë¸ì„ ë‹¤ì‹œ HF ë³´ì•ˆ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤
hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)
lm_model = hf_model.model

state_dict = model.state_dict()
lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict["embed.W_E"].cpu())
for l in range(model.cfg.n_layers):
    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(
        einops.rearrange(
            state_dict[f"blocks.{l}.attn.W_O"], "n h m->m (n h)", n=model.cfg.n_heads
        ).contiguous()
    )
    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(
        torch.transpose(state_dict[f"blocks.{l}.mlp.W_out"], 0, 1).contiguous()
    )

hf_model.push_to_hub(f"{MODEL_ID}-abliterated")
```

# âš–ï¸ DPO Fine-Tuning

ì´ì „ ì„¹ì…˜ì˜ abliterated ë° ì†ŒìŠ¤ ëª¨ë¸ì„ Open LLM Leaderboard ë° Nousì˜ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ì—ì„œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤:

<img src="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_2.png" />


<div class="content-ad"></div>

ë³´ì‹œë‹¤ì‹œí”¼, ì›ë³¸ ëª¨ë¸ì€ Llama 3 8B Instructë³´ë‹¤ í˜„ì €í•˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ëŠ” ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì ˆì‚­ëœ ë²„ì „ì—ì„œ ì„±ëŠ¥ í•˜ë½ì„ ê´€ì°°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ˆì‚­ ê³¼ì •ì€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ëª¨ë¸ì˜ í’ˆì§ˆì„ ì €í•˜ì‹œí‚¨ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ì ˆì‚­ëœ ëª¨ë¸ì„ ì¶”ê°€ë¡œ í›ˆë ¨í•˜ì—¬ íšŒë³µì‹œí‚¤ëŠ” ì•„ì´ë””ì–´ê°€ ìˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ì„¸ë°€ ì¡°ì •ëœ ëª¨ë¸ë“¤ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Llama 3 8B Instructì€ ì§€ë„ í•™ìŠµ ì„¸ë°€ ì¡°ì •ì— ìˆì–´ì„œ ê½¤ ì·¨ì•½í•©ë‹ˆë‹¤. ì¶”ê°€ì ì¸ SFTëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦´ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

ëŒ€ì²´ë¡œ, ì„ í˜¸ ë§ì¶¤ì´ ìƒë‹¹íˆ ê°€ë³ê³  ìš°ë¦¬ì˜ ì ˆì‚­ëœ ëª¨ë¸ì„ ë‡Œê°œë°•í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. DPOëŠ” ì‚¬ìš©í•˜ê¸° ì‰½ê³  ìš°ìˆ˜í•œ ì¶”ì  ë ˆì½”ë“œë¡œ ì—¬ê¸°ì„œ ì¢‹ì€ í›„ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ mlabonne/orpo-dpo-mix-40k ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” LazyAxolotl (Axolotlì„ ë§Œë“¤ì–´ ì¤€ Wing Lianì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤)ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— ì‚¬ìš©í•œ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```js
base_model: mlabonne/Daredevil-8B-abliterated
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

load_in_8bit: false
load_in_4bit: true
strict: false
save_safetensors: true

rl: dpo
chat_template: chatml
datasets:
  - path: mlabonne/orpo-dpo-mix-40k
    split: train
    type: chatml.intel

dataset_prepared_path:
val_set_size: 0.0
output_dir: ./out

adapter: qlora
lora_model_dir:

sequence_len: 2048
sample_packing: false
pad_to_sequence_len: false

lora_r: 64
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:

wandb_project: axolotl
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 8
micro_batch_size: 1
num_epochs: 1
optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 5e-6
train_on_inputs: false
group_by_length: false
... (ì´ì–´ì§)
```

<div class="content-ad"></div>

6xA6000 GPUì™€ DeepSpeed ZeRO-2ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í–ˆì–´ìš”. í›ˆë ¨ì—ëŠ” ì•½ 6ì‹œê°„ 45ë¶„ì´ ì†Œìš”ë˜ì—ˆë‹µë‹ˆë‹¤. W&Bì—ì„œ ì–»ì€ í›ˆë ¨ ê³¡ì„ ì„ ì—¬ê¸°ì— ê°€ì ¸ì™”ì–´ìš”:

DPOë¥¼ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•œ mlabonne/NeuralDaredevil-8B-abliterated ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆì–´ìš”. ì €í¬ê°€ ì•ì„œ ì§€ì›Œë²„ë¦° ë²„ì „ì„ ìˆ˜ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë™ì¼í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€í–ˆì–´ìš”:

![í›ˆë ¨ ê³¡ì„ ](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_3.png)

ì´ ì¶”ê°€ í›ˆë ¨ì„ í†µí•´ ì§€ì›Œì§„ ì˜í–¥ ëŒ€ë¶€ë¶„ì„ íšŒë³µí•  ìˆ˜ ìˆì—ˆì–´ìš”. ëª¨ë¸ì´ ê°œì„ ë˜ì§€ ì•ŠëŠ” í•œ ì˜ì—­ì€ GSM8K, ìˆ˜í•™ ë°ì´í„° ì„¸íŠ¸, ì¸ë°ìš”, ì´ëŠ” orpo-dpo-mix-40kê°€ ë” ë§ì€ ìˆ˜í•™ ìƒ˜í”Œì„ í•„ìš”ë¡œ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•  ìˆ˜ ìˆì–´ìš”.

<div class="content-ad"></div>

ìµœì¢… ëª¨ë¸ì€ 8B ë²”ì£¼ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ìë‘í•˜ëŠ” ë¯¸ê²€ì—´ LLMì…ë‹ˆë‹¤. í•„í„°ë§ì´ í•„ìš” ì—†ì„ ë•ŒëŠ” Llama 3 8B Instructì˜ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤. LM Studioì—ì„œ GGUFì™€ ê°™ì€ ì–‘ìí™”ëœ ë²„ì „ì„ ì‚¬ìš©í•´ ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

# ê²°ë¡ 

ì´ ê¸€ì—ì„œëŠ” ì†Œëª…í™”(abliteration) ê°œë…ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë¸ì˜ í™œì„±í™”ë¥¼ í•´ë¡­ê³  í•´ë¥¼ ë¼ì¹˜ì§€ ì•ŠëŠ” í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•˜ì—¬ ê±°ë¶€ ë°©í–¥ì„ ê³„ì‚°í•˜ê³ , ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ê±°ë¶€ë¥¼ ê·¸ë§Œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ì•ˆì „ ì„¸ë°€ì¡°ì •ì˜ ì·¨ì•½ì„±ì„ ë³´ì—¬ì£¼ë©° ìœ¤ë¦¬ì  ê³ ë ¤ ì‚¬í•­ì„ ë˜ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ëŠ” Daredevil-8Bì— ì†Œëª…í™”ë¥¼ ì ìš©í•˜ì—¬ í•„í„°ë§ì„ í•´ì œí–ˆì§€ë§Œ, ì´ë¡œ ì¸í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ í›„ DPOë¥¼ ì‚¬ìš©í•˜ì—¬ NeuralDaredevil-8B ëª¨ë¸ì„ ìƒì„±í•˜ì—¬ ì™„ì „íˆ ë¯¸ê²€ì—´ì´ê³  ê³ í’ˆì§ˆì˜ 8B LLMì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì†Œëª…í™”ëŠ” ì •ë ¬ ì œê±°ì— êµ­í•œë˜ì§€ ì•Šìœ¼ë©°, ë‹¤ì‹œ êµìœ¡ ì—†ì´ ì„¸ë°€ ì¡°ì •ì˜ ì¼ì¢…ìœ¼ë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ MopeyMuleì˜ FailSpyì˜ ê²½ìš°ì²˜ëŸ¼ ì¢Œì ˆì ì¸ ëŒ€í™” ìŠ¤íƒ€ì¼ì„ ì±„íƒí•˜ëŠ” ê²ƒê³¼ ê°™ì´ ì°½ì˜ì ìœ¼ë¡œ ë‹¤ë¥¸ ëª©í‘œì—ë„ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

ì´ ê¸°ì‚¬ê°€ ë§ˆìŒì— ë“¤ì—ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”. ë” ë§ì€ ë‚´ìš©ì„ ë³´ê³  ì‹¶ë‹¤ë©´ Hugging Faceì™€ Twitterì˜ @maximelabonneë¥¼ íŒ”ë¡œìš°í•´ ì£¼ì„¸ìš”.

# ì°¸ê³  ìë£Œ

- FailSpy, â€œabliterator library,â€ GitHub, 2024.
- Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, â€œRefusal in LLMs is mediated by a single direction,â€ Lesswrong, 2024.