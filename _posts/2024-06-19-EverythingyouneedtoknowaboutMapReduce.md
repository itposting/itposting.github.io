---
title: "MapReduce에 대해 알아야 할 모든 것"
description: ""
coverImage: "/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_0.png"
date: 2024-06-19 05:12
ogImage: 
  url: /assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_0.png
tag: Tech
originalTitle: "Everything you need to know about MapReduce"
link: "https://medium.com/data-engineer-things/everything-you-need-to-know-about-mapreduce-aff1c664f3b5"
---


## Google에서 제공하는 'MapReduce: 대규모 클러스터에서 간소화된 데이터 처리' 논문의 모든 주요 통찰

![image](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_0.png)

# 목차

- 동기
- 모델
- MapReduce 구현
- 지원 기능

<div class="content-ad"></div>

# 소개

2009년에 데이터 엔지니어로 일하셨군요.

회사에서는 HDFS에 저장된 TB 단위의 데이터를 사용하고 계시군요.

데이터 처리를 위한 로직을 작성하셨군요.

<div class="content-ad"></div>

하나의 기계에서 실행했군요.

작업을 끝내는 데 반나절이 걸렸군요.

그래서 어떻게 최적화했나요?

더 많은 기계를 사용해야 한다는 것을 깨달았죠.

<div class="content-ad"></div>

그러나 어떻게 하면 계산을 신뢰할 수 있게 병렬 처리할 수 있을까요?

당신이 2009년에 있었던 상황을 기억해봅시다:

- Google BigQuery 출시 1년 전 (2010년)
- Apache Spark 개발 2년 전 (2012년)
- Amazon Redshift 출시 4년 전 (2013년)
- AWS에서 Snowflake 출시 5년 전 (2014년)

가장 가능성이 큰 선택은 구글에서 2004년에 처음으로 소개되었고 나중에 Yahoo가 오픈 소스화한 대규모 병렬 처리 프레임워크인 MapReduce였습니다.

<div class="content-ad"></div>

이번 주에는 Google의 전형적인 논문을 통해 이 프레임워크에 대해 배우게 될 것입니다: MapReduce: 대규모 클러스터에서 간소화된 데이터 처리.

# 동기

Google에서는 수백 개의 연산이 대량의 데이터를 처리합니다. 이 중 대부분은 간단합니다. 그러나 데이터가 너무 많아서 한 대의 기계에서 처리할 수 없으며, 연산을 수백 대 또는 수천 대의 기계로 분산하여 실행하고 합리적으로 완료해야 합니다. 여기서 도전 과제가 있습니다:

- 연산을 어떻게 병렬화할 것인가?
- 데이터를 어떻게 효율적으로 분산할 것인가?
- 장애를 어떻게 처리할 것인가?

<div class="content-ad"></div>

이를 해결하기 위해 Google은 병렬화의 세부 사항을 추상화하여 간단한 계산을 표현할 수 있는 새로운 추상화를 설계했습니다. 이 모델은 Lisp와 다른 함수형 언어의 map 및 reduce 원시 기능에서 영감을 받았습니다. 이 작업의 Google 주요 기여 사항은 다음과 같습니다:

- 병렬 계산을 정의하는 간단하고 강력한 인터페이스.
- 대규모 계산의 자동 병렬화와 분배를 가능하게 함.
- 상용 기계에서 높은 성능 달성 가능.

# 모델

![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_1.png)

<div class="content-ad"></div>

다음은 사용자가 정의하는 두 가지 기능을 갖는 모델입니다:

- Map: 키/값 쌍 입력을 받아들이고 중간 키/값 쌍을 출력하는 함수입니다. 라이브러리는 동일한 키의 모든 값들을 그룹화하고 Reduce 작업에 전달합니다.
- Reduce: Map 작업으로부터 중간 값들을 받습니다. 중간 값들은 이터레이터를 통해 Reduce에 제공됩니다. 그런 다음 Reduce 함수에서 정의된 로직을 사용하여 동일한 키의 중간 값들을 병합합니다 (예: Count, Sum 등). Reduce는 일반적으로 최대 하나의 출력 값을 생성합니다.

정의 이후에 MapReduce 프로그램은 대규모 커머디티 머신 클러스터에서 병렬화되어 실행됩니다. 런타임은 사용자 개입 없이 데이터 파티셔닝, 장애 허용 및 머신 간 통신을 처리할 것입니다.

# MapReduce 구현

<div class="content-ad"></div>

# 실행 개요

시스템은 데이터를 M개의 분할로 자동으로 분할합니다. 이 M개의 분할에서의 Map 호출은 여러 대의 기계에 분산됩니다. 이러한 분할은 서로 다른 기계에 의해 병렬로 처리될 수 있습니다. 중요한 점을 처리하기 위해 중요한 공간을 R 버킷으로 분할하는 파티션 함수(예: 키의 해시 함수)를 사용하여 Reduce 호출이 분산됩니다. 사용자는 파티션(R) 수와 파티션 함수를 정의할 수 있습니다.

**일반적인 MapReduce 흐름에 대한 이미지:**

![MapReduce](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_2.png)

일반적인 MapReduce 흐름에 따른 일반적인 단계는 다음과 같습니다:

<div class="content-ad"></div>

- 맵리듀스 라이브러리는 입력 파일을 일반적으로 16에서 64 메가바이트(MB)로 나눈 M 개의 조각으로 분할합니다. (사용자가 크기/조각 팩터를 구성할 수 있습니다.)
- 그런 다음 여러 대의 기계에서 프로그램을 복사본으로 시작합니다. (여러 대의 기계에서 맵리듀스 프로세스가 실행될 것이라고 생각할 수 있습니다.)
- 맵리듀스 프로세스 중 하나는 마스터라고 불리며 나머지는 마스터에서 작업을 수신하는 워커입니다.
- M개의 맵 작업과 R개의 리듀스 작업이 있고, 유휴 상태의 워커는 마스터로부터 맵 또는 리듀스 작업을 수신합니다.
- 맵 워커는 해당 분할을 읽어 각 쌍을 사용자가 정의한 맵 함수에 전달합니다. (예: 각 값에 X를 곱하거나). 워커는 중간 키/값 쌍 출력을 메모리에 버퍼링합니다.
- 워커는 주기적으로 버퍼링된 쌍을 로컬 디스크에 기록한 다음 그 위치를 디스크에 마스터에게 알립니다.
- 마스터는 리듀스 워커에게 이러한 위치에 대해 통지합니다. 통지를 받은 리듀스 워커는 맵 워커의 로컬 디스크에서 버퍼링된 데이터를 읽기 위해 원격 프로시저 호출을 사용합니다.
- 중간 데이터 읽기가 완료되면 리듀스 워커는 중간 키를 기준으로 데이터를 정렬하여 동일한 키의 모든 발생을 그룹화합니다. 리듀스 워커는 데이터를 정렬해야하는데, 이는 맵 워커에서 서로 다른 키를 처리해야 하기 때문입니다. 정렬은 동일한 키의 값이 서로 가까이 있도록합니다.
- 리듀스 워커는 정렬된 중간 데이터를 반복하여 각 고유 키마다 해당 키와 대응하는 중간 값 집합을 사용자의 리듀스 함수에 전달한 다음, 리듀스 함수의 출력을 이 리듀스 파티션에 대한 최종 출력 파일에 추가합니다.
- 모든 맵 및 리듀스 작업이 완료되면 마스터가 사용자 프로그램을 깨웁니다 (종류의 비동기 프로세스).
- 성공적으로 완료된 후, 맵리듀스 실행의 출력은 R 버킷과 관련된 R 출력 파일에서 사용할 수 있습니다. 분리된 파일을 반환하면 사용자가 이러한 결과 파일을 다른 맵리듀스 프로그램이나 다른 분산 애플리케이션에 입력할 수 있습니다.

# 마스터 데이터 구조

마스터는 각 맵 및 리듀스 작업의 상태와 워커 기계의 식별 정보를 저장합니다. 마스터는 또한 맵 및 리듀스 워커 사이의 "중개역할"을 합니다: 맵 워커로부터의 중간 파일의 위치를 리듀스 워커에게 알려줍니다. 각 완료된 맵 작업에 대해 마스터는 중간 파일의 위치와 크기를 저장합니다. 맵 워커들이 작업을 완료하면 중간 파일의 위치 및 크기 정보를 마스터에게 업데이트합니다. 마스터는 이 정보를 진행 중인 리듀스 작업을 하는 워커에게 푸시합니다.

# 내고장성

<div class="content-ad"></div>

MapReduce의 궁극적인 목표 중 하나는 여러 대의 기계에서 대량의 데이터를 신뢰성 있게 처리하는 것입니다. 그렇다면, 만약 실패가 발생한다면 어떨까요?

작업자의 실패

![작업자 이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_3.png)

- 마스터는 주기적으로 핑을 보내 작업자의 상태를 확인합니다.
- 작업자로부터 일정 시간 동안 응답이 없으면, 마스터는 작업자의 실패로 간주합니다.
- 완료되거나 실패한 Map 작업은 대기 상태로 재설정되어 마스터가 다른 기계에서 이러한 작업을 다시 예약할 수 있습니다. 완료된 Map 작업은 로컬 디스크에 결과가 저장되므로, 만약 Map 기계가 실패하면 Map 결과도 접근할 수 없게 됩니다.
- 실패한 Reduce 작업은 또한 대기 상태로 설정되어 다시 예약될 수 있습니다. 완료된 Reduce 작업은 결과가 전역 파일 시스템에 저장되므로 다시 실행할 필요가 없습니다.
- 작업자 A가 Map 작업을 처리하고, 나중에 작업자 A가 실패하면, 마스터의 일정에 따라 작업자 B가 이 Map 작업을 담당하게 되며, 모든 Reduce 작업자에게 다시 실행이 필요함을 통지합니다. 처음에 Map Worker A의 데이터를 사용한 모든 Reduce 작업은 Worker B의 데이터를 읽게 됩니다.

<div class="content-ad"></div>

실패한 마스터

- 마스터는 주기적인 메타데이터 체크포인트를 작성합니다.
- 마스터가 다운되면, 마지막 체크포인트 상태에서 새로운 마스터를 시작할 수 있습니다.

# 지역성

![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_4.png)

<div class="content-ad"></div>

구글은 GFS가 관리하는 입력 데이터가 클러스터의 기계들의 로컬 디스크에 저장되어 있음을 이용하여 네트워크 대역폭을 활용합니다. GFS는 각 파일을 64MB 블록으로 나누고 각 블록의 중복된 복제본 (기본 3개)을 다른 기계에 저장합니다. MapReduce 마스터는 해당 입력의 복제본을 저장하는 기계에 Map 작업을 스케줄하려고 노력합니다. 마스터가 그 방법으로 스케줄을 할 수 없으면 해당 작업의 입력 데이터의 복제본 근처에 Map 작업을 스케줄하려고 시도합니다. 이렇게 함으로써 대부분의 워커가 입력 데이터를 로컬에서 읽고 네트워크 대역폭을 소비하지 않도록 보장합니다.

## 작업의 정밀도

MapReduce는 M 단계를 M 조각으로, Reduce 단계를 R (사용자가 제한함) 조각으로 나눕니다. 일반적으로 M과 R은 사용 가능한 워커보다 훨씬 더 커야 합니다. 각 워커가 여러 다양한 작업을 수행하도록 함으로써 동적으로 작업을 분산하고 워커가 실패할 때의 복구 속도를 높일 수 있습니다: 실패한 Map 작업은 완료된 모든 다른 워커 기계들 사이에 분산될 수 있습니다. M과 R의 크기에는 제한이 있어야 합니다. 마스터는 O(M + R) 스케줄링 결정을 내려야 하고 O(M ∗ R) 상태를 메모리에 유지해야 하기 때문에 마스터에 부담이 많이 가지 않도록 하는 것이 필요합니다.

## 백업 작업

<div class="content-ad"></div>

<img src="/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_5.png" />

맵리듀스 작업의 전체 대기 시간이 증가하는 일반적인 이유 중 하나는 "straggler"입니다. straggler는 소수의 맵 또는 리듀스 작업 중 하나를 완료하는 데 더 오랜 시간이 걸리는 머신을 가리킵니다. straggler는 나쁜 디스크가 장착된 머신이거나 마스터가 머신에서 여러 작업을 예약하여 작업 간 리소스 경합을 야기하는 등 다양한 이유로 발생할 수 있습니다. Google은 이에 대한 해결책을 갖고 있습니다. MapReduce 작업이 거의 완료되기 직전에 마스터는 진행 중인 나머지 작업의 백업 실행을 예약합니다. 이 백업 작업은 기본 작업과 함께 병행하여 실행됩니다. 주요 작업 또는 백업 실행이 완료되면 작업은 완료된 것으로 표시됩니다. 따라서 주요 작업에 문제가 발생하여 처리 속도가 늦추어지면, straggler 증상이 없는 백업 작업이 프로세스를 효율적으로 처리할 수 있습니다.

# 지원 기능

맵리듀스의 기본 기능은 대부분의 요구사항을 충족시키지만, Google은 몇 가지 확장 기능이 유용하다고 판단했습니다.

<div class="content-ad"></div>

# 파티션 함수

기본 데이터 파티션 함수는 해싱입니다. 다른 논리가 필요한 상황을 지원하기 위해 MapReduce 라이브러리 사용자는 사용자 정의 파티션 함수를 제공할 수 있습니다.

# 순서 보장

MapReduce는 중간 키/값 쌍이 주어진 파티션 내에서 키 순서를 오름차순으로 처리함을 보장합니다. 이는 각 파티션마다 정렬된 출력 파일을 생성하는 것이 쉬워지도록 만듭니다. 사용자가 데이터가 정렬되어 있으면 편리하다고 생각할 때 유용합니다.

<div class="content-ad"></div>

# Combiner Function

컴바이너 함수는 맵 워커에서 실행됩니다. 일반적으로 사용자는 컴바이너 함수와 리듀스 함수를 구현하는 데 동일한 코드를 사용합니다. 둘 사이의 유일한 차이점은 MapReduce 라이브러리가 함수의 출력을 처리하는 방법입니다:

- 리듀스 함수의 출력은 최종 출력 파일에 작성됩니다.
- 컴바이너 함수의 출력은 중간 파일에 작성되고, 이 파일은 리듀스 작업으로 전송됩니다.

# 입력 및 출력 유형

<div class="content-ad"></div>

맵리듀스는 다양한 형식의 입력 데이터를 읽을 수 있는 지원을 제공합니다. 예를 들어, 텍스트 유형은 키를 파일 내 오프셋으로 처리하고 값을 라인의 내용으로 처리합니다. 사용자는 간단한 리더 인터페이스를 구현하여 새로운 입력 유형을 지원할 수 있습니다. 출력 유형에 대해, 맵리듀스는 다양한 형식으로 데이터를 생성하기 위한 일련의 출력 유형을 지원하며 사용자가 새로운 출력 유형을 정의할 수도 있습니다.

# 부작용

맵리듀스를 사용하면 매핑 또는 리듀싱 작업이 추가 파일을 추가로 생성할 수 있는지 여부를 지정할 수 있습니다.

# 잘못된 레코드 건너뛰기

<div class="content-ad"></div>

가끔 사용자 코드에 버그가 있어 Map 또는 Reduce 함수가 특정 레코드에서 크래시하는 경우가 있습니다. 이러한 버그로 인해 MapReduce 프로그램이 완료되지 못할 수 있습니다. 보통은 버그를 수정하는 것이 일반적인 해결책이지만 때로는 추가 조치가 필요할 수도 있습니다. 버그가 제3자 라이브러리에서 발생할 수 있으며 해당 소스 코드에 접근할 수 없는 경우가 있습니다. 또한 경우에 따라 일부 레코드를 무시하는 것이 허용됩니다. Google은 옵션 실행 모드를 제공하여 MapReduce 라이브러리가 어떤 레코드가 크래시를 발생시킬 수 있는지 감지하고 이러한 레코드를 건너뛰어 전진 진행을 이룰 수 있습니다.

# 상태 정보

마스터는 내부 HTTP 서버를 실행하고 MapReduce 프로그램의 모니터링 및 추적을 위한 일련의 상태 페이지를 제공합니다. 상태 페이지에는 계산의 진행 상황을 보여주는 정보가 포함되어 있습니다. 완료된 작업, 진행 중인 작업, 입력 크기, 중간 크기, 출력 크기 등이 표시됩니다. 또한 각 작업에서 생성된 표준 오류 및 표준 출력 파일에 대한 링크도 포함되어 있습니다. 최상위 상태 페이지에는 실패한 워커와 그들이 실패할 때 처리하던 맵 및 리듀스 작업이 표시됩니다. 사용자는 이 데이터를 사용하여 계산이 얼마나 걸릴지 예상하고 더 많은 리소스를 추가해야 하는지 여부를 판단할 수 있습니다.

# 카운터

<div class="content-ad"></div>

MapReduce 라이브러리는 다양한 이벤트의 발생 횟수를 세는 카운터를 제공합니다. 예를 들어, 사용자 코드는 처리된 총 단어 수를 세고 싶어할 수 있습니다. 이 기능을 사용하려면 사용자는 명명된 카운터 객체를 만들고 Map 및 Reduce 함수에서 카운터를 적절하게 증가시킵니다. 워커는 주기적으로 카운터 값을 마스터에 보고하며, 이 보고는 마스터로부터 ping-health-check 요청에 대한 응답과 함께 전송됩니다. 마스터는 성공한 Map 및 Reduce 작업에서 카운터 값을 집계하고 프로그램이 완료될 때 사용자 코드로 반환합니다. 카운터 값을 집계할 때 마스터는 동일한 Map/Reduce 작업, 백업 작업 또는 실패한 작업의 중복 실행의 영향으로 발생하는 중복을 제거합니다.

# 마무리

이 글을 통해 큰 규모 클러스터에서 간단한 데이터 처리를 위한 MapReduce의 편리함을 알아냈습니다. 두 가지 Map 및 Reduce 함수로 구성되어 있지만 이 프레임워크는 상용품 기계의 대규모 클러스터에서 연산을 병렬화하는 매우 효율적이고 견고한 방법을 제공합니다. 글을 마치기 전에 한 가지 더 알려드립니다: BigQuery 처리 엔진인 Dremel은 MapReduce의 영감을 받아 개발되었습니다.

이제 이만 쉬어 가도록 하겠습니다.

<div class="content-ad"></div>

다음 블로그에서 만나요 ;)

# 참고 자료

[1] Jeffrey Dean and Sanjay Ghemawat, MapReduce: Simplified Data Processing on Large Clusters (2004).