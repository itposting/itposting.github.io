---
title: "특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 "
description: ""
coverImage: "/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png"
date: 2024-06-19 09:48
ogImage: 
  url: /assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png
tag: Tech
originalTitle: "Data Engineering pipeline leveraging Airflow , Kafka"
link: "https://medium.com/@akhilmakol/data-engineering-pipeline-leveraging-airflow-kafka-063b3a1cd252"
---


# 소개

이 문서에서는 Apache Airflow 및 Kafka(오픈 소스)를 활용하여 실시간 날씨 업데이트를 읽고 Twitter의 이벤트 스트림을 분석하여 대중의 반응을 이해하는 데이터 엔지니어링 파이프라인 아키텍처를 솔루션 디자인하는 방법에 대한 기본적인 개요를 제공합니다.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png)

# ETL 기본 원칙

<div class="content-ad"></div>

ETL은 데이터 웨어하우스 또는 데이터 마트와 같은 분석 환경을 위해 데이터 획득 및 준비를 자동화하는 데이터 파이프라인 엔지니어링 방법론인 추출, 변환, 로드를 의미합니다. 이는 다양한 소스에서 데이터를 수집하고 표준 형식으로 편집한 다음 시각화, 탐색 및 모델링을 위한 새로운 환경으로 로드하는 것을 포함하며, 자동화 및 의사 결정을 지원합니다.

# ELT 기본

ELT 프로세스는 추출, 로드 및 변환을 나타내며, 단곅하계의 단계 순서로 인한 독특한 차이로 인해 ETL과 다릅니다. ELT에서는 데이터가 데이터 레이크와 같은 목적지 환경으로 원본 형식 그대로 직접 로드됩니다. 이를 통해 목적지 플랫폼 내에서 필요에 따라 변환을 수행하고 동적으로 사용자 주도적 변경을 가능하게 합니다.

# ETL과 ELT 비교

<div class="content-ad"></div>

ETL과 ELT의 차이점: ETL 파이프라인에서는 변환 작업이 목적지에 도달하기 전에 데이터 파이프라인 내에서 발생하는 반면, ELT는 변환 작업을 분리하여 목적지 환경에서 필요한 대로 수행할 수 있습니다. ETL은 엄격하고 목적이 명확하지만, ELT는 유연하며 빅 데이터 처리에 대한 셀프 서비스 분석을 제공합니다.

# 데이터 수집 기술

다양한 데이터 수집 기술에는 다음이 포함됩니다:

- 완전 수집 대 부분 수집

<div class="content-ad"></div>

- 전체 로딩: 데이터베이스에 초기 히스토리를 로드합니다. 추적 데이터는 새 창고에서 시작됩니다.
- 점진적 로딩: 새 데이터를 삽입하거나 이미 로드된 데이터를 업데이트합니다. 거래 히스토리를 누적하는 데 사용됩니다. 데이터 양과 속도에 따라 일괄 또는 스트림 로드될 수 있습니다.

정기 로딩 vs. 요청 로딩:

- 정기 로딩: 매일 거래를 데이터베이스에 주기적으로 로드하며, 스크립트 작업에 의해 자동화됩니다.
- 요청 로딩: 소스 데이터가 지정된 크기에 도달하거나 움직임, 소리 또는 임의의 변경 이벤트와 같은 다양한 이벤트에 의해 트리거됩니다.

일괄 처리 vs. 스트림 로딩:

<div class="content-ad"></div>

- 배치 로딩: 시간별로 정의된 단위로 데이터를 로드하며 일반적으로 몇 시간에서 며칠 동안 누적됩니다.
- 스트림 로딩: 데이터가 제공되는 즉시 실시간으로 로드됩니다.
- 마이크로 배치 로딩: 즉시 처리를 위한 최근 데이터에 액세스합니다.

푸시 대 수신 데이터 로딩:

- 수신 방법: 클라이언트가 서버로부터 데이터를 요청합니다(예: RSS 피드, 이메일).
- 푸시 방법: 클라이언트가 서버 서비스에 구독하여 데이터를 실시간으로 전달 받습니다(예: 푸시 알림, 즉각 메시징 서비스).

# 데이터 파이프라인이란 무엇인가요?

<div class="content-ad"></div>

데이터 파이프라인은 데이터의 이동 또는 수정에 특히 관련이 있어요. 이러한 파이프라인은 데이터를 한 곳이나 형식에서 다른 곳이나 형식으로 운송하는 것을 목표로 하며, 데이터를 추출하고 최종적으로 적재하기 위해 선택적 변환 단계를 통해 안내하는 시스템을 구성합니다.

파이프라인을 통해 흐르는 데이터를 시각화하는 것은 데이터 패킷으로 표현할 수 있으며, 이는 데이터의 단위를 넓게 이야기합니다. 이러한 패킷은 단일 레코드나 이벤트에서 대량 데이터 수집물까지 다양할 수 있어요. 이 맥락에서 데이터 패킷은 파이프라인으로 흡수되기 위해 대기열에 정리되며, 데이터 파이프라인의 길이는 단일 패킷이 횡단하는 데 걸리는 시간을 의미합니다. 패킷 간의 화살표는 처리량 지연이나 연속 패킷 도착 사이의 시간을 나타냅니다.

데이터 파이프라인 주요 성능 지표

- 지연 시간: 데이터 패킷이 파이프라인을 통과하는 총 시간을 의미합니다. 지연 시간은 파이프라인 내 각 처리 단계에서 소요된 개별 시간의 합으로, 파이프라인 내 가장 느린 프로세스에 의해 제한됩니다. 예를 들어, 웹 페이지의 로딩 시간은 서버 속도에 따라 결정되며 인터넷 서비스 속도와 관계없이 서버 속도에 의해 제어됩니다.
- 처리량: 이는 시간 단위당 파이프라인을 통해 처리될 수 있는 데이터 양을 의미합니다. 처리량을 증가시키는 것은 시간 단위당 더 많은 패킷을 처리하고 큰 상자를 차례차례 통과시키는 우리 친구 사슬 예시와 유사합니다.

<div class="content-ad"></div>

데이터 파이프라인 응용:

- 간단한 복사 파이프라인: 파일 백업과 같이 데이터를 한 위치에서 다른 위치로 복사하는 작업을 포함합니다.
- 데이터 레이크 통합: 분산된 래 데이터 소스를 데이터 레이크에 통합하는 작업입니다.
- 거래 기록 이동: 거래 기록을 데이터 웨어하우스로 전송하는 작업입니다.
- IoT 데이터 스트리밍: IoT 장치에서 데이터를 스트리밍하여 대시보드나 경보 시스템에서 정보를 제공하는 것을 말합니다.
- 기계 학습용 데이터 준비: 기계 학습의 개발이나 제품화를 위해 래 데이터를 준비하는 작업입니다.
- 메시지 보내기 및 받기: 이메일, SMS 또는 온라인 비디오 회의와 같은 애플리케이션을 포함합니다.

# 주요 데이터 파이프라인 프로세스

데이터 파이프라인 프로세스는 일반적으로 구조화된 일련의 단계를 따릅니다:

<div class="content-ad"></div>

- 추출: 하나 이상의 소스에서 데이터를 검색하는 과정입니다.
- 투입: 추출된 데이터는 후속 처리를 위해 파이프라인에 투입됩니다.
- 변환: 파이프라인 내의 선택적인 단계에서 데이터를 변환할 수 있습니다.
- 로딩: 최종 단계는 변환된 데이터를 대상 시설로 로드합니다.
- 스케줄링/트리거링: 작업을 필요에 따라 예약하거나 트리거하는 메커니즘입니다.
- 모니터링: 전체 워크플로우가 효율적으로 작동하도록 모니터링됩니다.
- 유지보수 및 최적화: 원활한 파이프라인 운영을 보장하기 위해 정기적인 유지보수 및 최적화 작업이 수행됩니다.

# Apache Airflow

- Python 기반의 오픈 소스 "구성과 코드" 플랫폼입니다. AirBNB에서 오픈 소스로 공개되었습니다.
- 데이터 파이프라인 워크플로우를 작성, 예약 및 모니터링할 수 있습니다.
- 확장 가능하며 병렬 컴퓨팅 노드를 지원하며 주요 클라우드 플랫폼과 통합됩니다.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_1.png)

<div class="content-ad"></div>

- 스케줄러: 예약된 워크플로우를 트리거합니다.
- 실행기: 작업을 Worker에 할당하여 실행합니다.
- 웹 서버: DAG 검사, 트리거 및 디버깅을 위한 대화형 UI를 호스팅합니다.
- DAG 디렉토리: 스케줄러, 실행기 및 Worker가 액세스할 수 있는 DAG 파일을 저장합니다.
- 메타데이터 데이터베이스: 각 DAG 및 해당 작업의 상태를 유지합니다.

DAG 및 작업 라이프사이클

DAG(유향 비순환 그래프)는 작업 간의 종속성과 실행 순서를 지정합니다. 'DAG'는 순환이나 사이클이 없는 관계를 나타내는 특정 유형의 그래프로, 노드와 간선으로 구성되며 방향성을 가진 간선이 노드 간의 흐름을 보여줍니다.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_2.png)

<div class="content-ad"></div>

작업 상태:

- 상태 없음: 실행을 위해 대기 중인 작업.
- 스케줄됨: 의존성에 따라 실행이 예약된 작업.
- 제거됨: 실행이 시작된 이후에 사라진 작업.
- 상류 작업 실패: 상류 작업에서 실패 발생.
- 대기 중: 워커 가용성을 기다리는 작업.
- 실행 중: 워커에 의해 실행 중인 작업.
- 성공: 오류 없이 작업이 완료된 상태.
- 실패: 실행 중에 오류가 발생한 작업.
- 재시도 예정: 남은 재시도 횟수가 남아 있는 실패한 작업으로, 다시 예약됨.
- 이상적인 작업 흐름: '상태 없음'에서 '스케줄됨'으로, '대기 중'으로, '실행 중'으로 이어져 '성공'으로 마무리됨.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_3.png)

Airflow DAG 스크립트의 논리 블록

<div class="content-ad"></div>

- 라이브러리 가져오기: 필요한 Python 라이브러리 가져오기.
- DAG 인수: DAG에 대한 기본 인수(시작 날짜와 같은 것) 정의.
- DAG 정의: 특정 속성을 사용하여 DAG 인스턴스화.
- 작업 정의: DAG 내부의 개별 작업(노드) 정의.
- 작업 파이프라인: 작업 간의 의존성을 지정하여 작업 간의 흐름을 구축.

이러한 논리적 블록이 포함된 Python 스크립트 예제를 살펴보세요.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_4.png)

# Kafka를 활용한 스트리밍 파이프라인 구축

<div class="content-ad"></div>

이벤트는 이벤트 스트리밍의 맥락에서 엔티티의 관찰 가능한 상태 업데이트를 설명하는 데이터를 의미합니다. 예시로는 자동차의 GPS 좌표, 방 온도, 또는 응용 프로그램의 RAM 사용량 등이 있습니다.

이벤트는 다양한 형식으로 제공됩니다:

- 텍스트, 숫자 또는 날짜와 같은 원시 유형
- 값이 원시 또는 복합 유형인 키-값 쌍 형식(e.g., JSON, XML)
- 타임스탬프가 포함된 시간 감도를 위한 키-값 형식

한 소스에서 한 대상으로: 이벤트 스트리밍은 소스(센서, 데이터베이스, 응용 프로그램)가 실시간 이벤트를 지속적으로 생성하고 이를 대상지(파일 시스템, 데이터베이스, 응용 프로그램)로 전달하는 것을 의미합니다. 이 과정은 이벤트 스트리밍이라고 불립니다.

<div class="content-ad"></div>

많은 출처에서 많은 대상으로: 다양한 통신 프로토콜(FTP, HTTP, JDBC, SCP)을 사용하는 여러 분산 이벤트 소스 및 대상을 관리하는 것은 도전이 될 수 있습니다. 이벤트 스트림 플랫폼(ESP)은 미들웨어로 작용하여 다양한 이벤트 기반 ETL의 처리를 간단하게 만듭니다.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_5.png)

ESP 구성 요소

- 이벤트 브로커: 이벤트를 수신하고 소비하는 핵심 구성 요소입니다.
- 이벤트 저장소: 받은 이벤트를 저장하여 대상이 비동기적으로 검색할 수 있도록 합니다.
- 분석 및 쿼리 엔진: 저장된 이벤트를 쿼리하고 분석합니다.

<div class="content-ad"></div>

이벤트 브로커는 중요한 컴포넌트입니다. 이는 다음과 같은 구성 요소들을 포함합니다:

- Ingester: 다양한 소스에서 이벤트를 효율적으로 수신합니다.
- Processor: 직렬화, 역직렬화, 압축, 압축 해제, 암호화 및 복호화와 같은 작업을 수행합니다.
- Consumption: 저장소에서 이벤트를 검색하고 구독된 대상에게 분배합니다.

인기 있는 이벤트 처리 시스템 솔루션:

- Apache Kafka
- Amazon Kinesis
- Apache Flink
- Apache Spark
- Apache Storm

<div class="content-ad"></div>

아파치 카프카: 독특한 기능과 광범위한 응용 시나리오를 갖춘 가장 인기 있는 ESP 중 하나입니다. 카프카는 분산 클라이언트-서버 모델을 따릅니다.

- 서버 측: 효율적인 협업을 위해 ZooKeeper가 관리하는 여러 브로커로 구성됩니다.
- 네트워크 통신: 클라이언트와 서버 간의 데이터 교환에 TCP를 활용합니다.
- 클라이언트 측: CLI, 자바, 스칼라, REST API 및 타사 옵션을 포함한 다양한 클라이언트를 제공합니다.

카프카의 인기 이유는?

- 확장성: 데이터를 여러 브로커에 분산하여 확장성과 고 처리량을 보장합니다.
- 높은 신뢰성: 안정성을 위해 여러 파티션과 복제를 사용합니다.
- 영구적인 지속성: 이벤트를 영구적으로 저장하여 소비자의 편의에 맞게 사용할 수 있습니다.
- 오픈 소스: 특정 요구 사항에 맞춰 사용자 정의가 가능하여 무료로 제공됩니다.

<div class="content-ad"></div>

# 카프카 아키텍처

카프카 클러스터는 여러 브로커로 구성되어 있으며, 각 브로커는 이벤트를 수신, 저장, 처리 및 배포하는 역할을 합니다. ZooKeeper에 의해 조율되는 이러한 브로커들은 로그 또는 트랜잭션과 같은 특정 이벤트 유형을 저장하는 데이터베이스와 유사한 주제를 관리합니다.

파티셔닝과 복제: 카프카는 장애 허용성과 병렬 이벤트 처리를 위해 파티셔닝과 복제를 사용합니다. 일부 브로커가 실패하더라도, 카프카는 주제 파티션을 운영 중인 브로커에 분산시킴으로써 지속성을 보장합니다.

Kafka CLI를 사용한 주제 관리: 카프카 명령줄 인터페이스는 카프카 클러스터 내에서 주제를 생성, 나열, 설명 및 삭제하는 기능을 제공합니다. 명령에는 정의된 파티션 및 복제로 주제를 생성하고 주제 및 구성에 대한 자세한 정보를 얻는 등의 작업이 포함됩니다.

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_6.png)

카프카 프로듀서

카프카 프로듀서는 이벤트를 토픽 파티션에 발행하는 클라이언트 앱입니다. 이벤트는 선택적 파티셔닝을 위해 키와 연결될 수 있습니다. 프로듀서 CLI를 사용하면 프로듀서를 관리하고 지정된 토픽에 이벤트를 키와 함께 발행할 수 있습니다.

컨슈머로 이벤트 읽기: 컨슈머는 토픽에 가입하고 저장된 이벤트를 읽어 순차적으로 오프셋을 유지합니다. 오프셋을 재설정함으로써 컨슈머는 처음부터 이벤트를 다시 재생할 수 있습니다. 카프카 컨슈머와 프로듀서는 독립적으로 작동하여 동기화 없이 이벤트를 저장하고 소비할 수 있습니다.

<div class="content-ad"></div>

끝까지 이어지는 이벤트: 날씨 파이프라인

단계 1: 이벤트 소스 정의

극단적인 날씨에 대한 대중의 반응을 이해하기 위해 날씨와 트위터 이벤트 스트림을 분석하고 싶다고 상상해보세요. 두 가지 주요 이벤트 소스를 활용할 것입니다:

- IBM Weather API: JSON 형식의 실시간 및 예보 날씨 데이터를 제공합니다.
- Twitter API: JSON 형식의 실시간 트윗 및 언급을 제공합니다.

<div class="content-ad"></div>

단계 2: Kafka 토픽 구성

Kafka 클러스터에서 날씨 및 트위터 이벤트용 전용 토픽을 생성하여 데이터 흐름을 효율적으로 처리하기 위해 적절한 파티션 및 복제를 보장하세요.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_7.png)

단계 3: 프로듀서 개발

<div class="content-ad"></div>

각 이벤트 소스에 대해 특정한 프로듀서를 개발하세요. 이들은 JSON 데이터를 바이트로 직렬화하고 해당 Kafka 토픽으로 게시할 것입니다.

![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_8.png)

단계 4: 컨슈머 구현

날씨 및 Twitter 이벤트용 전용 컨슈머를 생성하세요. 이 컨슈머들은 Kafka 토픽에서 바이트를 역직렬화하여 JSON 데이터로 변환한 후 처리할 것입니다.

<div class="content-ad"></div>

<img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_9.png" />

단계 5: Persistence를 위한 DB Writer 통합

이벤트 데이터를 관계형 데이터베이스에 쓰고 싶다면 DB writer를 사용하십시오. 이 구성 요소는 컨슈머에서 JSON 파일을 구문 분석하고 해당 데이터베이스 레코드를 생성합니다.

```js
#db writer EXAMPLE
import json
import sqlite3

def write_to_database(record):
    connection = sqlite3.connect("event_database.db")
    cursor = connection.cursor()
    cursor.execute("INSERT INTO events VALUES (?)", (json.dumps(record),))
    connection.commit()
    connection.close()

record = {"event_type": "weather", "data": {"temperature": 25, "location": "NYC"}}
write_to_database(record)
```

<div class="content-ad"></div>

6단계: SQL을 사용한 데이터베이스 상호작용

데이터베이스에 레코드를 쓰기 위해 SQL 삽입문을 사용하세요. 이 단계는 카프카 토픽에서 데이터를 영구 저장소 솔루션으로 전환하는 과정을 완료합니다.

```js
-- SQL 삽입 예시
INSERT INTO events VALUES ('{"event_type": "weather", "data": {"temperature": 25, "location": "NYC"}');
```

7단계: 시각화 및 분석

<div class="content-ad"></div>

마침내, 수집하고 저장된 이벤트 데이터로부터 통찰력 있는 시각화와 분석을 위해 데이터베이스 레코드를 쿼리하세요. 수집된 이벤트 데이터로부터 가치 있는 통찰력을 얻기 위해 대시보드를 사용하는 것이 가장 좋습니다.

이 end-to-end 파이프라인은 다양한 구성 요소의 원활한 통합을 보여주며, 이벤트 스트림을 관리하는 Kafka의 유연성과 강력함을 강조합니다.

참고: 본 블로그 게시물에서 제공된 메모 및 정보는 "ETL 및 쉘, Airflow 및 Kafka를 사용한 데이터 파이프라인" 과정 중에 편집되었으며 개인적인 용도를 위한 기본 개요를 제공하기 위한 것입니다.