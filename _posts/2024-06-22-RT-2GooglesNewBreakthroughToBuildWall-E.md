---
title: "RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결"
description: ""
coverImage: "/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png"
date: 2024-06-22 19:34
ogImage: 
  url: /assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png
tag: Tech
originalTitle: "RT-2, Google’s New Breakthrough To Build Wall-E"
link: "https://medium.com/@ignacio.de.gregorio.noblejas/rt-2-googles-new-breakthrough-to-build-wall-e-5c27b1fdf754"
---


안녕하세요! 아래는 Markdown 형식으로 변경된 표입니다.


![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png)


ChatGPT가 2022년 11월에 출시된 이후로, 전 세계가 AI를 중심으로 돌아가는 것 같죠.

하지만 걱정하지 마세요. 이 글은 'ChatGPT는 놀라운 기술이다'를 다시 말하는 글이 아니에요. 오늘은 더 혁명적인 주제에 대해 이야기할 거예요.

바로 Google Deepmind의 새로운 로봇, RT-2에 대해요.

<div class="content-ad"></div>

로봇 팔이라 해도 RT-2는 그 중에서도 뛰어난 능력을 자랑해요. 실제로 RT-2를 만드는 데 구글은 이제까지 본 적 없는 새로운 AI 모델을 만들어야 했죠.

융합된 지능이 이곳에 있습니다.

그리고 아마도 월-이도요.

## 여기에서 함께 나와 AI에 대해 쉽게 배워보세요!

<div class="content-ad"></div>

# 새로운 모델 클래스

지난 6개월 동안 인류가 AI로 이룬 성과는 정말 놀라운 것입니다.

간단히 말해, 우리는 인간과 자연어를 통해 소통하는 기계를 누구나 이용할 수 있게 만들었습니다.

하지만 AI의 잠재력은 단순한 텍스트를 뛰어 넘습니다. 많은 연구자들이 더 큰 목표를 향해 눈치를 돌리고 있습니다.

<div class="content-ad"></div>

## 다중 모드 구축을 향한 길

최근 팟캐스트에서 AI 슈퍼히어로 앤드류 엔지가 컴퓨터 비전이라는 이미지를 처리하고 이해하기 위해 모델을 훈련시키는 AI 분야가 "텍스트 프롬프팅"보다 약 "두 년 뒤"라고 언급했습니다. 그러나 그는 그것이 해당 모델들과 동등한 혁명이 될 것으로 예상했습니다.

그가 "텍스트 프롬프팅"로 언급한 것은 다름 아닌 대형 언어 모델 또는 LLM(Large Language Models)로, 그 중 가장 대표적인 예가 ChatGPT입니다.

그러나 우리 모두가 알다시피, 우리가 능력 면에서 인간 수준의 기계를 구축하려면 우리인간이 다중 모드인것보다 훨씬 더 많은 것이 필요합니다.

<div class="content-ad"></div>

일반적인 용어로 말하면, 우리는 세상에 대한 이해를 텍스트만으로 구축하지 않습니다. 우리에겐 눈이 있고, 귀가 있고, 촉감도 있죠... 우리의 모든 감각이 세상이 무엇인지를 파악하는 데 도움을 줍니다.

실제로 이러한 감각들은 우리가 어릴 때 세상에 대해 배우는 데 도움을 줍니다. 우리가 맨 처음 문장을 읽기 보다 훨씬 이전부터 말이죠.

그러므로 AI 연구자들이 텍스트뿐만 아니라 다른 것들도 처리하는 모델, 혹은 명확히 말하자면 멀티모달 모델을 만들고 싶어하는 것은 자연스러운 일입니다.

그리고 이에 대한 AI 공간에서 가장 흥미로운 혁신 중 하나는 시각-언어 모델 중 하나입니다.

<div class="content-ad"></div>

## VLMs, 더 ‘인간적인’ 기계들

비전-언어 모델(VLMs)은 OpenAI의 CLIP이나 Microsoft의 Kosmos와 같은 모델로, 이름에서 알 수 있듯이 텍스트뿐만 아니라 이미지도 처리하는 모델입니다.

예를 들어, OpenAI의 CLIP은 대조 손실 절차를 따라 이미지와 텍스트 사이에 얽혀 있는 임베딩 공간을 생성함으로써 작동합니다(간단히 말하면 동일한 것을 설명하는 이미지와 텍스트를 모아주는 것이 중요하며 Dall-E와 같은 확산 모델을 구축하는 데 필수적입니다). 그러나 Kosmos와 같은 모델은 아직 탐험하지 않은 잠재력을 보여주는 모델입니다.

Kosmos의 작동 방식은 매우 간단합니다. 이미지나 텍스트(또는 둘 다)를 보내면, 이에 기반하여 텍스트를 제공합니다.

<div class="content-ad"></div>

참고로, Carnegie Mellon University에서 개발한 이름이 GILL인 또 다른 VLM을 아래에서 확인할 수 있습니다. 시각적 프롬프팅이 정확히 무엇인지 명확하게 이해할 수 있습니다:

![GILL](https://miro.medium.com/v2/resize:fit:1056/1*IXxrTUmN37X9vs8dqK85-w.gif)

하지만 이제 Google은 한 단계 더 나아가고 있습니다.

그들은 이러한 모델을 현실 세계로 가져오고 있습니다.

<div class="content-ad"></div>

로봇공학을 더 나은 수준으로 발전시키려는 시도 중에, Google Deepmind는 깨달음을 얻었습니다:

그리고 그것으로, Google은 인공지능 로봇학의 새로운 핵심 요소인 Vision-language-action 모델(VLAs)을 만들었습니다.

간단히 말해, VLAs는 로봇이 이후 이동을 수행하는 데 사용할 수 있는 동작을 출력할 수 있는 모델입니다.

하지만 그것들이 정말 무엇이며, 왜 그렇게 혁신적인 것일까요?

<div class="content-ad"></div>

## Wall-e, 그게 진짜야?

로봇 공학 분야에서 연구자들을 미치게 만드는 것이 있다면, 그것은 분명히 데이터일 겁니다.

사실 데이터의 부재일 때든요.

AI 모델이 데이터의 양과 질에 완전히 의존한다는 점을 고려할 때, 후자는 가능했지만 전자는 정말 악몽이었어요.

<div class="content-ad"></div>

따라서, 로봇공학을 더 나은 수준으로 발전시키기 위해 구글이 베팅을 했습니다:

그들은 웹 규모의 텍스트와 이미지 데이터에 접근할 수 있는 VLM(범용 언어 모델)을 사용해 이러한 모델이 학습한 표현을 로봇으로 전이할 수 있다고 가설을 세웠습니다.

간단히 말해서, 로봇 데이터를 기반으로 세계에 대한 고수준 의미 지식을 갖춘 로봇을 만드는 것이 이루기 어려운 일이라면, 기존의 VLM 모델을 사용하고 고품질의 로봇 데이터로 적합화시키는 것은 어떨까요?

그리고 이제 이를 VLAs(로봇 언어 모델)라고 부를 수 있게 된 것입니다.

<div class="content-ad"></div>

## 행동 예측기

LLMs나 VLMs와 마찬가지로 VLAs도 동일한 일을 수행합니다. 토큰을 예측합니다.

그러나 ChatGPT와 같이 텍스트 토큰을 예측하는 대신, RT-2와 같은 VLAs는 로봇이 수행해야 할 작업을 카메라 관측에 기반하여 예측할 수도 있습니다. 아래 이미지에서 확인할 수 있습니다:

![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_1.png)

<div class="content-ad"></div>

얻은 결과는 우리에게 매우 중요한 두 가지 교훈을 알려줍니다.

## 일반화와 발생

AI로봇 기술의 최첨단인 RT-2를 평가할 때 결과는 매우 명확합니다(RT-2 모델은 녹색과 파란색으로 표시됨):

![RT-2 모델](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_2.png)

<div class="content-ad"></div>

안녕하세요! 항상 그렇듯이, 요점은 세부사항에 있습니다. RT-2를 주의 깊게 관찰하면, 이는 로봇공학의 가장 어려운 두 가지 임무, 즉 일반화와 신흥성에 뛰어난 성과를 보여줍니다.

간단히 말하자면, RT-2는 교육 중 본 적이 없는 상황, 예를 들어 보이지 않는 물체나 환경과 같은 상황에서도 잘 수행하는 능력(일반화)과 VLM 덕분에 얻은 언어 지식의 규모로 학습한 새로운 예상치 못한 능력(신흥성)을 펼쳐냈습니다.

가장 놀라운 점은: 이것이 첫 번째 추론이 가능한 로봇이라는 것입니다.

이 점을 증명하기 위해, 구글이 진행한 6,000건 이상의 시험 중에서 몇 가지 인상적인 행동이 포함되어 있습니다:

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_3.png)

한 예를 들어 ‘추론’으로 볼 수 있는 것을 분석해 봅시다. 맨 위 두 번째 왼쪽을 보면, 로봇은 두 번째 과자봉지의 나쁜 위치를 이해하여 요청에 완벽히 대답할 수 있습니다.

이 작업을 수행하려면, 모델은 카메라로 촬영된 이미지의 밀도가 높은 의미 지식이 필요했습니다. 즉, 탁자 끝에 있는 봉지가 떨어질 수 있다는 것을 이해하면서, 다른 봉지는 그렇지 않다는 것을 알아냈습니다.

비슷하게, 아래 오른쪽 예에서 모델은 당나귀와 문어의 차이를 이해할 뿐만 아니라, “육지 동물”이 당나귀를 의미한다는 것을 생각할 수도 있습니다.

<div class="content-ad"></div>

더 정량적인 관점에서 구글은 모델을 세 가지 기준으로 평가했습니다:

- 심볼 이해
- 추론
- 인간 인식

놀랍게도, RT-2는 이 모든 면에서 우수하게 성과를 거두었으며, 다음에 나온 예시와 같은 동작을 수행할 수 있는 능력을 입증했습니다:

![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_4.png)

<div class="content-ad"></div>

인상적인 점은 RT-2가 알 수 없는 상황에서 잘 작동할 수 있었던 것처럼, 위의 이미지들도 새로운 업무를 수행할 수 있는 능력을 보여줍니다. 다시 말해, 시각 언어 모델을 추가하더라도 새로운 로봇 동작을 생성하는 데는 한계가 있었지만 (논문에서 인정함), 로봇에 풍부한 의미 지식을 전달하여 배치, 물체 인식, 논리 추론과 같은 복잡하고 신흥 개념에 대해 훨씬 더 인식력을 갖도록 만들었습니다.

# 혁신의 바퀴는 계속 회전합니다

RT-2를 본 이후에는 내년 말까지 세계 각국의 제조업체가 이러한 로봇을 사용하여 프로세스를 개선하는 것이 놀라운 일이 아닐 것입니다.

<div class="content-ad"></div>

인공지능 로봇들은 주변 환경에 대해 훨씬 더 인식력을 가지고 있습니다. 그들의 신경망 속에 점점 더 복잡한 세계 모델을 구축하여 하루가 다르게 진화하고 있는데, 이는 수천 년이 걸린 인간들의 진화에 의해 이루어졌던 것과 매우 가까워지고 있습니다.

그러나 많은 사람들은 이러한 모델들이 단순한 확률론적 앵무새에 불과하다고 주장할 것입니다. 이들은 그저 "지능적인 활동"을 단순히 암기하는 것으로 여기는 것이죠.

그러나 가장 이상하고 예상치 못한 경우에도 "지능적으로" 행동하는 기계들을 보면, 이 모델들이 자신이 하는 일을 실제로 이해한다고 주장하지 않을 수 있는 것이 정말로 도전이 됩니다. 이제 이러한 모델들은 "체화된 지능"이라고 묘사하는 것과 같이 지능적인 물리적 행동을 수행할 수 있는 조건에 이르렀습니다.

RT-2에 관한 것을 생각해보면, 이는 바로 진행 중인 로봇 기술이 미래의 몇 달 동안 어떻게 진화할지에 대한 단순한 꼭대기에 지나지 않을 것입니다. 그래서 아마도 영화 '월-E'가 우리 삶에서 생각했던 것보다 그렇게 멀리 떨어져 있지 않다는 생각이 들 수도 있겠네요.

<div class="content-ad"></div>

프레스 릴리스와 연구 논문 링크입니다.