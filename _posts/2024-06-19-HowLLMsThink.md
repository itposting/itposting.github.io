---
title: "LLM의 사고 방식"
description: ""
coverImage: "/assets/img/2024-06-19-HowLLMsThink_0.png"
date: 2024-06-19 19:38
ogImage: 
  url: /assets/img/2024-06-19-HowLLMsThink_0.png
tag: Tech
originalTitle: "How LLMs Think"
link: "https://medium.com/towards-data-science/how-llms-think-d8754a79017d"
---


## 리서치 페이퍼 요약: "Scaling Monosemanticity: Claude 3 Sonnet에서 해석 가능한 기능 추출"

![이미지](/assets/img/2024-06-19-HowLLMsThink_0.png)

안녕하세요! AI 모델이 "어떻게 생각하는지" 궁금한 적이 있으신가요? 기계의 머릿속을 엿볼 수 있다면 어떨까요? Anthropic의 혁신적인 논문이 바로 그것을 탐구합니다. "Scaling Monosemanticity: Claude 3 Sonnet에서 해석 가능한 기능 추출"라는 제목의 연구는 AI의 사고 과정을 이해하고 해석하는 데 초점을 맞추고 있습니다.

연구자들은 Claude 3 Sonnet 모델에서 기능을 추출하여 유명인, 도시, 심지어 소프트웨어의 보안 취약점에 대해 생각한 내용을 보여주는 결과를 얻었습니다. 이는 AI의 머릿속을 엿볼 수 있는 것과 같아서, AI가 이해하고 결정을 내리는 데 사용하는 개념을 드러내줍니다.

<div class="content-ad"></div>

## 연구 논문 요약

논문에서 Anthropic 팀은 Adly Templeton, Tom Conerly, Jonathan Marcus 등을 포함하여 AI 모델을 더 투명하게 만드는 것을 목표로 하였습니다. 그들은 중간 규모의 AI 모델인 Claude 3 Sonnet에 초점을 맞추었으며, 모델의 각 기능에 명확하고 하나의 의미가 있는 monosemanticity를 확장하려고 노력했습니다.

그러나 monosemanticity를 확장하는 것이 왜 중요한 것일까요? 그리고 monosemanticity가 정확히 무엇인가요? 이에 대해 곧 자세히 살펴보겠습니다.

## 연구의 중요성

<div class="content-ad"></div>

AI 모델의 특징을 이해하고 해석하는 것은 매우 중요합니다. 이를 통해 이러한 모델이 결정을 내리는 방식을 파악할 수 있어 더욱 신뢰할 수 있고 개선하기 쉬워집니다. 이러한 특징을 해석할 수 있는 경우, AI 모델의 버그 수정, 정제, 최적화가 더욱 쉬워집니다.

이 연구는 AI 안전에 상당한 영향을 미칩니다. 편견, 속임수, 혹은 위험한 콘텐츠와 관련된 특징을 식별함으로써, 이러한 위험을 줄일 수 있는 방법을 개발할 수 있습니다. 이는 AI 시스템이 일상생활에 점점 더 통합되는 상황에서 윤리적 고려사항과 안전이 중요한 요소로 부각됨에 특히 중요합니다.

이 연구의 주요 이점 중 하나는 대규모 언어 모델(Large Language Model, LLM)이 어떻게 "생각"하는지를 이해하는 방법을 보여준다는 것입니다. 특징을 추출하고 해석함으로써, 이러한 복잡한 모델의 내부 작업에 대한 통찰을 얻을 수 있습니다. 이는 이 모델들이 어떤 결정을 내리는지 이해할 수 있게 해주며, 그들의 "사고 과정"을 엿볼 수 있는 방법을 제공합니다.

## 배경

<div class="content-ad"></div>

이전에 언급된 몇 가지 이상한 용어를 살펴보겠습니다:

단미의미성 (Monosemanticity)
단미의미성은 거대한 건물의 각 자물쇠에 하나씩 특정 열쇠가 있는 것과 같습니다. 이 건물은 AI 모델을 나타내고, 각 자물쇠는 모델이 이해하는 특징이나 개념입니다. 단미의미성에서는 각 열쇠(특징)가 하나의 자물쇠(개념)에 완벽하게 맞습니다. 이는 특정 열쇠가 사용될 때 항상 같은 자물쇠를 열게 된다는 것을 의미합니다. 이 일관성은 모델이 결정을 내릴 때 모델이 무엇에 대해 생각하는지 정확히 이해하도록 도와줍니다. 우리는 어떤 열쇠가 어떤 자물쇠를 열었는지 알기 때문입니다.

희소 오토인코더 (Sparse Autoencoders)
희소 오토인코더는 매우 효율적인 탐정과 같습니다. 클러터링된 큰 방(데이터)에 많은 물건이 흩어져 있는 상황을 상상해보세요. 탐정의 임무는 방에서 발생한 일을 완벽히 설명하는 몇 가지 주요 물건(중요한 특징)을 찾는 것입니다. "희소"라는 용어는 이 탐정이 가능한 한 적은 단서를 사용하여 미스터리를 해결하려고 하며, 핵심적인 증거들에만 집중한다는 것을 의미합니다. 이 연구에서 희소 오토인코더는 이 탐정처럼 작용하여 AI 모델에서 명확하고 이해하기 쉬운 특징을 식별하고 추출하는 데 도움을 줍니다. 모델 내부에서 무슨 일이 벌어지고 있는지 쉽게 이해할 수 있게 해줍니다.

자세한 내용은 Andrew Ng의 오토인코더에 대한 유용한 강의 노트를 확인해 보세요.

<div class="content-ad"></div>

## 이전 작업

이전 연구는 희소 오토인코더를 사용하여 작은 AI 모델에서 해석 가능한 특징을 추출하는 방법을 탐구함으로써 기초를 다졌습니다. 이러한 연구들은 희소 오토인코더가 더 간단한 모델에서 유의미한 특징을 효과적으로 식별할 수 있다는 것을 보여주었습니다. 그러나 이러한 방법이 Claude 3 Sonnet과 같은 더 크고 복잡한 모델로 확장될 수 있는지에 대한 중요한 우려사항이 있었습니다.

이전 연구는 희소 오토인코더가 작은 모델에서 주요 특징을 식별하고 표현할 수 있는지를 증명하는 데 초점을 맞추었습니다. 이들은 추출된 특징이 의미 있고 해석 가능하다는 것을 성공적으로 입증하였습니다. 그러나 주요 제한 사항은 이러한 기술이 더 단순한 모델에서만 테스트되었다는 것이었습니다. 확대는 필수적이었습니다. Claude 3 Sonnet과 같이 더 크고 복잡한 모델은 더 복잡한 데이터와 작업을 다루기 때문에, 추출된 특징이 동일한 수준의 명확성과 유용성을 유지하는 것이 어려워졌습니다.

본 연구는 이러한 기초 위에 건설하여 이러한 방법을 보다 발전된 AI 시스템으로 확장하는 것을 목표로 합니다. 연구진들은 더 큰 모델의 높은 복잡성과 차원을 다루기 위해 희소 오토인코더를 적용하고 수정했습니다. 확장의 어려움에 대처함으로써, 이 연구는 더 복잡한 모델에서도 추출된 특징이 명확하고 유용성을 유지하기를 보장하여, AI 의사 결정 과정의 이해와 해석을 발전시키고자 합니다.

<div class="content-ad"></div>

## 희소 오토인코더의 확장

클로드 3 소네트와 같은 대규모 모델과 함께 희소 오토인코더를 확장하는 것은 작고 지역적인 도서관에서 광대한 국가 아카이브를 관리하는 것과 같습니다. 더 큰 데이터셋의 규모와 복잡성을 처리하기 위해 작은 컬렉션에 잘 작동했던 기술들을 조정해야 합니다.

희소 오토인코더는 데이터의 주요 특징을 식별하고 표현하면서 활성 특징의 수를 낮추는 것을 목표로 합니다. 마치 수천 권의 책 중 정확히 어떤 몇 권이 당신의 질문에 답할 것인지를 아는 사서와 비슷합니다.

![이미지](/assets/img/2024-06-19-HowLLMsThink_1.png)

<div class="content-ad"></div>

두 가지 주요 가설이 이 스케일링을 이끌고 있습니다:

선형 표현 가설
AI가 이해하는 각 개념을 나타내는 각 별의 거대한 밤하늘 지도를 상상해보세요. 이 가설은 각 개념(또는 별)이 모델의 활성화 공간에서 특정 방향으로 정렬된다는 것을 제안합니다. 별 한 개를 가리키는 공간을 통과하는 선을 그으면, 그 별을 그 방향으로 고유하게 식별할 수 있다는 것과 거의 같습니다.

중첩 가설
밤하늘 비유를 바탕으로 한 이 가설은 AI가 이러한 방향을 사용하여 거의 수직선을 사용하여 방향보다 더 많은 별을 매핑할 수 있다는 것과 같습니다. 이를 통해 AI는 이러한 방향을 효율적으로 결합하는 고유한 방법을 찾아 정보를 효율적으로 실을 수 있습니다. 이는 마치 서로 다른 층에 섬세하게 매핑하여 하늘에 더 많은 별을 맞추는 것과 같습니다.

이러한 가설들을 적용함으로써 연구자들은 효과적으로 희소 오토인코더를 Claude 3 Sonnet과 같은 대규모 모델과 함께 작동하도록 확대할 수 있게 되었으며, 이를 통해 데이터의 간단하고 복잡한 특징을 포착하고 표현할 수 있게 되었습니다.

<div class="content-ad"></div>

## 모델 훈련

수많은 증거 중에서 중요한 증거를 찾을 수 있도록 거대한 도서관에서 탐정들을 훈련하는 것을 상상해보세요. 이것은 연구자들이 Claude 3 Sonnet이라는 복잡한 AI 모델과 작업할 때 희소 오토인코더(SAEs)를 이용한 일과 유사합니다. 이들은 이들 탐정들이 Claude 3 Sonnet 모델이 나타내는 더 크고 복잡한 데이터 세트를 처리할 수 있도록 훈련 기술을 수정해야 했습니다.

연구자들은 모델의 중간층에서 잔여 스트림 활성화에 SAE를 적용하기로 결정했습니다. 중간층을 탐정의 조사에서 중요한 체크포인트로 생각해보세요. 여기서 많은 흥미로운 추상적인 단서들을 찾을 수 있습니다. 그들은 이 포인트를 선택한 이유는 다음과 같습니다:

- 크기가 작음: 잔여 스트림은 다른 층보다 작아서 계산 자원 면에서 더 저렴합니다.
- 교차 층 중첩 완화: 이는 서로 다른 층에서의 신호들이 섞이는 문제를 가리킵니다. 이는 서로 다른 맛들이 섞여 구별하기 어려워지는 것과 유사합니다.
- 추상적인 특징이 풍부함: 중간층은 흥미로운 고수준 개념을 포함하고 있을 가능성이 높습니다.

<div class="content-ad"></div>

팀은 SAE의 세 가지 버전을 훈련시켰어요. 각각은 다른 특징 처리 능력을 가지고 있었어요: 1M 특징, 4M 특징 및 34M 특징. 각 SAE에 대해 목표는 정확도를 유지하면서 활성 특징의 수를 낮추는 것이었어요:

- 활성 특징: 평균적으로 언제든지 300개 미만의 특징이 활성화되어 있었고, 모델 활성화의 분산을 적어도 65% 설명했어요.
- 사용되지 않는 특징: 활성화되지 않는 특징입니다. 1M SAE에는 대략 2%의 사용되지 않는 특징이 발견되었고, 4M SAE에는 약 35%가 있었으며 34M SAE에는 65%가 있었어요. 미래의 개선 사항은 이러한 수를 줄이는 것을 목표로 할 거예요.

## 스케일링 법칙: 훈련 최적화

목표는 평균 제곱 오차(MSE)와 L1 패널티를 결합한 손실 함수를 사용하여 재구성 정확도와 활성 특징의 수를 균형있게 유지하는 것이었어요.

<div class="content-ad"></div>

또한, 그들은 계산 예산 내에서 최적의 훈련 단계와 피처 개수를 결정하는 데 도움이 되는 스케일링 법칙을 적용했습니다. 본질적으로, 스케일링 법칙은 계산 자원을 늘릴수록 피처 개수와 훈련 단계 수가 예측 가능한 패턴을 따라 증가해야 함을 알려줍니다. 이는 종종 제곱 법칙을 따르게 됩니다.

그들은 계산 예산을 증가시킴에 따라 최적의 피처 개수와 훈련 단계도 제곱 법칙을 따라 증가함을 발견했습니다.

![image](/assets/img/2024-06-19-HowLLMsThink_2.png)

그들은 최상의 학습 속도도 제곱 법칙 트렌드를 따랐으며, 더 큰 실행에 적합한 속도를 선택하는 데 도움이 되었습니다.

<div class="content-ad"></div>

## 수학적 기반

희소 오토인코더 모델의 핵심 수학적 원리는 활성화를 해석 가능한 특징으로 분해하는 방법을 이해하는 데 필수적입니다.

인코더
인코더는 학습된 선형 변환 다음에 ReLU 비선형성을 적용하여 입력 활성화를 고차원 공간으로 변환합니다. 이는 다음과 같이 나타낼 수 있습니다:

![이미지](/assets/img/2024-06-19-HowLLMsThink_3.png)

<div class="content-ad"></div>

여기서 W^enc와 b^enc는 인코더의 가중치와 편향을 나타내고, fi(x)는 특징 i의 활성화를 나타냅니다.

디코더
디코더는 다른 선형 변환을 사용하여 특징에서 원래 활성화를 재구성하려고 시도합니다:


![image](/assets/img/2024-06-19-HowLLMsThink_4.png)


W^dec와 b^dec는 디코더의 가중치와 바이어스입니다. 용어 fi(x) W^dec는 재구성에 특징 i의 기여를 나타냅니다.

<div class="content-ad"></div>

손실
모델은 재구성 오차와 희소성 패널티를 최소화하기 위해 훈련됩니다:

![image](/assets/img/2024-06-19-HowLLMsThink_5.png)

이 손실 함수는 재구성이 정확하고 (오차의 L2 노름을 최소화) 동시에 활성 특성의 수를 낮게 유지합니다 (계수 λ로 강제되는 L1 정규화 항이 있다).

## 해석 가능한 특성

<div class="content-ad"></div>

연구 결과, Claude 3 소네트 모델 내에 해석 가능한 다양한 특징들이 발견되었습니다. 이는 추상적이고 구체적인 개념을 포괄하며, 모델의 내부 프로세스 및 의사 결정 패턴에 대한 통찰을 제공합니다.

추상적 특징: 이러한 특징에는 모델이 이해하고 정보를 처리하는 데 사용하는 고수준 개념이 포함됩니다. 감정, 의도와 같은 주제, 과학 또는 기술과 같은 보다 넓은 범주와 같은 예시가 있습니다.

구체적 특징: 이러한 특징은 이름이나 지리적 위치, 특정 물건과 같이 더 구체적이고 실질적입니다. 이러한 특징들은 식별 가능한 현실 세계 개체에 직접적으로 연결될 수 있습니다.

예를 들어, 모델에는 잘 알려진 인물에 대한 언급에 반응해 활성화되는 특징들이 있습니다. "알버트 아인슈타인"에 대한 특정 특징이 있을 수 있으며, 이는 텍스트가 그에 대해 이나 그의 물리학 연구에 언급될 때마다 활성화됩니다. 이러한 특징은 모델이 연결을 만들고 아인슈타인에 대한 맥락적으로 관련된 정보를 생성하는 데 도움을 줍니다.

<div class="content-ad"></div>


<img src="/assets/img/2024-06-19-HowLLMsThink_6.png" />

Similarly, there are features that respond to references to cities, countries, and other geographical entities. For example, a feature for “Paris” might activate when the text talks about the Eiffel Tower, French culture, or events happening in the city. This helps the model understand and contextualize discussions about these places.

The model can also identify and activate features related to security vulnerabilities in code or systems. For example, there might be a feature that recognizes mentions of “buffer overflow” or “SQL injection,” which are common security issues in software development. This capability is crucial for applications involving cybersecurity, as it allows the model to detect and highlight potential risks.

<img src="/assets/img/2024-06-19-HowLLMsThink_7.png" />


<div class="content-ad"></div>

편견과 관련된 기능들도 식별되었는데, 이는 인종, 성별 또는 다른 형태의 편견을 탐지하는 것을 포함합니다. 이러한 기능들을 이해함으로써, 개발자들은 편향된 결과물을 완화하는 데 노력할 수 있어서 AI가 더 공정하고 평등하게 행동하도록 할 수 있습니다.

![image](/assets/img/2024-06-19-HowLLMsThink_8.png)

이해 가능한 이러한 기능들은 모델이 구체적이고 넓은 개념을 포착하고 활용하는 능력을 나타냅니다. 이러한 기능들을 이해함으로써 연구자들은 Claude 3 Sonnet이 정보를 처리하는 방식을 더 잘 이해할 수 있어서 모델의 행동을 더 투명하고 예측 가능하게 만들 수 있습니다. 이러한 이해는 AI 신뢰성, 안전성 및 인간의 가치와 조화를 이루는 데 중요합니다.

## 결론

<div class="content-ad"></div>

이 연구는 클로드 3 소네트 모델의 내부 작동을 이해하고 해석하는 데 중요한 발전을 이루었습니다.

이 연구는 클로드 3 소네트에서 추상적이고 구체적인 특징을 성공적으로 추출하여 AI의 의사 결정 과정을 더 투명하게 만들었습니다. 예시로는 유명인물, 도시 및 보안 취약점에 대한 특징이 있습니다.

이 연구는 보안 취약점, 편향 및 속임수 행위를 탐지하는 등 AI 안전과 관련된 기능을 확인했습니다. 이러한 특징을 이해하는 것은 더 안전하고 신뢰할 수 있는 AI 시스템을 개발하는 데 매우 중요합니다.

해석 가능한 AI 기능의 중요성은 과대평가할 수 없습니다. 이 기능들은 AI 모델을 디버그하고 개선하며 최적화하는 데 우리의 능력을 향상시킵니다. 이를 통해 더 나은 성능과 신뢰성을 제공합니다. 또한, 이러한 기능들은 투명하게 운영되고 인간의 가치와 특히 안전과 윤리 분야에서 인간의 가치와 일치하는 AI 시스템을 보장하기 위해 필수적입니다.

<div class="content-ad"></div>

# 참고 자료

- Anthropic. Adly Templeton 등. “Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.” Anthropic Research, 2024.
- Ng, Andrew. “Autoencoders: 개요 및 응용.” 강의 노트, 스탠포드 대학교.
- Anthropic. “AI 안전에 대한 핵심적인 관점.” Anthropic 안전 가이드라인, 2024.