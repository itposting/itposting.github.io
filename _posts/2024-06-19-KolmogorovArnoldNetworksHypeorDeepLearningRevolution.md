---
title: "콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가"
description: ""
coverImage: "/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png"
date: 2024-06-19 20:01
ogImage: 
  url: /assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png
tag: Tech
originalTitle: "Kolmogorov–Arnold Networks: Hype or Deep Learning Revolution?"
link: "https://medium.com/@machine-learning-made-simple/understanding-kolmogorov-arnold-networks-possible-successors-to-mlps-4f2a912e69df"
---


## 더 좋은 해석 가능성, 작은 네트워크 크기 및 학습 가능한 활성화 함수가 MLPs를 무너뜨리게 될까요?

# 주요 내용 (기사 개요)

Substack 그룹 채팅, LinkedIn 등에서 생각을 공유해 준 모든 분들께 감사드립니다. 거기서 우리가 가지는 대화들을 모두 사랑하고, 앞으로도 계속 많은 대화를 이어갈 예정입니다.

Kolmogorov–Arnold Networks 및 그들이 과학적 기능을 모델링하는 데 특히 유리할 수 있는 잠재력에 대해 많은 이야기가 되었습니다. 본 기사에서는 KANs 및 그들이 새로운 세대의 딥 러닝에서의 타당성에 대해 탐구할 것입니다.

<div class="content-ad"></div>

![KANs 이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png)

먼저, KANs와 그들을 가능하게 하는 이론에 대해 간단히 살펴보겠습니다.

콜모고로프-아놀드 표현 정리: 모든 무서운 방정식과 정의를 건너뛰고 간단한 설명으로 해결합시다. KART는 여러 입력을 가진 연속 함수는 단일 입력 (사인이나 제곱과 같은)의 간단한 함수들을 결합하고 더하는 것으로 생성할 수 있다는 것을 말합니다. 예를 들어, 다중 변수 함수 f(x,y)= x*y는 ( (x + y)² — (x² +y²) ) / 2로 표현할 수 있습니다. 이는 덧셈, 뺄셈 및 제곱 연산(단일 입력 함수)만 사용합니다. 실제 KART는 뺄셈을 덧셈으로 다시 구성하는 것이 포함되지만 여기서는 이를 간단하게 유지하겠습니다.

KANs (콜모고로프-아놀드 네트워크) - 기존의 MLP (다층 퍼셉트론)과 달리 고정된 노드 활성화 함수를 갖는 MLPs와 달리, KANs는 가장자리에 학습 가능한 활성화 함수를 사용하여 선형 가중치를 비선형 가중치로 대체합니다. 이는 KANs를 보다 정확하고 해석 가능하게 만들어주며, 특히 학문적 응용 및 일상생활에서 자주 발견되는 희소한 합성 구조와 관련이 있는 함수에 유용합니다.

<div class="content-ad"></div>

![Image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_1.png)

와우, 구성 요소 구조가 뭔지 궁금하셨나요? 제처럼 gawaar(무지한 사람)이라면 "희소한 구성 요소 구조"라는 말에 혼란스러울 수 있습니다. 그럼 이건 뭐길래요, 그렇게 중요한가요? 간단히 말하자면- 특정 함수가 몇 개의 간단한 함수로 구성되어 있고 각 함수가 입력 변수 중 일부에만 의존할 때 그 함수는 희소한 구성 요소 구조를 갖고 있습니다. 예를 들어, 함수 f(x, y, z) = sin(x) * exp(y) + z는 구성 요소가 희소합니다.

- 세 가지 간단한 함수인 sin(x), exp(y), z로 이루어져 있습니다.
- 각 간단한 함수는 하나의 입력 변수(x, y, z 중 하나)에만 의존합니다.

비교적으로, f(x, y, z) = x² * y³ + sin(x + y + z)와 같은 함수는 덜 희소하다고 볼 수 있습니다. 왜냐하면 더 복잡한 연산(x² * y³와 같은)이 필요하며 sin 함수에서 모든 세 가지 입력 변수를 결합해야하기 때문입니다.

<div class="content-ad"></div>

수학적으로 엄격한 정의를 원하는 경우, "딥러닝의 기초: 계산 가능한 함수의 복합 희박성"에서 발견한 정의는 다음과 같습니다.

그들의 중요성을 요약하면, 저자들은 과학과 현실에서 만나는 대부분의 함수가 더 단순한 구조를 갖고 있어서, 그것들을 모델링하기 위해 MLP에 대안으로 KAN이 매력적이라고 언급합니다. 아래 표는 이 주장을 뒷받침하는 것으로 보입니다.

![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_2.png)

KAN과 스플라인- KAN의 또 다른 흥미로운 특성은 스플라인의 활용입니다. 우리의 목적을 위해, 스플라인은 곡선에 맞게 구부러질 수 있는 유연한 자 등처럼 생각해야 합니다. 이들은 부드럽게 연결된 여러 다항 조각들로 이루어져 있습니다.

<div class="content-ad"></div>

![이미지](https://miro.medium.com/v2/resize:fit:576/0*9JUhBPlynwlgU86F.gif)

스플라인을 활성화 함수로 사용함으로써 KAN은 입력 변수 간의 복잡한 관계를 학습할 수 있으면서 해석 가능성과 지역적인 제어를 유지할 수 있습니다. 뒤에 나오는 기사에서는 스플라인에 대해 더 자세히 설명하겠습니다.

KAN에는 장단점이 있습니다. 먼저, 세 가지 주목할 만한 단점이 있습니다.
- 연구 부족 - 새로운 아이디어인만큼 이해하지 못하는 것은 용서될 수 있지만, 새로운 아이디어를 살펴볼 때는 항상 이를 기억하는 것이 중요합니다. AI 연구 분야에서 천재아이디어에서 비실용적인 물건이 된 사례가 흔한 일입니다. 우리가 알기론 KAN에는 특정한 시점을 넘어서 발전하기 어렵게 하는 근본적인 장애물이 있을 수도 있습니다.
- 시장 적합성 - 현재는 트랜스포머와 NN에 특화된 하드웨어가 만들어지고 있습니다. 이러한 개발은 KAN에 대한 강력한 선택 편향을 만들 수 있으며, 결국 그들의 수용을 방해할 수 있습니다 (이 훌륭한 의견을 제공해준 Patrick McGuinness에게 감사드립니다).
- 훈련 속도가 느림 - KAN 훈련은 NN보다 10배 느립니다. 작업 중이신 내용에 따라 이는 그들을 부정할 수도 있고 큰 문제가 되지 않을 수도 있습니다. 수용/연구가 더 많아짐에 따라 해결되기도 할 것입니다 (혹시 여러분 중 누가 이 문제를 해결하게 된다면 좋겠네요). 하지만 현재로서는 더 많은 주류 방향으로의 수용을 막을 것이며 이는 규모 측면에서 주도되고 있는 곳에 있습니다.

<div class="content-ad"></div>

KANs은 여러 가지 장점을 가지고 있습니다:

- 향상된 정확도: 다양한 작업에 대해 MLP보다 적은 매개변수로 더 낮은 RMSE 손실을 달성할 수 있습니다. 저자들은 몇 가지 놀라운 결과를 보여주며 Deepmind를 이기는 것을 포함합니다-

![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_3.png)

- 유리한 스케일링 법칙: 더 빠른 신경 스케일링 법칙을 나타내며, 모델 크기가 커짐에 따라 성능 향상이 더 큽니다.

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_4.png)

- Catastrophic forgetting을 완화: 일반 MLPs는 이전에 학습한 내용을 새로운 내용으로 덮어쓰기 때문에 NNs가 이전 입력을 '잊어버리는' 문제를 발생시킵니다. KANs는 지속적인 학습을 용이하게 하는 지역 가용성을 활용합니다. 제가 Spines에 대해 언급할 때 이 기계적 측면에 대해 자세히 이야기하겠습니다. tl;dr 섹션에서- 1년 전 대화에서 일반 NNs의 주요 제한 사항으로 이를 지적한 첫 번째 사람으로 Dr. Bill Lambos에게 특별한 감사를 전하고 싶습니다. 그의 예측과 계산 신경과학자이자 생물학자(최초의 AI 전문가)로서의 통찰력은 여러 차례 옳았습니다.

![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_5.png)

- 설명 가능성- KANs는 더 설명 가능하며, 특정 분야에 매우 유리합니다. 저희 그룹 채팅 참가자 중 한 명이 그룹 채팅에서 다음과 같이 말했습니다-

<div class="content-ad"></div>

- 상호작용성 - 사람들이 KAN과 조작하여 다양한 결과를 달성할 수 있는 근본적인 수준에서 상호작용할 수 있습니다. 네트워크 내에서 도메인 전문 지식을 입력하는이 사용법은 나에게 매우 유망해 보이며, 더 많은 사람들이 그에 대해 이야기하지 않는 것에 놀랍습니다. 제 반응이 과하게 반응인가요? 당신의 생각을 듣고 싶어요

![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_6.png)

이러한 장점들은 미래 탐사에 매력적인 전망을 제공합니다.

이 기사의 나머지 부분은 KAN이 작동하는 속성을 조사할 것입니다. 기대하는 대로, 논의할 많은 포인트가 있으며, 때로는 이 기사가 약간 압도적일 수 있습니다. 이 분해 방법을 다루기 위해 우리는 적어도 느림을 준다는 구태의 없는 말을 사용할 것입니다. 우리는 현재의 신경망 기반 아키텍처에 대한 심층 학습의 이론적 기초와 그 파급 효과로 시작합시다.

<div class="content-ad"></div>

저는 다양한 컨설팅 및 자문 서비스를 제공합니다. 함께 일할 수 있는 방법을 탐색하고 싶다면 여기 있는 내 소셜 미디어를 통해 저에게 연락하거나 이 이메일에 답장해주세요.

15만 명 이상의 기술 리더와 함께하고 AI의 가장 중요한 아이디어에 대한 통찰을 무료 뉴스레터인 "AI Made Simple"을 통해 받아보세요.

# 이론적 기반

## A. 유니버설 근사 정리와 딥 러닝

<div class="content-ad"></div>

Universal Approximation Theorem (UAT)는 시그모이드 활성화 함수를 사용하는 뉴럴 네트워크가 한정된 수의 뉴런을 가진 단일 은닉층을 포함하여, 임의의 정확도로 제한된 세트의 연속 함수를 근사화할 수 있다고 말합니다. 우리가 해야 할 일은 비선형 함수의 다양한 조합을 계속해서 쌓아가는 것 뿐입니다. 그렇게 함으로써 우리가 원하는 함수의 근사치를 얻을 수 있습니다. (ML 모델에 데이터를 제공할 때, 우리는 모든 특징과 그들의 타겟과의 관계를 모델링하는 함수를 원합니다.)

![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_7.png)

나중에 다른 비선형성으로 확장되었으며, 이것이 왜 큰 신경망이 잘 작동하는지에 중요한 역할을 합니다. 큰 신경 네트워크 → 더 좋은 복잡한 비선형 관계를 모델링하기 위한 능력 → 연속 함수에 대한 더 가까운 근사값.

계속하기 전에, 많은 사람들이 종종 간과하는 중요한 점이 있습니다. 모든 데이터셋/도메인이 연속 함수로 모델링될 수 있는 것은 아니라는 것입니다. 예를 들어, 더욱 울퉁불퉁한 의사결정 경계를 가진 특정 데이터셋이 있습니다. 이러한 경우, 신경망은 보다 매끄러운 의사결정 경계를 선호하므로 랜덤 포레스트와 같은 트리 기반 알고리즘이 더 나은 선택일 수 있습니다.

<div class="content-ad"></div>

위 논문의 부록에서 저자들은 위 시각화에 관한 다음 문장을 제시했습니다-

이제 이산적인 결정 공간을 다루는 매우 중요한 사용 사례도 있습니다. 여기서 신경망과 그래디언트 기반 방법은 대단히 Yamcha'ed 됩니다. 정부 검열에 대항하기 위한 제네바 프로젝트가 좋은 예입니다. 인공지능은 정부 검열을 회피하기 위해 창의적인 방법으로 4가지 기능을 결합하려고합니다. 이에 대해 자세히 알아 보려면 저희의 기사인 "AI를 사용하여 검열에 맞서는 방법" (이 프로젝트에 대한 깊은 살펴 보기) 또는 "인터넷을 다시 제어하는 방법" (인터넷의 기관적 조작의 영향을 줄이기 위해 적용할 수있는 몇 가지 AI 기술에 대한 살펴 보기)를 읽으실 수 있습니다.

<div class="content-ad"></div>

이것이 중요한 이유는 신경망 및 그 구현에 대한 토론에서 종종 간과되는 세세한 부분이기 때문에 강조하고 있습니다. KAN(콜모고로프-아놀드 Representation Theorem)이 더 일반화되면서 비슷한 일이 발생할 수 있다는 걱정이 듭니다. 이 중요한 포인트를 언급했으니, KAN의 이야기로 넘어가겠습니다.

## B. 콜모고로프-아놀드 Representation Theorem 이해하기

콜모고로프-아놀드 Representation Theorem(KAT)은 바운드 영역에서의 임의의 연속 다변수 함수를 연속 단변수 함수의 유한 조합과 덧셈 연산으로 표현할 수 있다는 것을 제시합니다. 이 정리는 원칙적으로 고차원 함수를 학습하는 것을 단변수 함수의 다항수만 학습하는 것으로 줄일 수 있다는 것을 시사합니다. (비록 이러한 1D 함수 중 일부는 매끄럽지 않거나 프랙탈이거나 학습할 수 없을 수도 있습니다.)

<img src="/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_10.png" />

<div class="content-ad"></div>

최근 연구에 따르면, 과학 응용 프로그램에서 많이 사용되는 다변량 함수들은 매끄럽고 희소한 구조를 가지며, 효과적인 실제 구현을 가능하게 합니다. 이것이 왜 KAN이 우리 과학계 사람들 사이에서 인기가 높아 보이는지 알겠죠. 이 부분은 이해하셨으니, 이제 KART의 매우 흥미로운 함의로 넘어가 봅시다.

## C. KAN은 차원의 저주에 강하게 대응할 수 있을 것입니다

차원의 저주란 입력 차원이 증가함에 따라 주어진 정확도를 달성하기 위해 필요한 데이터 포인트 또는 모델 파라미터의 급격한 증가를 가리킵니다. 이 문제는 고차원에서 데이터 포인트가 서로 점점 더 희소하고 멀어지기 때문에 발생하며, 기저 관계를 캡처하기 위해 더 많은 데이터와 모델 복잡성이 필요해집니다.

KAN은 단변량 함수에 의존하고 합성 구조를 활용할 수 있기 때문에 차원의 저주를 완화할 수 있는 잠재력이 있습니다. 이것은 정말 멋지지만, 저는 현재 더 큰 규모와 더 많은 다양성이 통합된 상황에서 어떻게 잘 유지될지에 대해 의심하고 있습니다. 그럼에도 불구하고, 그들이 어떻게 진전될지에 대해 조심스럽게 낙관적입니다.

<div class="content-ad"></div>

기반이 마련되었으니 이제 실제 KAN 구조와 그 작동을 이루는 다른 부분들에 대해 이야기해 보겠습니다.

![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_11.png)

# 콜모고로프-아놀드 네트워크(KANs): 심층적인 탐구

## A. KANs의 아키텍처

<div class="content-ad"></div>

페이퍼의 2.2절에는 많은 정의와 표기법이 나와 있어서, 한 걸음씩 세밀히 설명해보겠습니다. 먼저, 일부 배경 정보-

![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_12.png)

가장 중요한 부분은 마지막 강조 부분입니다- 우리는 더 넓고 깊은 KANs가 필요합니다 (smh, 이제는 인공지능 모델을 몸매 비하하는 걸 믿을 수가 없네). 그렇다면 우리가 KANs를 어떻게 강화할 수 있을까요? 임의로 큰 신경망의 비밀은 내일이 없는 것처럼 레이어를 쌓을 수 있는 능력에 있습니다. 그저 마음대로 레이어를 쌓아올리기가 KANs로는 문제인 이유는, 우리가 레이어에 대응하는 유사한 개념이 부족하기 때문입니다. 그래서, KAN 레이어에 대한 정의를 찾아보고, 이것을 확장하여 더 큰 chonky bois를 얻어보겠습니다.

우리는 KAN 레이어를 다음과 같이 정의할 수 있습니다-

<div class="content-ad"></div>

```
![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_13.png)

더 깊은 KAN을 만들어봅시다.

KAN의 형태는 배열로 표시됩니다: [n0, n1, …, nL], 여기서 ni는 i번째 레이어의 노드 수를 의미합니다. 차원이 n0인 입력 벡터 x0가 주어지면, L개 레이어로 구성된 KAN 네트워크는 출력을 다음과 같이 계산합니다:

KAN(x) = (ΦL−1 ◦ ΦL−2 ◦ … ◦ Φ1 ◦ Φ0)x. 이는 출력이 입력 레이어부터 시작하여 각 레이어의 활성화 함수를 순차적으로 적용하여 얻어진다는 의미입니다.
```

<div class="content-ad"></div>

아래와 같이 다시 작성할 수도 있습니다.

![Image 1](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_14.png)

우리의 작업은 미분 가능하기 때문에 backprop으로 KAN을 학습할 수 있습니다.

![Image 2](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_15.png)

<div class="content-ad"></div>

KAN 레이어를 최적화하는 중요한 몇 가지 팁이 있어요.

잔차 활성화 함수. 우리는 잔차 연결과 비슷한 기저 함수 𝑏⁢(𝑥)(residual connections)를 포함하여 활성화 함수 𝜙⁢(𝑥)가 기저 함수 𝑏⁢(𝑥)과 스플라인 함수의 합으로 정의됩니다:

𝜙⁢(𝑥)=𝑤⁢(𝑏⁢(𝑥)+spline⁢(𝑥)).(2.10)

우리는 다음과 같이 설정합니다- 𝑏⁢(𝑥)=silu⁢(𝑥)=𝑥/(1+𝑒^−𝑥)(2.11)

<div class="content-ad"></div>

일반적으로, spline⁢(𝑥)는 B-스플라인의 선형 조합으로 매개화됩니다. 이 조합은 다음과 같습니다:

![B-spline formula](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_16.png)

초기화: "각 활성화 함수는 spline⁢(𝑥)≈0으로 초기화됩니다. 𝑤는 Xavier 초기화에 따라 초기화되며, 이 초기화는 MLP의 선형 레이어를 초기화하는 데 사용됩니다."

스플라인 격자의 업데이트: "입력 활성화에 따라 각 격자를 업데이트하여, 활성화 값이 훈련 중 고정된 영역을 벗어나는 문제를 해결합니다."

<div class="content-ad"></div>

KAN이 기본적으로 더 복잡하기 때문에, 동일 크기의 MLP보다 매우 많은 매개변수를 가지게 됩니다. 간단히 말하자면, 다음과 같은 네트워크를 가정해 봅시다.
1) 깊이가 L인,
2) 너비가 n0 = n1 = · · · = nL = N 인 층,
3) 각 spline이 주로 k(일반적으로 k = 3)차이며 G 구간(G + 1개의 그리드 포인트)에서 

![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_17.png)

하지만, KAN은 보통 MLP보다 훨씬 작은 𝑁이 필요합니다.

<div class="content-ad"></div>

- 매개변수를 줄임으로써,
- 더 나은 일반화를 달성합니다

![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_18.png)

- 해석 가능성을 용이하게 함- 더 작은 네트워크는 읽고 확인하기 쉽습니다.

지금까지 오셨으면, 확실히 KANs에서 많이 보이는 스플라인을 알아차릴 수 있었을 것입니다. 그리고 당신은 틀리지 않았습니다- 스플라인은 KANs의 매우 중요한 부분입니다. 그래서 다음으로 그것들에 대해 논의할 것입니다.

<div class="content-ad"></div>

## B. 스플라인 매개 변수화 및 KANs

여러 점을 통과하는 매끄러운 곡선을 그리려고 상상해보세요. 컴퓨터에게 원하는 대로 곡선을 그리도록 하는 방법은 무엇일까요? 여기서 B-스플라인이 등장합니다. 이들은 본질적으로 "매듭(knots)"이라고 불리는 점 세트 위에 정의된 부드럽고 조각으로 나뉘는 다항식 곡선입니다.

![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_19.png)

이러한 매듭은 곡선을 세그먼트로 나누고, 각 세그먼트 내에서 B-스플라인은 특정 차수의 다항식입니다. 매력은 이러한 세그먼트들이 매끄럽게 연결되어 원하는 수준의 부드러움을 가진 연속적인 곡선을 만든다는 데 있습니다.

<div class="content-ad"></div>

B-스플라인은 매우 멋지기 때문에 그들의 유연성에서 Camavinga의 스타일을 띠고 있습니다. 매우 부드러운 곡선을 원한다면 더 높은 차수의 B-스플라인을 사용해야 합니다. 곡선의 모양에 대한 유연성과 제어를 더 원한다면 매듭의 위치를 조절할 수 있습니다. 이러한 이유로 B-스플라인은 다양한 함수를 표현하는 데 놀라운 유연성을 제공합니다. 그래서 다음 번에 "문화인" 여러분들 중 한 분이 선호하는 애니메이션, AI 또는 심지어 포토샵으로 가공한 모델에 대한 갈망이 생길 때, B 스플라인의 신에게 감사하는 것을 잊지 마세요.

B-스플라인은 KANs에서 중요한 역할을 합니다. 왜냐하면 엣지에서 배우는 활성화 함수를 표현하는 강력하고 해석 가능한 방식을 제공하기 때문입니다. ReLU나 시그모이드와 같은 고정된 활성화 함수 대신 KANs는 이러한 활성화 함수를 근사하기 위해 B-스플라인을 사용합니다. 이것은 여러 가지 이점을 제공합니다:

- 정확성: B-스플라인은 고정성이 높은 복잡한 비선형 함수를 높은 정확도로 근사할 수 있습니다.
- 지역 제어: B-스플라인의 계수를 변경하면 조정된 매듭 부근에서만 곡선의 모양에 영향을 줍니다. 이는 전체 네트워크 구조에 영향을 미치지 않고 활성화 함수의 동작을 세세하게 제어할 수 있게 합니다.

![이미지](https://miro.medium.com/v2/resize:fit:1400/0*OhuCnxJme6EnYIWX.gif)

<div class="content-ad"></div>

- 해석 가능성: B-스플라인은 구간 다항식으로 구성되어 있기 때문에 블랙박스 신경망보다 시각적으로 직관적이며 이해하기 쉽습니다. 이는 학습된 활성화 함수가 더 해석 가능하며 과학적 발견을 용이하게 합니다. "우리는 KAN이 매듭 이론(4.3절)과 응축물질 물리의 상 전이 경계(4.4절) 모두를 (재)발견할 수 있다는 것을 입증합니다. KAN은 정확성(마지막 절)과 해석 가능성(이 절)으로 인해 AI+과학의 기초 모델이 될 수 있습니다."

Splines를 최상의 성능으로 작동하기 위해서는 그들을 지원하는 것이 필요합니다. 이것이 Grid Extension 기술이 필요한 곳입니다.

## C. Splines 개선을 위한 Grid Extension

Grid Extension 기술은 KAN의 중요한 구성 요소로, 스플라인에 내재된 제한된 해상도를 해결합니다. 스플라인은 유한한 점 그리드 상에 정의되어 있으므로 정확도는 그리드의 밀도에 따라 달라집니다. Grid Extension 기술은 모델이 기반이 되는 함수의 점점 미세한 세부사항들을 학습할 수 있도록 훈련 중에 그리드 밀도를 증가시킴으로써 이를 해결합니다.

<div class="content-ad"></div>

```
![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_20.png)

이 프로세스는 기존의 더 거친 스플라인에 새로운, 더 세밀한 스플라인을 맞추는 것을 포함합니다. 이를 통해 모델이 데이터 분포의 변화에 적응하고 이전에 배운 지식을 버리지 않고 정확성을 향상시킬 수 있습니다.

![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_21.png)

KANs에 대한 해석 가능성이 얼마나 중요한지 고려할 때, 가능한 한 우리의 KANs가 해석 가능하도록 보장할 수 있는 방법에 대해 이야기해 보겠습니다.
```

<div class="content-ad"></div>

## D. 해석 가능성을 위한 KAN 단순화

KAN의 기본 아키텍처는 해석 가능성을 촉진하지만 활성화 함수의 많은 수로 모델을 이해하기 어렵게 만들 수 있습니다. 이를 해결하기 위해 여러 가지 단순화 기술이 사용됩니다:

- 희소화 규제 (L1 및 엔트로피): 희소화 규제는 활성화 함수의 크기를 벌칙으로 삼아 희소한 표현을 촉진하며, 중요한 관계만을 나타내는 일부 활성화 함수만 값이 큰 방향으로 유도합니다. 이는 모델이 학습한 가장 중요한 관계를 식별하는 데 도움이 됩니다.
- 가지치기 기술: 가지치기 기술은 중요도 점수에 기초하여 네트워크에서 노드를 자동으로 제거하여 아키텍처를 더 단순화하고 매개변수 수를 줄입니다.
- 심볼화: 심볼화는 사용자가 활성화 함수를 삼각, 지수 또는 로그 함수와 같은 기호적 표현으로 변환할 수 있게 합니다. 이 단계는 활성화 함수의 숫자적 표현을 보다 쉽게 해석 가능한 기호적 공식으로 변환하기 위해 인간의 전문 지식을 활용합니다.

<img src="/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_22.png" />

<div class="content-ad"></div>

## 해석가능성의 주요 이점은 사람들(사용자, 전문가 등)이 프로세스/훈련 중간에서 AI를 결정으로 이끌 수 있도록 하는 것입니다. 이 상호작용은 AI 솔루션을 구축하는 가장 간과된 측면 중 하나이며, 대부분의 팀이 이를 구축하는 데 아무것도 하지 않거나 거의 하지 않습니다. 그러나 제대로 하면 게임 체인저가 될 수 있습니다. 그러니 KAN과 그들의 상호작용을 어떻게 만들 수 있는지에 대해 이야기해보겠습니다.

## E. 대화형 KAN

KAN의 주요 이점 중 하나는 그들의 고유한 상호작용성입니다. 시각화 도구와 상징적 조작 기능을 활용하여 사용자는 학습된 표현을 개량하고 근본적인 관계에 대한 보다 깊은 통찰력을 얻기 위해 모델과 협업할 수 있습니다.

<div class="content-ad"></div>

이 대화식 방식을 통해 사용자는 모델의 학습 과정을 안내할 수 있습니다.

- 활성화 함수 시각화: 사용자는 활성화 함수를 시각화하여 잠재적인 상징적 표현을 식별하고 모델의 의사 결정 과정을 이해할 수 있습니다.
- 활성화 함수 수동 설정: 사용자는 도메인 지식을 기반으로 특정 활성화 함수를 상징적 공식에 수동으로 맞출 수 있어 모델의 해석가능성과 정확성을 향상시킬 수 있습니다.
- 네트워크 반복적으로 개선: 사용자는 모델 구조, 하이퍼파라미터 및 정규화 전략을 반복적으로 조정하여 정확성과 상호 운용성 사이의 균형을 최적화할 수 있습니다.

모든 이러한 것들이 함께 결합하여 우리에게 KANs를 제공합니다- 결함이 있고 재미있으며 극도로 흥미로운 것입니다. 저에게는 KANs가 MLP의 가장 근본적인 문제 중 일부를 진정으로 다루는 신선한 시도를 제공한다고 생각됩니다. 그래서 이 논문은 모든 인정을 받아야 한다고 생각합니다. KANs가 쓸모 없거나 너무 특정한 제약이 있다고 해도- 우리는 KANs를 더 자세히 연구함으로써 많은 통찰을 얻을 것이라고 생각합니다.

그리고 그로써 코끼리 번식이 해결되었다고 생각해주세요.

<div class="content-ad"></div>

만약 이 기사가 마음에 드셨고 공유하고 싶으시다면 아래 가이드라인을 참조해 주세요.

이것으로 이 기사를 마치겠습니다. 여러분의 시간을 감사히 여깁니다. 언제나처럼, 저와 함께 일하길 희망하시거나 제 다른 작품을 확인해 보고 싶다면, 제 링크는 이 이메일/게시물의 맨 끝부분에 있을 거예요. 그리고 만약 이 글에서 가치를 발견했다면, 더 많은 사람들과 공유해 주시면 감사하겠습니다. 여러분과 같은 입소문 추천이 제 성장을 돕는데 큰 도움이 됩니다.

![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_24.png)

저는 정보를 제공하고 유용하며, 부당한 영향으로부터 독립된 작품을 만드는 데 많은 노력을 기울였습니다. 제 글을 지원하고 싶으시다면, 이 뉴스레터의 유로 구독자가 되는 것을 고려해 주세요. 이를 통해 더 많은 노력을 기울여 쓰기/연구를 할 수 있고, 더 많은 사람들에게 도달하며, 제 치명적인 초콜릿 우유 중독을 지원할 수 있습니다. 매주 10만 명이 넘는 독자들에게 인공지능 연구와 엔지니어링의 가장 중요한 아이디어들을 대중화하는 데 도와주세요.

<div class="content-ad"></div>

초콜릿 우유를 사주실래요?

부탁드립니다~

PS- 우리는 "자신의 능력에 맞게 지불"하는 모델을 따릅니다. 자세한 내용 및 여러분에게 적합한 방법을 찾는 데 도움이 될 포스트를 확인해보세요.

나는 주기적으로 미니 업데이트를 Microblogging 사이트인 Twitter(https://twitter.com/Machine01776819), Threads(https://www.threads.net/@iseethings404), TikTok(https://www.tiktok.com/@devansh_ai_made_simple)에서 공유하고 있어요. 제 학습 내용을 계속 알고 싶다면 팔로우해주세요.

# 언제든지 연락주세요~

<div class="content-ad"></div>

아래 링크를 사용하여 다른 콘텐츠를 확인하거나 튜터링에 대해 더 알아보거나 프로젝트에 대해 연락하거나 인사를 전하실 수 있습니다.

기술, AI 및 기계 학습에 대한 작은 단편들은 여기에서 확인하세요.

AI 뉴스레터 - [artificialintelligencemadesimple.substack.com](https://artificialintelligencemadesimple.substack.com/)

제 할머니가 좋아하는 기술 뉴스레터 - [codinginterviewsmadesimple.substack.com](https://codinginterviewsmadesimple.substack.com/)

<div class="content-ad"></div>

내 다른 기사들도 확인해보세요. : [https://rb.gy/zn1aiu](https://rb.gy/zn1aiu)

내 유튜브 채널: [https://rb.gy/88iwdd](https://rb.gy/88iwdd)

LinkedIn에서 연락해요. 함께 소통해요: [https://rb.gy/m5ok2y](https://rb.gy/m5ok2y)

내 인스타그램: [https://rb.gy/gmvuy9](https://rb.gy/gmvuy9)

<div class="content-ad"></div>

제 트위터: [https://twitter.com/Machine01776819](https://twitter.com/Machine01776819)