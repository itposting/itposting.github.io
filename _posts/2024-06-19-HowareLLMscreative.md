---
title: "LLM은 어떻게 창의적인가요"
description: ""
coverImage: "/assets/img/2024-06-19-HowareLLMscreative_0.png"
date: 2024-06-19 03:52
ogImage: 
  url: /assets/img/2024-06-19-HowareLLMscreative_0.png
tag: Tech
originalTitle: "How are LLMs creative?"
link: "https://medium.com/towards-artificial-intelligence/how-are-llms-creative-c9a348a0e354"
---


## LLMs의 창의성에 대한 과학 - Softmax, 온도

GPT, Llama 등의 생성 모델을 사용해 보았다면 '온도'라는 용어를 만난 적이 있을 것입니다.

![이미지](/assets/img/2024-06-19-HowareLLMscreative_0.png)

우선, '온도'는 생성된 콘텐츠의 창의성을 제어하는 매개변수입니다.

<div class="content-ad"></div>

하지만 기다려봐, 그건 엄청나게 소위 '너드' 같지 않나요? 처음에 이를 들었을 때 나는 분명히 멋진 물리학적 현상이 일어나고 있을 것이라고 생각했어요. 그리고 왜냐하면 대부분의 사람들이 온도와 연결짓는 것이기 때문이죠.

이 게시물에서, 나는 생성모델에 대한 온도를 설명하고 특히 LLM (Large Language Models)에서의 작동 방식과 창의력을 수학적으로 보여드릴 거에요.

대형 언어 모델(LLMs)은 자기 회귀 모델의 한 종류입니다. 자기 회귀 모델이란 무엇일까요? 간단히 말해, 이는 과거 값을 사용하여 미래 값을 예측하는 통계 모델들을 의미합니다. LLM의 경우, 과거 값은 당신이 입력하고, 미래 값은 생성된 토큰입니다.

LLMs은 한 번에 전체 문장을 생성할 수 없다는 것을 알아두어야 합니다. 왜냐하면 그들은 다음 토큰을 예측하는데 사용되는 자기 회귀형 모델이기 때문입니다. 이 토큰은 입력에 추가되어 다른 토큰을 생성하기 위해 사용되며, 그 체인은 'EOT'란 끝 토큰이 생성될 때까지 계속됩니다. 이는 모델에게 생성을 중단하도록 신호를 보냅니다.

<div class="content-ad"></div>

```
![이미지](https://miro.medium.com/v2/resize:fit:1400/1*mrIpGPlXZHL_K-e640HFaw.gif)

다음 토큰을 생성하기 위해 LLM은 어휘 사전에 있는 모든 가능한 토큰에 대한 확률 목록을 출력합니다.

![이미지](/assets/img/2024-06-19-HowareLLMscreative_1.png)

LLM의 어휘 사전 크기가 100이면, 한 번에 이 100개의 토큰 중 하나만 생성할 수 있습니다. 이 100개의 토큰 각각에 대해 해당 토큰이 다음 시퀀스에서 얼마나 가능성 있는지를 나타내는 확률 점수가 반환됩니다. 실제로, 어휘 사전 크기는 훨씬 더 큽니다. 예를 들어, GPT-4V는 32K 토큰의 어휘 사전 크기를 가지고 있습니다.
```

<div class="content-ad"></div>

한 마디로 LLM은 토큰 시퀀스를 입력으로 받아 처리하고, 어휘 사전의 각 토큰에 대한 확률 목록을 출력합니다. 일반적으로, 가장 높은 확률을 가진 토큰이 다음 세대의 토큰으로 반환됩니다.

## 소프트맥스

확률 목록을 계산하는 작업은 소프트맥스 레이어에 의해 수행됩니다. 모든 LLM은 로짓 벡터(로짓은 각 토큰에 연관된 비정규화된 raw scores입니다)를 입력으로 받고, 적절한 확률 분포를 출력하는 '마법 같은' 레이어를 최종 레이어로 갖습니다.

일단 소프트맥스를 내려놓고 효율적으로 확률을 생성하기 위한 자체 함수를 만들어봅시다. 우리의 함수는 특정 기준을 충족해야 합니다. 성공하면 왜 소프트맥스를 사용해야 하죠?

<div class="content-ad"></div>

- 1. 입력 벡터를 가져와 동일한 크기의 출력 벡터를 생성합니다.
- 2. 출력 벡터의 각 요소가 음수가 아닌지 확인합니다(확률은 음수일 수 없음).
- 3. 입력 값이 클수록 출력 값이 클 수 있도록 합니다.
- 4. 출력 벡터의 모든 요소가 1이 되도록 합니다(확률 분포임).

위의 조건을 만족하기 위해 각 요소별 변환을 수행하게 됩니다. 변환은 음이 아닌 값이어야 하며, 항상 양의 정수가 출력되는 함수를 사용해야 합니다. 또한, 입력 값이 클수록 항상 더 큰 출력 값으로 변환되도록 하는 단조 증가함수여야 합니다.

eˣ 함수는 위의 모든 조건(1~3)을 모두 만족합니다.

[이미지](/assets/img/2024-06-19-HowareLLMscreative_2.png)

<div class="content-ad"></div>

우리의 전략은 입력 벡터의 각 요소에 eˣ 함수를 적용하여, 길이는 동일하지만 모든 양수인 숫자로 이루어진 벡터를 생성합니다.

![image](/assets/img/2024-06-19-HowareLLMscreative_3.png)

하지만 우리가 가장 중요하게 놓친 요구 사항이 있었어요, 그거 발견했나요?

출력 벡터의 값들이 1보다 큽니다. 곡선에서 볼 수 있듯이, x = 0일 때 y = 1이고, x가 0을 초과하면 y가 1을 초과합니다. 이는 우리의 경우에 맞지 않는데, 확률이 필요하기 때문입니다. 각 값이 1보다 작아야 하고, 모든 값이 합쳐서 1이 되어야 합니다.

<div class="content-ad"></div>

이 문제를 해결하기 위해 출력 벡터의 모든 요소를 출력 벡터의 모든 요소의 합으로 나눌 수 있습니다. 이렇게하면 각 값이 1보다 작아지고 모두 1로 합산됩니다. 이 단계를 정규화라고 합니다.

![이미지](/assets/img/2024-06-19-HowareLLMscreative_4.png)

이제 소프트맥스에 도착했습니다. 하하. 네, 이것이 바로 우리가 이해하기 위해 도출한 소프트맥스 함수입니다. 단순히 설명하는 것보다 더 나은 이해를 도모했죠.

참고: 변환 함수는 미분 가능해야 하므로 손실을 전파할 수 있습니다. 이것이 소프트맥스에서 'e'를 선택하는 또 다른 이유입니다. 사실 지수 함수의 도함수를 계산하는 것이 가장 쉬운 일이라고 믿습니다. :)

<div class="content-ad"></div>

이제 우리는 Softmax 함수를 사용하여 LLM에서 확률이 어떻게 생성되는지 이해했어요. 모델에 어떻게 창의성을 도입할 수 있는지 알아보겠어요.

## AI 모델에서의 창의성, 정말?

일부 창의성을 도입하려면 Softmax에 의해 생성된 확률 분포를 "평평하게(flatten)" 만들어야 해요. "평평하게"라고 무슨 뜻일까요?

![이미지](/assets/img/2024-06-19-HowareLLMscreative_5.png)

<div class="content-ad"></div>

```
![이미지](/assets/img/2024-06-19-HowareLLMscreative_6.png)

한 예를 통해 이해해 봅시다.

LLM에 입력되는 내용:

“대화를 완성하세요.”
```

<div class="content-ad"></div>

A: "안녕, 어떻게 지내니?"

B: "LLM은 이제 B가 말하는 첫 번째 토큰을 예측하도록 지정되었습니다.

간단하게 하기 위해 단어 vocab을 5개만 고려합시다."

<div class="content-ad"></div>

이 모델은 이 입력을 처리하여 로짓 벡터를 생성하며, 이 벡터는 소프트맥스에 의해 확률로 변환됩니다. 소프트맥스 레이어의 입력인 로짓 벡터는 [0.1, 0,0.5,1, 4, 0.6]이며, 토큰[‘Ni Hao’, ‘Konnichiwa’, ‘Hola’, ‘Namaste’, ‘Hello’, ‘Ciao’]에 대한 출력은 [0.01, 0.01 0.02 0.04 0.86 0.02]입니다.

‘Hello’(목록에서 다섯 번째 단어)을 다음 토큰으로 선택할 확률은 86%입니다. 그러나 확률 점수 중 하나가 지배하는 경우 재미가 없죠? 즉, ‘Hello’가 출력으로 선택될 가능성이 거의 확실하다는 것이죠. 동일한 상황이 발생하면 모델이 항상 "Hello" 토큰을 선택할 확률이 높습니다. 이는 매우 예측성이 강하며 창의성이 아니라 반대입니다. 랜덤성에 대한 공간이 없으며 다양한 유효한 토큰을 시도할 여지가 없습니다.

만약 같은 토큰[‘Ni Hao’, ‘Konnichiwa’, ‘Hola’, ‘Namaste’, ‘Hello’, ‘Ciao’]에 대해 소프트맥스 출력이 [0.13 0.12 0.14 0.15 0.28 0.14]라면 어떻게 될까요?

“Hello”가 선택될 가능성은 여전히 높지만, 이전보다 다른 토큰들도 좋은 기회를 갖고 있습니다.

<div class="content-ad"></div>

이 경우, 모델이 일반적인 "안녕"을 생성하는 것에서 조금 벗어나 "남테", "올라" 등을 시도할 여지가 있습니다. 기본적으로 약간의 무작위성을 통합하는 것이죠. 이는 파급 효과를 일으키며 대화 전체가 가장 예상치 못한 방향으로 전개될 수 있습니다. 예를 들어, B가 일본어로 말하고 A가 A가 한 언어를 알아보려고 노력하는 상황 등이 발생할 수 있습니다. 이 창조적으로 들리지 않나요? 저에게는 더 나은 정의가 없어요 :-)

온도 매개변수('T')의 목표는 생성된 내용의 무작위성을 제어하는 것입니다.

소프트맥스 함수를 수정하여 출력 벡터가 주로 우세한 확률 점수를 갖지 않도록 만드는 방법에 대해 이해해 보겠습니다(i.e., 확률 분포를 평평하게 만드는 것).

소프트맥스 함수는 각 요소를 입력 벡터에서 eˣ로 변환합니다. 이를 통해 eˣ 함수를 살펴봐야 한다는 아이디어가 얻어집니다.

<div class="content-ad"></div>

가장 간단한 예로서 2차원 입력 벡터 [1,2]를 살펴보고, 이를 이해하기 위해 Softmax를 적용해 봅시다.

![Softmax 예시 이미지](/assets/img/2024-06-19-HowareLLMscreative_7.png)

요소별로 eˣ 변환을 수행하면, [2.71, 7.39]를 얻을 수 있습니다.

그리고 최종 Softmax 결과는 [2.71/(2.71+7.39), 7.39/(2.71+7.39)]이 될 것입니다.

<div class="content-ad"></div>

= [0.25, 0.75]

두 번째 항목이 50%의 큰 마진으로 우세함을 나타냅니다. 

그러나 우리의 목표는 두 값의 소프트맥스 출력을 가깝게 만드는 것이죠, 그렇지 않나요?

eˣ 곡선을 살펴보면 이 차이가 주로 eˣ 곡선의 성질 때문인 것으로 추측할 수 있습니다.

<div class="content-ad"></div>

그렇다면, 이제 우리의 목표는 eˣ 함수를 조정하여 eˣ(1)과 eˣ(2)의 차이가 크지 않도록 만드는 것으로 전환됩니다. 만약 ||eˣ(1) - eˣ(2) ||가 작다면, 그것은 확률 간의 차이를 작게 만들 것이고 이것은 본질적으로 확률의 평탄화라고 불리게 됩니다. 다시 말해, eˣ 곡선의 가파른 정도를 줄입니다.

<img src="/assets/img/2024-06-19-HowareLLMscreative_8.png" />

어떨 때, 파란색 곡선에서 ||eˣ(1) — eˣ(2) ||의 차이가 주황색 곡선보다 더 작은 것을 볼 수 있습니다.

주황색 곡선: eˣ 함수

<div class="content-ad"></div>

파란색 곡선: e^(1/2 ⋅ x) 함수, 우리는 분모를 온도 'T'라 부르며, 이 경우에는 2와 같습니다.

‘T’가 증가하면 곡선이 덜 가파르게 변해 확률이 더 가까워진다는 뜻입니다. 아래는 Softmax with Temperature에 대한 대화식 desmos 그래프 링크이니, 곡선의 가파름을 제어하는 방법에 대해 더 나은 직관을 얻을 수 있도록 살펴보세요.

이 섹션을 건너뛰셔도 됩니다. 이것은 미적분적이며 'e^(1/2 ⋅ x)'가 어떤 지점에서 'eˣ'보다 작은 도함수를 가지는 이유로 증명하는 부분입니다.

파란색 곡선을 사용한 Softmax 과정(T=2)은 다음과 같을 것입니다:

<div class="content-ad"></div>

```
<img src="/assets/img/2024-06-19-HowareLLMscreative_9.png" />

바닐라 소프트맥스로 [0.25, 0.75]을 얻었지만 온도 (T)가 2일 때 [0.38, 0.62]를 얻었어요. 상당한 진전이죠, 알겠죠?

매개변수 'T'를 증가시킴으로써 확률 분포를 더 평평하게 만들 수 있어요. T = 1이면 바닐라 소프트맥스와 본질적으로 같아지며, T가 1보다 낮게 설정되면 출력이 더 예측 가능해져요. 온도를 계속 증가시키면 어느 정도 지나면 생성된 콘텐츠가 암호화된 텍스트처럼 보일거에요.

```js
import numpy as np


def softmax(xs):
    return np.exp(xs) / sum(np.exp(xs))
def softmax_t(xs, t):
    return np.exp(xs/t) / sum(np.exp(xs/t))
xs = np.array([ 1 , 2 ])
print(softmax(xs))
print(softmax_t(xs, 2))
print(softmax_t(xs, 5))
OUTPUT:
[0.26894142 0.73105858]    #T = 1
[0.37754067 0.62245933]    #T = 2
[0.450166 0.549834]        #T = 5
```

<div class="content-ad"></div>

Mistral 7B 모델에게 네팔에 대해 작성하도록 요청했어요. 첫 번째 텍스트에서 온도 'T'를 1로 설정했고, 두 번째 텍스트에서는 온도 'T'를 2로 설정했어요.

차이를 확인해보세요

T = 0.5

네팔은 중국과 인도 사이에 위치한 아름다우며 다양한 나라로 알려져 있어요. 세계에서 가장 높은 산인 에베레스트를 비롯해 멋진 산악 풍경으로 유명해요.

<div class="content-ad"></div>

T = 2

네팔의 중심지에는 겨울에 종종 온도가 섭씨 4도까지 떨어지는 곳이 있습니다. 세계에서 가장 숨 막히는 눈이 덮인 산들 중 일부를 발견할 수 있어요.

네, 두 번째 텍스트가 좀 더 창의적인 느낌이죠??

한 마디로, 물리학적으로 온도는 분자의 무작위성의 정도이며, 이를 통해 우리에게 몸체에 함유된 열 에너지에 대한 감각을 제공합니다.

<div class="content-ad"></div>

표 태그를 마크다운 형식으로 변경해 주세요.

<div class="content-ad"></div>

https://en.wikipedia.org/wiki/Softmax_function