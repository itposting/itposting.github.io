---
title: "아파치 드루이드 아키텍처"
description: ""
coverImage: "/assets/img/2024-06-20-TheArchitectureofApacheDruid_0.png"
date: 2024-06-20 15:36
ogImage: 
  url: /assets/img/2024-06-20-TheArchitectureofApacheDruid_0.png
tag: Tech
originalTitle: "The Architecture of Apache Druid"
link: "https://medium.com/data-engineer-things/the-architecture-of-apache-druid-e92d64ba4360"
---


## 하둡이 모든 문제를 해결할 수 있는 때

# 목차

- 시작
- 아키텍처
- 실시간 노드
- 역사적 노드
- 브로커 노드
- 코디네이터 노드

# 소개

<div class="content-ad"></div>

이번 주에는 가장 유명한 실시간 OLAP 시스템 중 하나인 Apache Druid에 대해 자세히 살펴볼 예정이에요. 어떻게 작동하는지 궁금했던 적이 있나요? 이 블로그 글은 'Druid — A Real-time Analytical Data Store' 논문을 읽은 후에 작성되었어요.

참고: 이 논문은 2014년에 발표되었기 때문에 Druid의 일부 세부 사항이 변경/업데이트되었을 수 있어요.

# 시작부터

2004년, 구글이 산업에서 가장 영향력 있는 논문 중 하나인 'MapReduce: Simplified Data Processing on Large Clusters'를 발표했어요. 구글은 MapReduce를 도입하여 상용 컴퓨터를 사용하여 대규모 데이터 처리를 달성했어요. 그 뒤로 Hadoop 프로젝트가 짧은 시간 내에 나왔고, HDFS와 MapReduce 프레임워크는 많은 기업에 대한 대용량 데이터 분석의 기반을 마련했어요. 대용량 데이터의 저장 및 처리를 가능케 한 Hadoop 시스템이 있지만, 일부 단점이 있고 모든 요구 사항을 충족시키지 못할 수 있어요.

<div class="content-ad"></div>

메타마켓(지금은 Rill이라 불림)은 마케터들이 마케팅 인사이트에 더 쉽게 접근하고 상호 작용하며 시각화하는 데 도움을 주는 회사입니다. 메타마켓 제품은 고도로 병행적인 환경에서 쿼리 성능과 데이터 가용성에 대한 보장이 필요합니다. 그들은 곧 하둡이 필요한 것을 지원할 수 없다는 것을 깨달았습니다. 오픈 소스 솔루션을 많이 연구한 후, 그들은 사용 가능한 솔루션 이상이 필요하다는 것을 깨달았습니다. 그래서 대용량 데이터에 대한 실시간 분석을 제공하는 데이터 저장소인 Druid를 만들었습니다.

메타마켓 초기에는 사용자가 이벤트 스트림을 탐색할 수 있는 대시보드 솔루션에 초점을 맞추고 있었습니다. 대시보드 아래의 데이터는 사용자가 상호 작용할 수 있는 속도로 빠르게 처리되고 반환되어야 했습니다. 쿼리 지연 시간 외에도 시스템은 고 가용성(다운 타임이 비즈니스에 해를 입힐 수 있음) 및 병행성(많은 사용자가 동시에 제품을 사용함)이 필요합니다. 이것이 메타마켓이 Druid를 스스로 개발해야 하는 이유입니다.

# 아키텍처

![이미지](https://miro.medium.com/v2/resize:fit:1400/0*LEShd3zbfB-0RWim.gif)

<div class="content-ad"></div>

Druid은 공유하지 않는 아키텍처를 갖고 있어요. Druid 클러스터에는 다양한 종류의 노드가 있어요. 각 노드 유형은 일련의 책임을 맡고 있어요. 아래 섹션에서는 Druid의 노드 유형에 대한 자세한 내용을 다룰 거예요.

# 실시간 노드

실시간 노드는 이벤트 스트림을 수집하고 쿼리하는 작업을 맡고 있어요. 이벤트를 즉시 쿼리할 수 있도록 처리할 수 있어요. 그들은 Zookeeper(드루이드의 상태 관리 구성 요소)에 상태 및 책임 있는 데이터를 알려줄 거예요. 실시간 노드는 이벤트를 위한 로우 저장소처럼 작동해요. 이러한 노드는 모든 수신 이벤트를 위해 인메모리 인덱스 버퍼를 유지해요. 이러한 인덱스는 데이터가 수집될 때 점진적으로 생성돼요. 이 인덱스는 직접 쿼리될 수 있어요.

메모리 제한 때문에 실시간 노드는 인메모리 인덱스를 디스크에 두 가지 방식으로 유지해요: 주기적으로 또는 최대 행 임계값을 초과했을 때. 이 프로세스는 로우 저장 형식을 메모리에서 열 기반 저장 형식으로 변환해요. 그런 다음, 디스크의 데이터는 변경 불가능하게 저장돼요. 실시간 노드는 로컬로 유지되는 모든 데이터를 찾는 백그라운드 작업을 예약할 거예요. 이 작업은 이 인덱스를 병합하고 특정 시간 범위의 모든 수신된 이벤트를 포함하는 변경 불가능한 데이터 블록을 빌드해요. 논문에서 이 병합된 데이터를 "세그먼트"라고 부르고 있어요. 나중에, 실시간 노드는 이 세그먼트를 원격 저장소(드루이드에서 딥 스토리지라고 불리는)에 업로드할 거예요. S3나 HDFS와 같은 원격 저장소가 있어요.

<div class="content-ad"></div>


![Real-time nodes](https://miro.medium.com/v2/resize:fit:1400/0*kVg94BJBzWdZYS96.gif)

실시간 노드들은 주로 Kafka와 같은 메시지 버스를 사용합니다. 프로듀서는 데이터를 Kafka 토픽으로 전송하고, 그런 다음 실시간 노드가 Kafka 토픽에서 데이터를 읽어들이게 됩니다. Kafka와 같은 중간 계층이 있는 경우 몇 가지 장점이 있습니다:

- 이벤트 버퍼: Kafka는 이벤트 오프셋을 유지하여 실시간 노드가 현재 데이터 소비 위치를 알 수 있도록 합니다. 노드는 메모리 버퍼에 지속적으로 저장할 때마다 오프셋을 업데이트하며, 디스크에서도 이 정보를 저장합니다. 장애 발생 시 디스크가 여전히 사용 가능하다면, 실시간 노드는 디스크에 저장된 오프셋을 사용하여 해당 커밋된 오프셋부터 토픽을 계속 읽을 수 있습니다. 이는 복구 시간을 줄이는 데 도움이 됩니다.
- 단일 데이터 엔드포인트:

![Apache Druid Architecture](/assets/img/2024-06-20-TheArchitectureofApacheDruid_0.png)


<div class="content-ad"></div>

- 중복된 소비자: 여러 실시간 노드가 카프카 토픽으로부터 동일한 이벤트 세트를 수신하여 중복된 이벤트 스트림을 생성할 수 있습니다. 실시간 노드 1개가 실패할 경우, 다른 노드가 데이터가 수신되도록 보장합니다.
- 부하분산된 소비자: 여러 실시간 노드가 각각 스트림 파티션을 수신합니다. 따라서 시스템은 수신 처리량을 확장할 수 있습니다.

# 과거 노드

과거 노드는 실시간 노드에서 세그먼트를 로드하고 제공합니다. 세그먼트(실시간 노드에서)는 S3 또는 HDFS와 같은 저장소에 원격으로 저장되며, 노드의 로컬 디스크는 캐시로 사용됩니다. 실시간 노드와 마찬가지로, 과거 노드는 Zookeeper에 온라인 상태와 제공하는 데이터를 알립니다. Druid는 Zookeeper에게 과거 노드에게 세그먼트를 로드하고 삭제하는 방법에 대한 지침을 보냅니다. 이 지침에는 세그먼트가 딥 스토리지에 위치하고 세그먼트를 해제하고 처리하는 방법에 대한 정보도 포함됩니다.

![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YonIcYDmR1RAZM9XQ31Slg.gif)

<div class="content-ad"></div>

역사 노드는 데이터 서빙을 위해 깊은 저장소에서 특정 세그먼트를 다운로드합니다. 다운로드하기 전에 먼저 로컬 캐시를 확인합니다. 필요한 세그먼트가 캐시에 없는 경우 깊은 저장소에서 다운로드합니다. 다운로드 후에는 해당 상태를 Zookeeper에 알립니다. 역사 노드는 변경 불가능한 데이터만 처리하므로 세그먼트에서 읽기를 실행할 때 일관성을 보장할 수 있습니다. 변경 불가능성은 또한 Druid가 데이터 수정 여부에 신경 쓰지 않고 병렬화를 효율적으로 달성할 수 있도록 합니다.

![이미지](/assets/img/2024-06-20-TheArchitectureofApacheDruid_1.png)

역사 노드는 서로 다른 티어로 그룹화될 수 있습니다. 사용자는 각 티어마다 다른 성능 및 장애 허용 매개변수를 구성할 수 있습니다. 티어 노드의 목적은 더 중요한 세그먼트에 따라 더 높거나 낮은 우선 순위로 분배되도록하는 것입니다. 예를 들어, 사용자는 고 CPU 및 메모리를 가진 "핫" 티어의 역사 노드를 설정할 수 있습니다. "핫" 클러스터는 더 자주 액세스되는 데이터를 더 많이 다운로드할 것입니다. 또한 "콜드" 클러스터에는 덜 자주 액세스되는 세그먼트만 포함될 것입니다.

# 브로커 노드

<div class="content-ad"></div>

![image](https://miro.medium.com/v2/resize:fit:1400/1*75f72U25fy68ZFcgnVHLxg.gif)

브로커 노드는 "적절한" 쿼리를 필요한 데이터를 가진 히스토리컬 및 리얼타임 노드로 경로 지정합니다. 브로커 노드는 Zookeeper에서 메타데이터를 읽어 어떤 세그먼트가 쿼리 가능하며 해당 세그먼트가 어디에 위치하는지를 가리킵니다. 쿼리가 리얼타임 및 히스토리컬 노드에서 결과를 필요로 할 때, 브로커는 결과를 합쳐서 호출자에게 반환하기 전에 결합합니다.

이러한 노드는 LRU 전략을 사용한 캐시 구현이 있습니다. 캐시는 로컬 힙 메모리나 Memcached와 같은 외부 스토어를 사용할 수 있습니다. 브로커 노드가 쿼리를 받으면 해당 쿼리를 세트의 세그먼트로 매핑합니다. 쿼리 결과는 이미 캐시에 존재할 수 있기 때문에 다시 처리할 필요가 없습니다. 캐시에 존재하지 않는 결과에 대해서는 브로커 노드가 올바른 노드로 쿼리를 전달합니다:

- 히스토리컬 노드의 결과의 경우, 브로커는 미래 사용을 위해 세그먼트 단위로 이러한 결과를 캐싱합니다.
- 리얼타임 노드의 결과는 브로커가 캐싱하지 않습니다. 이렇게 함으로써 쿼리가 항상 결과의 신선도를 보장하는 리얼타임 노드에 의해 처리되도록 합니다.

<div class="content-ad"></div>

브로커 노드는 쿼리를 처리 노드로 라우팅하기에 중요합니다. 이를 위해 세그먼트 노드 매핑을 위해 Zookeeper와 통신해야 합니다. Zookeeper 장애가 발생한 경우, 브로커 노드는 클러스터의 최신 상태를 사용합니다 (이전 성공적인 Zookeeper 통신에서의 마지막 메타데이터). 브로커 노드는 클러스터 상태가 장애 전과 동일하다고 가정할 것입니다.

# 코디네이터 노드

이러한 노드는 역사적 노드의 데이터 관리 및 분배를 담당합니다. 코디네이터 노드는 역사적 노드에게 새로운 데이터를 로드하도록 지시하거나 오래된 데이터를 삭제하며 데이터를 복제하고 로드 밸런스를 맞추기 위해 데이터를 이동합니다. 코디네이터 노드에게는 리더 선출 프로세스가 있어 코디네이터 작업을 실행하는 노드를 결정하며, 다른 노드는 백업으로서 대체 역할을 합니다.

코디네이터 노드는 주기적으로 실행되어 클러스터의 현재 상태를 결정합니다. 그런 다음 예상 상태와 실제 상태를 비교하여 결정을 내립니다. (Kubernetes와 유사하죠?). 앞에서 언급한 노드 유형과 마찬가지로, 코디네이터 노드는 현재 클러스터 정보를 얻기 위해 Zookeeper와 통신합니다. 또한 이러한 노드는 추가 운영 매개변수 및 구성을 저장하는 MySQL 데이터베이스와 연결되어 있습니다. MySQL 데이터베이스에는 세그먼트가 클러스터에서 생성되고 삭제되며 복제되는 방식을 제어하는 규칙 테이블이 저장됩니다.

<div class="content-ad"></div>

룰 테이블에는 규칙이 포함되어 있습니다. 이 규칙은 역사적 세그먼트가 클러스터에서로드되고 삭제되는 방식을 제어합니다. 이러한 규칙을 통해 코디네이터가 다음을 알 수 있습니다:

- 세그먼트를 다른 역사 노드 티어에 할당하는 방법
- 각 티어에 세그먼트의 복제본이 어떻게 존재해야 하는지
- 언제 세그먼트를 삭제해야 하는지

코디네이터 노드는 또한 세그먼트의 분배를 제어함으로써 클러스터의 균형을 유지합니다. 더 나아가, 코디네이터 노드는 역사 노드에 동일한 세그먼트의 복사본을로드하여 장애 허용성과 가용성을 향상시킬 수 있습니다. 복제본의 수는 설정 가능합니다.

# 저장 형식

<div class="content-ad"></div>

드루이드(Druid)의 테이블은 타임스탬프가 찍힌 이벤트의 컬렉션으로, 일반적으로 각 세그먼트는 5~10백만 행 정도로 파티셔닝되어 있어요. 세그먼트는 기본 저장 단위이며, 복제 및 분배는 세그먼트 수준에서 수행됩니다. 모든 테이블에는 Druid가 필요로 하는 타임스탬프 열이 항상 있어요. Druid는 이 열을 데이터 분배 및 유지 정책에 사용하기 때문이에요. 세그먼트는 데이터 소스 식별자, 데이터 간격 및 버전에 의해 식별됩니다. 더 늦은 버전의 세그먼트에는 더 최신 데이터가 있어요. Druid에서 읽기 작업은 항상 최신 버전의 세그먼트에서 특정 시간 범위의 데이터를 읽어와요. 세그먼트는 컬럼 형식으로 원격 저장소에 저장되며, 이를 통해 더 효율적인 CPU 사용이 가능합니다. 필요한 데이터만 로드되기 때문이죠. Druid에는 다양한 데이터 형식을 지원하기 위한 여러 가지 컬럼 타입이 있어요. Druid는 디스크나 메모리에서 데이터를 더 효율적으로 압축하기 위해 다른 데이터 형식에 다양한 압축 방식을 적용할 거에요.

# 마무리

드루이드(Druid) - 실시간 분석 데이터 스토어 논문을 읽은 후의 제 메모가 여기까지에요. 여러분이 가치를 느끼셨으면 좋겠습니다. 만약 다른 실시간 OLAP 시스템에 대해 읽고 싶으시다면 링크드인의 Apache Pinot에 관한 제 글을 확인해보세요: Apache Pinot, 링크드인의 실시간 OLAP 시스템을 한 눈에!

다음 블로그에서 만나요!

<div class="content-ad"></div>

# 참고 자료

[1] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, Deep Ganguli, Druid — A Real-time Analytical Data Store

내 뉴스레터는 주간 블로그 스타일 이메일로, 저보다 더 똑똑한 사람들로부터 배운 것들을 기록합니다.

그러니 저와 함께 배우고 성장하고 싶다면 여기에서 구독하세요: https://vutr.substack.com.