---
title: "거대한 언어 모델 마스터하는 방법"
description: ""
coverImage: "/assets/img/2024-06-22-MasterLargeLanguageModels_0.png"
date: 2024-06-22 20:58
ogImage: 
  url: /assets/img/2024-06-22-MasterLargeLanguageModels_0.png
tag: Tech
originalTitle: "Master Large Language Models"
link: "https://medium.com/@dawoodsarfraz.cs/master-large-language-models-166f847d254f"
---


![MasterLargeLanguageModels](/assets/img/2024-06-22-MasterLargeLanguageModels_0.png)

인공 지능의 광활한 풍경 속에서 혁명적인 힘이 나타났습니다: 대규모 언어 모델(LLMs). 이러한 모델들은 그저 유행어가 아닙니다; 그들은 AI의 미래를 대표합니다. 인간과 유사한 텍스트를 이해하고 생성하는 능력은 그들을 주목받게 만들었으며, 오늘날 가장 흥미로운 그리고 역동적인 연구 분야 중 하나로 떠올랐습니다. 친구처럼 자연스럽게 응답하는 챗봇이나 인간의 글쓰기와 구분할 수 없을 정도로 텍스트를 자연스럽게 생성하는 컨텐츠 생성 시스템을 상상해보세요. 이러한 혁신적인 기술이 귀하의 흥미를 사로잡고 LLMs의 세계로 더 심층적으로 탐험하고 싶다면, 여기에 잘 오셨습니다.

여러분의 여정을 돕기 위해 종합적인 자료 목록을 소개합니다. 이 컬렉션은 다음을 포함합니다:

- 온라인 강의: 초급부터 고급 수준까지 안내해주는 구조화된 학습 경로에 접근하세요. 이 강의들은 LLMs의 이론적 기반, 실용적 구현, 그리고 실습 프로젝트를 다루며, 최고의 강사들로부터 배우고 LLM 기반 애플리케이션을 개발하고 배포하는 데 필요한 기술을 습득할 수 있습니다.
- 워크샵과 학회: AI와 LLM에 전념하는 워크샵 및 학회에 참석하세요. 이러한 행사들은 전문가들로부터 배우고 동료들과 네트워킹하며 분야에서 최신 연구 및 혁신을 발견할 수 있는 기회를 제공합니다. 학습 경험을 향상시킬 다가오는 행사를 주목하세요.
- 책과 유익한 기사: AI와 LLM 분야의 선도적인 전문가들이 저술한 종합적인 자료와 기사들을 탐색해보세요. 이러한 자료들은 LLMs를 숙달하는 데 필요한 깊은 지식, 사례 연구, 그리고 실용적인 조언을 제공합니다. LLM 개념, 응용 프로그램, 그리고 분야에서의 최신 발전사항에 대한 자세한 탐구에 몰두하세요. 이 기사들은 LLM의 기초부터 윤리적 영향, 그리고 미래 방향을 포함한 여러 주제를 다룹니다.
- GitHub 저장소: LLMs를 실험해볼 수 있는 다양한 실무 프로젝트, 코드 샘플, 그리고 도구들을 탐색해보세요. 이러한 저장소들은 실무 경험과 LLMs가 실제 시나리오에서 어떻게 구축되고 사용되는지에 대한 통찰을 제공합니다. 사전 훈련된 모델, 세부 조정 스크립트, 그리고 혁신적인 애플리케이션을 찾아 공부하고 수정해보세요.

<div class="content-ad"></div>

이러한 자료는 LLM(Large Language Models)에 대한 기초부터 최신 기술까지 철저히 이해할 수 있도록 설계되었습니다. 학생, 연구원 또는 산업 전문가이든, 이 안내서를 통해 LLM을 숙달하는 데 필요한 지식과 도구를 제공할 것입니다.

# 1. 기초 과정

- 머신 러닝 전문화 — Coursera

![LLM 이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_1.png)

<div class="content-ad"></div>

링크: Machine Learning 전문화 프로그램

설명: Machine Learning 전문화는 DeepLearning.AI와 Stanford Online이 공동으로 개발한 기초 온라인 프로그램입니다. 이 초보자 친화적인 프로그램은 머신 러닝의 기본 개념을 가르치고 이러한 기술을 사용하여 현실 세계 AI 애플리케이션을 만드는 방법을 가르쳐줍니다.

이 전문화 프로그램은 Stanford 대학에서 사업적 연구를 이끈 AI 비전가인 Andrew Ng가 가르치고 있습니다. 그는 Google Brain, Baidu, Landing.AI에서 중요한 연구를 이끌었으며 AI 분야를 발전시키기 위한 업적을 이루었습니다.

이 3개 과정으로 이루어진 전문화는 Andrew가 선도적으로 개선한 머신 러닝 과정의 업데이트된 버전으로, 2012년에 시작되어 현재까지 4.8백만 명 이상의 학습자가 수강한 만족도 평가 4.9점을 받았습니다.

<div class="content-ad"></div>

2. Stanford CS229: Machine Learning Course YouTube by Andrew Ng

![Stanford CS229](/assets/img/2024-06-22-MasterLargeLanguageModels_2.png)

Link: [YouTube Playlist](https://www.youtube.com/playlist)

Description: 첫 강의인 Andrew Ng의 기계 학습 강의를 듣습니다. 이 강의는 기계 학습과 통계적 패턴 인식에 대한 폭넓은 소개를 제공합니다. 지도 및 비지도 학습, 학습 이론, 강화 학습 그리고 제어에 대해 배울 수 있습니다. 최근 기계 학습 응용 및 기계용 알고리즘을 설계 및 개발해보세요.

<div class="content-ad"></div>

3. 딥 러닝 전문화 - Coursera

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_3.png)

링크: [딥 러닝 전문화](링크 주소)

설명: 딥 러닝 전문화는 딥 러닝의 능력, 도전, 결과를 이해하고 선두주자 AI 기술 발전에 참여할 수 있도록 준비하는 기초 프로그램입니다.

<div class="content-ad"></div>

이 특화과정에서는 합성곱 신경망, 순환 신경망, LSTM, 트랜스포머 등의 신경망 아키텍처를 구축하고 훈련시키는 방법을 배울 것입니다. 또한 Dropout, BatchNorm, Xavier/He 초기화 등의 전략을 활용하여 이를 개선하는 방법도 배우게 됩니다. 이론적 개념과 해당 산업적 응용을 파이썬과 TensorFlow를 이용하여 숙달하고, 음성 인식, 음악 합성, 챗봇, 기계 번역, 자연어 처리 등과 같은 실제 사례에 대처할 준비를 하세요.

4. Stanford CS224N: NLP with Deep Learning — YouTube

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_4.png)

링크: Stanford CS224N: NLP with Deep Learning

<div class="content-ad"></div>

설명: 이것은 NLP에 대한 최신 연구에 대한 철저한 소개와 지식의 보고서 대상지로, 귀하에게 지식의 보고서 대상지와 초심자를 위한 자료 보고서 대상지입니다. 본 강좌는 기계 학습 교수인 Christopher Manning Thomas M. Siebel 교수님에 의해 가르쳐지며, 언어학과 컴퓨터 과학 교수이자 스탠포드 인공지능 연구소 소장입니다.

5. 허깅페이스 트랜스포머 강의 — 허깅페이스

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_5.png)

링크: [허깅페이스 트랜스포머 강의](링크 주소)

<div class="content-ad"></div>

Description: 이 강좌는 HuggingFace 생태계의 라이브러리를 사용하여 NLP를 가르칩니다. HuggingFace의 다음 라이브러리의 내부 작업 및 사용법을 다룹니다:

- Transformers
- Tokenizers
- Datasets
- Accelerate

저자 소개:

Abubakar Abid는 스탠포드 대학에서 응용 기계 학습 분야에서 박사 학위를 받았습니다. 박사 과정 중에 그는 Gradio를 설립했는데, 이는 60만 개 이상의 머신 러닝 데모를 구축하는 데 사용되는 오픈 소스 Python 라이브러리입니다. Gradio는 Hugging Face에 의해 인수되었으며, 현재 Abubakar는 여기서 머신 러닝 팀 리드로 근무하고 있습니다.

<div class="content-ad"></div>

매튜 카리건은 허깅페이스에서 머신러닝 엔지니어로 일하고 있습니다. 아일랜드 더블린에 살며, 이전에는 Parse.ly에서 ML 엔지니어로 일했으며, 그 전에는 트리니티 대학교에서 박사 후 연구원으로 일했습니다. 기존 구조를 확장하여 AGI에 도달할 것이라고 믿지 않지만, 로봇 불멸에 대한 높은 희망을 품고 있습니다.

리산드르 데뷔는 허깅페이스에서 머신러닝 엔지니어로 일하고 있으며, 매우 초기 개발 단계부터 🤗 트랜스포머 라이브러리에 참여해왔습니다. 그의 목표는 매우 간단한 API를 통해 모두에게 자연어 처리를 액세스할 수 있도록 하는 것입니다.

실반 구거는 허깅페이스의 연구 엔지니어이자 🤗 트랜스포머 라이브러리의 핵심 유지 보수자 중 한 명입니다. 이전에는 fast.ai에서 연구 과학자로 일하며, Jeremy Howard와 함께 '딥러닝 코더를 위한 fastai와 PyTorch'를 공동 저술했습니다. 그의 연구 주요 관심사는 깊은 학습을 보다 쉽게 접근할 수 있도록 하는 것으로, 한정된 자원 상에서 빠르게 모델을 교육할 수 있도록 설계하고 개선하는 기술을 고안하고 있습니다.

다우드 칸은 허깅페이스에서 머신러닝 엔지니어로 일하고 있습니다. 뉴욕 출신이며, 뉴욕 대학교에서 컴퓨터 과학을 전공했습니다. 몇 년간 iOS 엔지니어로 일한 후 다우드는 동료 창업자들과 함께 Gradio를 시작했습니다. Gradio는 결국 허깅페이스에 인수되었습니다.

<div class="content-ad"></div>

Lewis Tunstall은 Hugging Face의 기계 학습 엔지니어로, 오픈 소스 도구를 개발하고 널리 사용할 수 있도록 하는 데 중점을 두고 있습니다. 또한 O'Reilly의 책 "Transformers와 함께하는 자연어 처리"의 공동 저자입니다.

Leandro von Werra는 Hugging Face의 오픈 소스 팀의 기계 학습 엔지니어이자 O'Reilly의 책 "Transformers와 함께하는 자연어 처리"의 공동 저자입니다. 그는 NLP 프로젝트를 전체 기고 학습 스택을 통해 작업하여 프로덕션 단계로 이끌어온 수 년 간의 산업 경험을 가지고 있습니다.

ChatGPT 개발자를 위한 프롬프트 엔지니어링 - Coursera

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_6.png)

<div class="content-ad"></div>

링크: ChatGPT 프롬프트 엔지니어링 코스

설명: ChatGPT는 인기 있는 LLM 모델이며, 이 코스는 더 나은 응답 생성을 위한 효과적인 프롬프트 작성의 가장 좋은 방법과 필수 원칙을 공유합니다.

## 2. LLM 특화 코스

- LLM 대학 — Cohere

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_7.png)

링크: LLM 대학교

설명: Cohere는 LLM을 숙달하기 위한 전문 과정을 제공합니다. 이론적 측면에서 NLP, LLM 및 세부 아키텍처를 자세히 다루는 순차적 트랙은 초심자를 대상으로 합니다. 비순차적 경로는 내부 작동보다는 강력한 모델의 실용적인 응용 및 사용 사례에 관심이 있는 경험 많은 개인을 위한 것입니다.

2. Stanford CS324: 대형언어모형 — Stanford 사이트

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_8.png)

링크: Stanford CS324: Large Language Models

설명: 본 과정은 다양한 모델의 복잡성에 대해 심층적으로 다룹니다. 기본 원리, 이론적 구조, 윤리적 고려 사항 및 실용적 적용에 대해 다루며, 학생들은 이러한 모델에 대한 포괄적인 이해를 얻게 됩니다. 또한, 이 과정은 현실적인 시나리오에서 학습자들이 자신의 지식을 적용할 수 있는 실습 경험을 제공하여 이론과 실무 간의 간극을 메울 수 있도록 돕습니다.

3. Princeton COS597G: Understanding Large Language Models — Princeton Site

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_9.png)

링크: [대형 언어 모델 이해하기](링크)

설명: 이는 근본적인 커리큘럼을 제공하는 대학원 수준의 과정으로, 심층 학습에 탁월한 선택입니다. BERT, GPT, T5 모델, 전문가 모델의 혼합, 검색 기반 모델 등과 같은 모델의 기술적 기반, 기능, 한계를 탐험하게 됩니다.

4. 풀 스택 LLM 부트캠프 — 풀 스택

<div class="content-ad"></div>

![LLM Bootcamp](/assets/img/2024-06-22-MasterLargeLanguageModels_10.png)

링크: [Full Stack LLM Bootcamp](링크)

설명: 풀 스택 LLM 부트캠프는 산업에 적합한 종합적인 코스로, 참가자들이 LLM 애플리케이션을 개발하고 배포하는 데 필수적인 기술을 갖추도록 합니다. 커리큘럼에는 프롬프트 엔지니어링 기술, LLM의 기초, 배포 전략 및 사용자 인터페이스 디자인 등의 다양한 주제가 포함되어 있습니다. 이러한 핵심 영역을 다루면서 부트캠프는 참가자들이 현실 세계 상황에서 LLM 기반 애플리케이션을 성공적으로 개발하고 구현하는 데 필요한 지식과 전문 지식을 얻을 수 있도록 보장합니다.

5. ETH Zurich: 대 언어 모델(LLM) - RycoLab

<div class="content-ad"></div>


![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_11.png)

링크: ETH Zurich: Large Language Models

설명: 이 새롭게 디자인된 강좌는 대규모 언어 모델 (LLMs)에 대해 심층적으로 탐구합니다. 이 강좌는 이러한 모델의 기초가 되는 확률적 원리, 신경망 모델링의 복잡성, 그리고 이를 훈련하는 과정에 대해 심층적으로 다룹니다. 또한, 이 강좌는 대규모 데이터셋을 처리하고 모델 성능을 개선하는데 필수적인 확장 기술을 다룹니다. 또한, LLM의 보안 및 잠재적 남용에 대한 중요한 토론이 포함되어 있어, 이러한 첨단 기술과 관련된 기술의 능력과 윤리적 고려 사항에 대한 광범위한 이해를 보장합니다.

6. 대규모 언어 모델 파인 튜닝 — Coursera


<div class="content-ad"></div>

![image](/assets/img/2024-06-22-MasterLargeLanguageModels_12.png)

Link: [Fine Tuning Large Language Models](#)

Description: 파인튜닝은 대형 언어 모델 (LLMs)을 사용자의 특정 요구에 맞게 조정할 수 있는 기술입니다. 본 강좌를 수료하면, 파인튜닝을 적용해야 하는 시점과 데이터를 이 기술에 맞게 준비하는 방법, 그리고 새로운 데이터로 LLM을 훈련하는 과정 등을 이해하게 됩니다. 또한 파인튜닝 모델의 성능을 평가하는 방법과 목표를 달성하고 의도한 애플리케이션에서 효과적으로 작동할 수 있도록 보장하는 방법도 배울 것입니다.

# 3. 기사 / 책

<div class="content-ad"></div>

- ChatGPT이 하는 일은 무엇이며 왜 작동하는 걸까? — 스티븐 월프램

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_13.png)

링크: ChatGPT가 하는 일은 무엇이며 왜 작동하는 걸까?

설명: 명성 높은 과학자인 스티븐 월프램은 ChatGPT의 기본 측면을 탐구하는 짧은 책을 썼습니다. 그는 신경망에서의 기원부터 변압기, 주의 메커니즘 및 자연어 처리를 통해 진보한 과정을 추적합니다. 이 책은 대형 언어 모델의 기능과 한계를 이해하고 싶은 사람들에게 강력히 추천됩니다.

<div class="content-ad"></div>

2. 시리즈 기사: 대형 언어 모델 — Jay Alammar

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_14.png)

링크: 시리즈 기사: 대형 언어 모델

설명: Jay Alammar의 블로그는 대형 언어 모델(LLM)과 트랜스포머를 공부하는 사람들에게 지식의 보물창고입니다. 그의 블로그는 시각화, 직관적 설명, 그리고 주제에 대한 포괄적인 다룸이 돋보여 독특합니다.

<div class="content-ad"></div>

4. 프로덕션용 LLM 애플리케이션 구축 — Chip Huyen

![LLM Applications for Production](/assets/img/2024-06-22-MasterLargeLanguageModels_15.png)

링크: 프로덕션용 LLM 애플리케이션 구축

설명: 이 기사에서는 LLMs를 프로덕션에 적용하는 도전을 논의합니다. 작업 조합성에 대한 통찰을 제공하며 유망한 사용 사례를 소개합니다. 실용적인 LLM에 관심 있는 모든 사람들에게 정말 가치 있는 내용입니다.

<div class="content-ad"></div>

# 4. 깃허브 저장소

![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_16.png)

1. Awesome-LLM ( 15.7k ⭐ )

링크: [Awesome-LLM](https://github.com/username/Awesome-LLM)

<div class="content-ad"></div>

설명: ChatGPT를 중심으로 초대형 언어 모델(LLM)에 초점을 맞춘 논문, 프레임워크, 도구, 강좌, 튜토리얼 및 리소스가 정리된 컬렉션입니다.

2. LLMsPracticalGuide ( 8.9k ⭐ )

링크: 대규모 언어 모델에 대한 실용적인 가이드

설명: LLMs의 방대한 영역을 탐색하는 실무자들을 돕는데 도움이 됩니다. ChatGPT를 다룬 연구 논문인 "ChatGPT 및 그 이상에 대한 실무에서 LLMs의 힘을 이용하기: ChatGPT 및 이상에 대한 조사"와 이 블로그를 기반으로 합니다.

<div class="content-ad"></div>

3. LLMSurvey (9.4k⭐)

링크: LLMSurvey

설명: 이것은 "대형 언어 모델 설문"이란 제목의 논문을 기반으로 한 설문 자료 및 리소스 모음입니다. 또한 GPT 시리즈 모델들의 기술적 진화를 보여주는 그림과 LLaMA에 대한 연구 작업의 진화 그래프도 포함되어 있습니다.

4. Awesome Graph-LLM (1.4k⭐)

<div class="content-ad"></div>

링크: Awesome-Graph-LLM

설명: 그래프 기반 기술과 LLMs의 교차점에 흥미를 가지는 사람들에게 유용한 소스입니다. 이 사이트는 이 신흥 분야를 탐구하는 연구 논문, 데이터셋, 벤치마크, 서베이 및 도구를 제공합니다.

5. Awesome Langchain ( 7k ⭐ )

링크: awesome-langchain

<div class="content-ad"></div>

설명: LangChain은 LLM 프로젝트에 대한 빠르고 효율적인 프레임워크이며 이 저장소는 LangChain 생태계와 관련된 계획 및 프로젝트를 추적하는 중심입니다.

# 5. 추가 자료 — 연구 및 설문 조사 논문

![마스터 대형 언어 모델](/assets/img/2024-06-22-MasterLargeLanguageModels_17.png)

- "AIGC 시대의 ChatGPT에 대한 완전한 조사" — LLM 초보자에게는 좋은 시작점입니다. ChatGPT의 기술, 응용 및 도전에 대해 포괄적으로 다룹니다.
- "대형 언어 모델 조사" — 최근 LLM의 발전을 다루며 사전 훈련, 적응 조정, 활용 및 용량 평가의 네 가지 주요 측면을 특히 다룹니다.
- "대형 언어 모델의 도전과 응용" — LLM의 도전 과제와 성공적인 적용 영역을 논의합니다.
- "Attention Is All You Need" — Transformer는 GPT 및 기타 LLM의 기초를 이루며, 이 논문은 Transformer 아키텍처를 소개합니다.
- "주석이 달린 Transformer" — Transformer 아키텍처에 대해 자세히 설명하고 주석이 달린 하버드 대학의 자료이며, 이는 여러 LLM에 기본적인 개념입니다.
- "설명된 Transformer" — 복잡한 개념을 이해할 수 있도록 도와주는 시각 안내서로 Transformer 아키텍처를 깊이 파악할 수 있습니다.
- "언어 이해를 위한 Deep Bidirectional Transformer 사전 훈련인 BERT" — 이 논문은 BERT를 소개하며, 이는 다수의 자연어 처리(NLP) 작업에 대한 새로운 기준을 제시하는 매우 영향력 있는 LLM입니다.