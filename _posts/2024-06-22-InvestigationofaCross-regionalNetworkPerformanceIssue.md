---
title: "크로스-리전 네트워크 성능 문제 조사 방법"
description: ""
coverImage: "/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_0.png"
date: 2024-06-22 00:31
ogImage: 
  url: /assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_0.png
tag: Tech
originalTitle: "Investigation of a Cross-regional Network Performance Issue"
link: "https://medium.com/@netflixtechblog/investigation-of-a-cross-regional-network-performance-issue-422d6218fdf1"
---


Hechao Li, Roger Cruz

# 클라우드 네트워킹 토폴로지

Netflix는 SVOD(Subscription Video on Demand)와 라이브 스트리밍, 그리고 게임 서비스에 필수적인 다양한 애플리케이션을 지원하는 매우 효율적인 클라우드 컴퓨팅 인프라를 운영합니다. Amazon AWS를 활용하여, 저희 인프라는 전 세계 다양한 지역에 걸쳐 호스팅되어 있습니다. 이러한 글로벌 배포는 저희 애플리케이션이 고객에게 더 가까운 위치에서 트래픽을 제공함으로써 콘텐츠를 보다 효과적으로 전달할 수 있게 합니다. 분산 시스템과 마찬가지로, 저희 애플리케이션은 때로는 서비스 제공을 계속적으로 유지하기 위해 지역 간 데이터 동기화가 필요합니다.

다음 다이어그램은 지역 간 트래픽을 위한 간단한 클라우드 네트워크 토폴로지를 보여줍니다.

<div class="content-ad"></div>

<img src="/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_0.png" />

# 처음 봤을 때의 문제

우리의 클라우드 네트워크 엔지니어링 당직팀은 교차 지역 트래픽이 있는 애플리케이션에 영향을 미치는 네트워크 문제를 해결하기 위한 요청을 받았습니다. 초반에는 애플리케이션이 시간 초과를 경험하고 있어서 아마도 최적의 네트워크 성능으로 인한 것이라고 생각되었습니다. 우리가 다 아는 대로, 네트워크 경로가 길수록 데이터 패킷이 통과해야 하는 장치가 많아져 문제가 발생할 가능성이 높아집니다. 이 사건에서는 클라이언트 애플리케이션이 미국 지역의 내부 서브넷에 위치하고 있으며 서버 애플리케이션이 유럽 지역의 외부 서브넷에 위치하고 있는 것으로 확인되었습니다. 따라서, 데이터 패킷이 인터넷을 통해 멀리 여행해야하므로 네트워크를 탓하는 것은 자연스럽습니다.

네트워크 엔지니어로서, 네트워크에 책임을 뒤질 때 우리의 초기 반응은 보통 “아니야, 네트워크가 문제일 수 없어”이며 우리의 작업은 이를 입증하는 것입니다. 최근 네트워크 인프라에 변경 사항이 없었으며 다른 애플리케이션에 영향을 주는 AWS 이슈가 보고되지 않은 상황에서 당직 엔지니어는 소음이 심한 이웃 문제를 의심하고 호스트 네트워크 엔지니어링 팀의 지원을 받기로 결정했습니다.

<div class="content-ad"></div>

# 이웃들 탓하기

이 맥락에서 잡음이 나는 이웃 문제는 컨테이너가 다른 네트워크 집약적인 컨테이너와 호스트를 공유할 때 발생합니다. 이러한 잡음이 나는 이웃들은 과도한 네트워크 자원을 소모하여 동일한 호스트의 다른 컨테이너가 네트워크 성능이 저하되는 문제를 일으킵니다. 각 컨테이너가 대역폭 제한을 가지고 있더라도, 과다 등록은 이와 같은 문제를 초래할 수 있습니다.

동일한 호스트의 다른 컨테이너들을 조사한 결과 - 이 중 대부분은 동일한 애플리케이션의 일부였습니다 - 우리는 잡음이 나는 이웃의 가능성을 빠르게 제외했습니다. 문제가 있는 컨테이너와 모든 다른 컨테이너의 네트워크 처리량은 설정된 대역폭 제한보다 크게 낮았습니다. 우리는 이 문제를 해결하기 위해 이 대역폭 제한을 제거하여 애플리케이션이 필요한만큼의 대역폭을 사용할 수 있도록 했지만, 문제는 여전히 지속되었습니다.

# 네트워크 탓하기

<div class="content-ad"></div>

네트워크에서 RST 플래그로 표시된 몇 개의 TCP 패킷을 관찰했습니다. 이 플래그는 연결을 즉시 종료해야 함을 나타냅니다. 이러한 패킷의 빈도가 과도하게 높지는 않았지만, RST 패킷의 존재는 여전히 네트워크에서 의심을 불러일으켰습니다. 이것이 실제로 네트워크로 인한 문제인지 확인하기 위해 클라이언트에서 tcpdump를 수행했습니다. 패킷 캡처 파일에서 정확히 30초 후에 닫힌 TCP 스트림을 발견했습니다.

18:47:06에 SYN

![image](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_1.png)

3-way 핸드쉐이크 (SYN, SYN-ACK, ACK) 이후, 트래픽은 정상적으로 흘렀습니다. 18:47:36에 FIN (30초 후)까지 아무 이상이 없었습니다.

<div class="content-ad"></div>

![packet_capture_image](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_2.png)

패킷 캡처 결과를 통해 클라이언트 애플리케이션이 FIN 패킷을 보내 연결 종료를 시작했음을 명확히 확인할 수 있었습니다. 그 이후로 서버는 데이터를 계속 보냈지만, 클라이언트가 이미 연결을 닫기로 결정했기 때문에, 이후 서버에서의 모든 데이터에 대해 클라이언트가 RST 패킷으로 응답했습니다.

패킷 손실 때문에 클라이언트가 연결을 닫지 않았는지 확인하기 위해 서버 측에서도 데이터를 캡처하여 모든 서버에서 보낸 패킷이 제대로 수신되었는지 확인했습니다. 이 작업은 서버 측의 NAT 게이트웨이(NGW)를 통해 패킷이 전달되는 복잡성으로 인해 어려움이 있었습니다. 이는 서버 측에서 클라이언트의 IP 및 포트가 NGW의 것으로 나타나는데 이는 클라이언트 측에서 볼 때와 다릅니다. 따라서 TCP 스트림을 정확하게 매칭시키기 위해 클라이언트 측에서 TCP 스트림을 식별하고, 원시 TCP 시퀀스 번호를 찾아 이 번호를 서버 측에서 필터로 사용하여 해당 TCP 스트림을 찾아야 했습니다.

클라이언트와 서버 측의 패킷 캡처 결과를 통해 서버에서 보낸 모든 패킷이 클라이언트가 FIN을 보내기 전에 제대로 수신되었음을 확인했습니다.

<div class="content-ad"></div>

현재 네트워크적인 측면에서 상황이 명확해 졌어요. 클라이언트가 서버에 데이터 요청을 보내 연결을 시작했어요. 서버는 문제 없이 클라이언트에게 데이터를 계속 전송했어요. 그러나 특정 시점에, 서버는 아직 전송할 데이터가 있는데도 클라이언트가 데이터 수신을 중단하기로 선택했어요. 이로 인해 문제가 클라이언트 애플리케이션 자체와 관련이 있을 수 있다는 의심을 품게 되었어요.

# 애플리케이션에 책임을 묻다

문제를 완전히 이해하기 위해, 이제 애플리케이션이 어떻게 작동하는지 이해해야 해요. 아래 다이어그램에 표시된 것처럼, 애플리케이션은 us-east-1 지역에서 실행됩니다. 이는 교차 지역 서버에서 데이터를 읽어 동일 지역의 소비자에 데이터를 작성합니다. 클라이언트는 컨테이너로 실행되고, 서버는 EC2 인스턴스입니다.

특히, 교차 지역에서의 읽기는 문제가 있었지만 쓰기 경로는 순조롭었습니다.가장 중요한 것은 데이터를 읽는 데 30초의 응용 프로그램 수준의 시간 제한이 있었다는 것이에요. 서버에서 초기 데이터 일괄을 30초 내에 읽지 못하면 응용 프로그램(클라이언트)이 오류를 발생시켰어요. 이 시간 제한을 60초로 증가시키면 모든 것이 예상대로 작동했어요. 이것이 클라이언트가 FIN을 시작한 이유가 된 것입니다 — 서버가 데이터 전송을 기다리는 데 인내심을 잃어서였죠.

<div class="content-ad"></div>

![2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_3](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_3.png)

아마 서버는 데이터를 더 느리게 전송하도록 업데이트 되었을까요? 아니면 클라이언트 애플리케이션이 데이터를 더 느리게 수신하도록 업데이트 되었을까요? 또는 30초 이내에 완전히 전송할 수 없을 정도로 데이터 양이 너무 커졌을까요? 불행하게도, 모든 이 3가지 질문에 대해 애플리케이션 소유자로부터 부정적인 답변을 받았습니다. 서버는 변경 없이 1년 이상 운영되어 왔고, 최신 클라이언트 롤아웃에서 중요한 업데이트가 없었으며, 데이터 양도 일정했습니다.

# 커널을 탓하다

최근 네트워크와 애플리케이션 모두 변경되지 않았다면, 무엇이 변경되었을까요? 실제로, 우리는 문제가 최근에 발행된 Linux 커널 업그레이드 (버전 6.5.13에서 6.6.10으로 업그레이드)와 동시에 발생했음을 발견했습니다. 이 가설을 테스트하기 위해, 커널 업그레이드를 롤백했을 때 애플리케이션의 정상 작동이 복원되었음을 확인했습니다.

<div class="content-ad"></div>

솔직히 말해서, 그 때 나는 커널 버그라고는 믿지 않았어요. 왜냐하면 저는 커널 내의 TCP 구현은 견고하고 안정적이라고 생각했거든요 (스포일러 주의: 얼마나 틀렸는지요!). 하지만 우리는 다른 각도에서 아이디어가 바닥나 있었어요.

좋은 커널 버전과 나쁜 커널 버전 사이에는 약 14,000개의 커밋이 있었어요. 팀의 엔지니어들은 체계적이고 성실하게 두 버전 사이를 이분법적으로 나누었어요. 이분법적 접근법이 몇 개의 커밋으로 좁혀지자, 커밋 메시지에 "tcp"가 포함된 변화가 우리의 주의를 끌었어요. 최종 이분법적 접근으로 이 커밋이 문제의 근원임이 확인되었어요.

또한, 이 커밋과 관련된 이메일 기록을 검토하는 도중, 같은 커널 업그레이드 이후에 파이썬 테스트 실패를 신고한 다른 사용자를 발견했어요. 비록 그들의 해결책이 우리 상황에 직접적으로 적용되지는 않았지만, 더 간단한 테스트도 우리 문제를 재현할 수 있음을 시사했어요. strace를 사용하여, 어플리케이션이 서버와 통신할 때 다음 소켓 옵션을 구성했다는 것을 관찰했어요:

```js
[pid 1699] setsockopt(917, SOL_IPV6, IPV6_V6ONLY, [0], 4) = 0
[pid 1699] setsockopt(917, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0
[pid 1699] setsockopt(917, SOL_SOCKET, SO_SNDBUF, [131072], 4) = 0
[pid 1699] setsockopt(917, SOL_SOCKET, SO_RCVBUF, [65536], 4) = 0
[pid 1699] setsockopt(917, SOL_TCP, TCP_NODELAY, [1], 4) = 0
```

<div class="content-ad"></div>

그런 다음, 클라이언트가 동일한 소켓 옵션 집합을 구성하는 서버에서 클라이언트로 파일을 전송하는 최소한의 클라이언트-서버 C 애플리케이션을 개발했습니다. 테스트 중에는 클라이언트가 FIN을 발행하기 전에 일반적으로 30초 내에 전송되는 데이터 양을 나타내는 10M 파일을 사용했습니다. 이전 커널에서는 이 교차 지역 전송이 22초만에 완료되었으나 새 커널에서는 39초가 걸렸습니다.

# 근본 원인

최소한의 재현 설정을 통해 문제의 근본 원인을 궁극적으로 파악할 수 있었습니다. 문제의 근본 원인을 이해하기 위해서는 TCP 수신 창에 대한 이해가 필수적입니다.

## TCP 수신 창

<div class="content-ad"></div>

간단히 말하면, TCP 수신 창은 수신자가 송신자에게 "이만큼의 바이트를 ACK하지 않고 보내도 괜찮다"라고 알려주는 것입니다. 송신자가 서버이고 수신자가 클라이언트인 경우에는:

![이미지](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_4.png)

## 창 크기

이제 TCP 수신 창 크기가 처리량에 영향을 줄 수 있다는 것을 알았으니, 질문은 이 창 크기가 어떻게 계산되는지입니다. 응용 프로그램 작성자로서 당신은 창 크기를 결정할 수 없지만, 받은 데이터를 버퍼링하는 데 사용할 메모리의 양을 결정할 수 있습니다. 이는 방금 전 strace 결과에서 본 SO_RCVBUF 소켓 옵션을 사용하여 설정됩니다. 그러나 이 옵션의 값은 수신 버퍼에 대기하는 응용 프로그램 데이터의 양을 의미한다는 것을 유의하십시오. man 7 socket에서 확인할 수 있습니다.

<div class="content-ad"></div>

사용자가 값 X를 제공하면, 커널은 변수 sk_rcvbuf에 2X를 저장합니다. 다시 말해, 커널은 장부 오버헤드가 실제 데이터와 마찬가지로 많다고 가정합니다(sk_rcvbuf의 50%와 같음).

## sysctl_tcp_adv_win_scale

하지만 위의 가정이 항상 맞지는 않을 수 있습니다. 실제로 오버헤드는 최대 전송 단위(MTU)와 같은 여러 요소에 따라 다를 수 있습니다. 그래서 커널은 이 sysctl_tcp_adv_win_scale을 제공했는데, 이를 사용하여 커널에 실제 오버헤드가 얼마인지 알려줄 수 있습니다. (99%의 사람들이 이 매개변수를 올바르게 설정하는 방법을 알지 못한다고 믿습니다. 저도 분명히 하나죠. 커널인데 오버헤드를 모르는데 어떻게 나에게 그걸 알려달라고 기대하겠어요?)

sysctl 문서에 따르면,

<div class="content-ad"></div>

거의 모든 사람들이 99% 기본값 1을 사용하고 있습니다. 이는 결국 rcvbuf/2^tcp_adv_win_scale = 1/2 * rcvbuf로 오버헤드가 계산됨을 의미합니다. 이것은 SO_RCVBUF 값을 설정할 때의 가정과 일치합니다.

요약해보겠습니다. SO_RCVBUF를 65536으로 설정한다고 가정해 봅시다. 이 값은 소켓 옵션의 setsockopt 시스콜에 의해 설정된 값입니다. 그러면 다음과 같은 결과가 나옵니다:

- SO_RCVBUF = 65536
- rcvbuf = 2 * 65536 = 131072
- 오버헤드 = rcvbuf / 2 = 131072 / 2 = 65536
- 수신 창 크기 = rcvbuf — 오버헤드 = 131072–65536 = 65536

(참고: 이 계산은 단순화된 것입니다. 실제 계산은 더 복잡합니다.)

<div class="content-ad"></div>

간략히 말해서, 커널 업그레이드 전 수신 윈도우 크기는 65536이었습니다. 이 창 크기로 응용 프로그램은 30초 내에 10M 데이터를 전송할 수 있었습니다.

## 변경 내용

이 커밋은 sysctl_tcp_adv_win_scale을 더 이상 사용하지 않도록 만들었고, 오버헤드나 창 크기를 더 정확하게 계산할 수 있는 스케일링 비율을 도입했습니다. 이것이 옳은 조치입니다. 변경으로 인해, 윈도우 크기는 이제 rcvbuf * scaling_ratio입니다.

그러면 scaling_ratio는 어떻게 계산되는 걸까요? skb-`len 및 truesize로 skb 내의 tcp 데이터 길이와 skb의 총 크기를 사용하여 계산됩니다. 이는 하드코딩된 50%보다 실제 데이터를 기반으로 한 더 정확한 비율입니다. 그럼 다음 질문은 여기 있습니다: TCP 핸드셰이크 중 데이터를 전송하기 전에 초기 scaling_ratio를 어떻게 결정할까요? 답은, 대략 0.25로 설정된 마법과 보수적인 비율이 선택되었습니다.

<div class="content-ad"></div>

이제 다음과 같습니다:

- SO_RCVBUF = 65536
- rcvbuf = 2 * 65536 = 131072
- 수신 창 크기 = rcvbuf * 0.25 = 131072 * 0.25 = 32768

요약하자면, 커널 업그레이드 후 수신 창 크기가 반으로 줄었습니다. 그 결과로 처리량이 절반으로 줄어 데이터 전송 시간이 두 배로 증가했죠.

당연히 궁금증이 생길 수 있습니다. 초기 창 크기가 작은 것은 이해되지만, 나중에 페이로드의 더 정확한 비율(즉, skb-`len/skb-`truesize)이 있을 때 창이 왜 커지지 않나요? 몇 가지 디버깅을 거친 뒤에 우리는 scaling_ratio가 더 정확한 skb-`len/skb-`truesize로 업데이트된다는 것을 알 수 있었어요. 우리의 경우에는 약 0.66입니다. 그러나 다른 변수인 window_clamp는 이에 맞게 업데이트되지 않습니다. window_clamp는 광고할 수 있는 최대 수신 창이며, 초기 scaling_ratio를 사용하여 0.25 * rcvbuf로 초기화됩니다. 그 결과로 수신 창 크기가 이 값으로 제한되어 더 커질 수 없습니다.

<div class="content-ad"></div>

# 문제 해결

이론상으로는 window_clamp를 scaling_ratio와 함께 업데이트하는 것이 해결책입니다. 그러나 다른 예기치 않은 동작을 도입하지 않는 간단한 해결책을 찾기 위해 최종 결정은 초기 scaling_ratio를 25%에서 50%로 증가시키는 것이었습니다. 이로써 수신 윈도우 크기가 원래의 기본 sysctl_tcp_adv_win_scale과 하위 호환성을 유지할 수 있습니다.

한편, 문제가 변경된 커널 동작뿐만 아니라 응용 프로그램이 SO_RCVBUF를 설정하고 30초의 응용 프로그램 수준 타임아웃을 가지고 있기 때문에 발생한다는 것을 유의해야 합니다. 사실, 응용 프로그램은 Kafka Connect이며 두 가지 설정 모두 기본 구성(receive.buffer.bytes=64k 및 request.timeout.ms=30s)입니다. 또한 receive.buffer.bytes를 -1로 변경하여 Linux가 수신 윈도우를 자동 조정할 수 있도록 허용하기 위해 카프카 티켓을 작성했습니다.

# 결론

<div class="content-ad"></div>

이번 디버깅 연습은 넷플릭스의 여러 계층과 인프라를 다루는 매우 흥미로운 경험이었습니다. 사실 "네트워크"가 문제가 아니었지만, 이번에는 네트워크를 구성하는 소프트웨어 구성 요소인 커널 내의 TCP 구현이 문제였음을 발견했습니다.

만약 이러한 기술적인 도전에 흥미를 느낀다면, 저희의 클라우드 인프라 엔지니어링 팀에 합류를 고려해보세요. 넷플릭스 채용 페이지를 방문하여 Cloud Engineering 직무 기회를 살펴보세요.

# 감사의 말

저희를 위해 이 문제를 조사하고 완화하기 위해 노력한 동료인 Alok Tiagi, Artem Tkachuk, Ethan Adams, Jorge Rodriguez, Nick Mahilani, Tycho Andersen 및 Vinay Rayini에게 특별히 감사드립니다. 또한 Linux 커널 네트워크 전문가인 Eric Dumazet에게 패치를 검토하고 적용해 준 것에 대해 감사의 말씀을 전합니다.