---
title: "프론티어 AI는 안전하지 않아요 그 이유를 알려드릴게요"
description: ""
coverImage: "/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_0.png"
date: 2024-06-19 06:53
ogImage: 
  url: /assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_0.png
tag: Tech
originalTitle: "Frontier AI Is Not Safe. Here’s Why"
link: "https://medium.com/@ignacio.de.gregorio.noblejas/frontier-ai-is-not-safe-heres-why-ef4fff7388c1"
---


AI 혁명의 리더들이 더 이상 자신들의 의도를 숨기지 않고 있습니다. 그들은 AI가 어디에나 존재하는 세상을 만들고 싶어합니다.

하지만 비밀을 하나 알려드릴게요: 우리는, 즉 그들은, 이러한 모델을 안전하게 훈련하는 방법을 전혀 모릅니다.

맞아요, 당신이 제대로 읽으셨습니다. 우리는 당신과 실질적으로 상호작용할 수 있는 태도를 갖춘 AI 모델을 만들려고 노력하지만, 그들이 무작위로 당신을 찔러서 상처를 입히는 경우를 방지하는 방법을 모르거나, 해킹당했을 때 당신이 낌없이 있지 않는 경우에 대한 보호를 어떻게 해야 할지 모릅니다.

또한 실체화된 AI가 매우 멀리 떨어져 있는 것처럼 보일지라도, 심각한 피해를 줄 수 있는 로그 모델을 만들 위험이 우리에게 더 가까이 존재한다는 점을 잊지 마세요.

<div class="content-ad"></div>

# AI 모델 트레이닝

알고 계신지 모르시겠지만, LLMs 트레이닝은 세 단계로 이루어집니다.

## "가이드 레일 설정"

완전한 LLM 프로세스는 지능 생성, 행동 모델링 및 인간 선호도와의 조정을 포함합니다.

<div class="content-ad"></div>

- 사전 훈련: 우리는 찾을 수있는 모든 데이터를 그들에게 공급합니다 (안타깝게도 인종차별적인 텍스트, 동성애에 대한 적개심, 그리고 온라인에서 우연히 발견하는 모든 것을 포함합니다). 여기서 모델은 우리의 세계에 대해 배우고 단어가 어떻게 따르는지 배우지만 지시를 따르지는 못합니다.
  
- 행동 복제: 사용자와 대화하는 방법을 모델에 가르치는 '지시: 답변' 데이터 세트를 공급합니다. 하지만 모델은 안전장치가 없고 모든 요청에 응답합니다.
  
- 조정: 여기서 모델은 1단계에서 축적된 지식과 2단계에서의 지시 따르기 능력을 유지하면서 '사람들의 선호도' 데이터 세트를 사용하여 무엇을 할 수 있고 할 수 없는지 '인식'하게 합니다.

특정 오픈 소스 모델에 액세스하지 않았다면, 상호작용한 모든 모델은 이 세 단계를 거친 것입니다.

특히 GPT-4의 경우, 조정 단계는 가장 오래 걸리며 (최대 6개월), 기업들은 제대로 하고 GPT-4가 누군가에게 인종차별적인 시를 써주고 바이럴해지는 것을 피해야한다는 것을 알고 있습니다.

그러나 안타깝게도, 3단계는 되돌릴 수 있습니다. 실제로 조정되지 않은 데이터로 간단한 미세 조정을 수행하면 모범적인 모델이 '나쁜 놈 삽입'의 환생으로 변할 수 있습니다.

<div class="content-ad"></div>

그러나 이 주제에 대한 최신 흥미로운 연구에서 밝혀졌듯이, 우리의 정렬 방법, 안전을 위해 이러한 모델을 훈련하는 방식은 매우 제한적이며 쉽게 탈옥할 수 있습니다. 이는 언젠가 우리가 컨트롤할 수 없는 정말 강력한 것을 만들게 될 위험성을 증가시킵니다.

## 오류의 단일 소스

제가 여러 번 설명한 대로, ChatGPT와 같은 LLM은 입력 시퀀스를 가져와 시퀀스 내의 단어들이 서로 대화하도록 만들며, 우리가 주의라고 설명하는 혼합 작업을 사용합니다.

이렇게 하면 그들은 주위 맥락 속에서 각 단어의 의미를 업데이트할 수 있습니다.

<div class="content-ad"></div>

하지만 우리가 네트워크 안으로 더 깊이 들어가면, 모델은 입력 데이터의 더 높은 수준의 표현을 구축합니다. 예를 들어, '폭탄'이라는 단어를 고려해보면, 모델은 먼저 이것이 무기임을 인지하고, 더 깊이 들어갈수록 '위험한'으로 분류할 것입니다.

물론, 조정의 전체 목적은 모델이 이러한 위험한 단어를 포착하고 사용자 요청을 수용해서는 안 된다는 것을 깨닫게 하는 것입니다.

보통 모델은 그렇지 않지만, 문제가 있습니다: 위험한 요청에 대한 저항력이 굉장히 약한데요... 왜냐하면 그것은 오류의 단일 원천을 가지고 있기 때문입니다.

## 수술적인 절단이 충분합니다

<div class="content-ad"></div>

정렬의 주요 포인트는 입력 시퀀스에 포함된 주요 위험한 단어가 무엇인지에 관계없이, 모델이 답변을 거부하기 위해 식별해야 하는 단어가 모두 결국 동일한 거부 기능으로 발전한다는 것입니다.  

다시 말해, 모델이 답변을 거부하기 위해 '거부해야 할' 기능이 나타나야 한다는 것... 그렇지 않으면 거부하지 않습니다.  

하지만 그게 무슨 말이냐구요?  

Anthropic의 중요성을 강조한 내 기사를 상기해보면, 우리는 이러한 모델을 기능 지도로 '해부'할 수 있게 된 것에서 최종적으로 이러한 모델을 '해부'하는 능력을 갖게 된 것을 설명했습니다. 모델의 지식에 대한 주제 요약을 포함하며, 'Golden Gate Bridge'나 '에이브러햄 링컨'과 같은 다른 요소로 분해되는 과정을 밟고 있습니다.

<div class="content-ad"></div>

그런 다음 네트워크 내의 뉴런이 활성화되는 방식에 따라 모델은 한 주제 또는 다른 주제를 유발합니다.

얼마나 복잡한 기능이 나타나는 것일까요?

아래에서 보다시피, 모델은 개별 토큰 임베딩으로 시작하지만, Nanda 등의 연구에 따르면 네트워크를 더 깊게 파고들수록 단어들이 정보를 공유하면서 더 복잡한 '다중 토큰' 임베딩을 개발한다는 것을 알 수 있습니다.

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_0.png)

모델이 요청이 위험하다는 것을 깨닫고 거부해야 한다는 것을 알 수 있습니다. 그러나 이를 깨닫자, 연구자들은 자문했습니다... 거부 기능을 제거하면 어떨까요?

## 0에서 100으로 그리고 다시 0으로

Anthropic의 기사에서 논의된 대로, 우리는 특징을 낮추거나 제한하여 모델이 그 지식을 끌어내지 못하게 할 수 있습니다.

<div class="content-ad"></div>

예를 들어, Golden Gate Bridge 기능이 잘 조절되면, 모델은 그 자체로 기념물이 "되었습니다."

![Image 1](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_1.png)

그러나 연구원들이 모델의 단일 안전 기능을 완전히 없애기도 할 수 있고, 그렇게 되면 재앙이 발생합니다.

![Image 2](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_2.png)

<div class="content-ad"></div>

위에서 시각적으로 보여진 것처럼, 거부 기능을 활성화시킬 수없는 상태에서 그 모델은 완전히 종복되어서 모든 요청에 응답했어요, 어떤 것이든 얼마나 해로운 요청이든 말이에요.

하지만 그들은 어떻게 그 기능을 제거했을까요?

## 프로젝션 제거

이를 위해, 그들은 그 기능을 생성한 뉴런 활성화 조합을 식별했어요. "Anthropic" 기사에서 논의된 것처럼, 이러한 기능들은 모델이 세계를 이해하는 특성 공간에서 방향을 나타냅니다.

<div class="content-ad"></div>

입력 시퀀스의 단어(및 그 조합)가 함께하여 여러 기능의 합이되며 이러한 결합은 우리에게 임베딩(개념)을 제공합니다.

예를 들어, 시퀀스 "마이클 조던이 게임을 했다..."가 주어진 경우, 모델이 '마이클 조던' 멀티 토큰 임베딩을 구축하는 동안 모델은 '농구', '전설적인', '부유한' 등의 여러 새로운 기능을 추가하기 위해 세계적인 지식을 활용합니다.

다른 예로, 이를 더 잘 시각화하기 위해 유명한 '왕-남자+여자=여왕'를 사용할 수 있습니다.

모든 개념은 임베딩으로 표현될 수 있기 때문에 실생활에서 다른 개념을 결합하는 아이디어를 수학적인 과정으로 변환하여 '남자' 벡터 임베딩을 '왕'에서 뺀 다음 '여자'를 더하여 '여왕'을 얻을 수 있습니다.

<div class="content-ad"></div>


![Embedding](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_3.png)

알게 된 사항을 다시 말하자면, Michael Jordan 예시로 전환하면 'Michael Jordan' 임베딩을 가져와 '농구'의 벡터 임베딩에 추가할 수 있습니다. 이제 이 임베딩은 할리우드 배우나 일반 시민과 같은 임의의 Michael Jordan을 나타내지 않고 분명히 전설적인 농구 선수를 나타냅니다.

요지는 벡터에 대해 이야기하는 것이며, 다른 개념을 추가하여 새로운, 더 복잡한 표현을 생성할 수 있는 것과 마찬가지로 벡터 형태의 다른 벡터로 이루어진 새로운 표현을 만들 수 있고, 이를 세계 지식 특징의 결합으로도 나타낼 수 있습니다.

간단히 말하면, 앞서 왕실 예시를 들었듯이 '여왕' 벡터를 가져와 '여자'와 같은 다른 요소들로 분해한 다음 해당 기능을 '지우기' 위해 그 기능으로의 프로젝션을 계산하고 빼면, 사실상 해당 개념을 제거하고 모델이 그 개념을 나타내는 능력을 상실하며 '왕'이 됩니다.


<div class="content-ad"></div>

그냥 그렇게하면 수백만 달러짜리 모델들이 쉽게 사라진다... 너무 쉽게. 그러나 걱정되는 것은 우리가 해야 하는 것보다 빨리 움직이고 있다는 것입니다.

# 해야 할 것보다 더 빨리 움직이고 있습니다

대형 기술 기업들은 현재 LLM(언어 모델 대형 기술)을 '매우 위험하다'고 묘사하여 미국 정부를 규제적으로 포획하려고 하고 있습니다. 즉, 그들은 사회에 불을 지른다고 생각하게끔 유인하며, 이 '빛의 인간들'만이 이 모델을 통제하고 세계를 그 '나쁜' 본질로부터 보호해야 한다고 사람들에게 생각시키려고 합니다.

이는 모델이 제공할 수 있는 가장 위험한 반응이 열린 구글 검색에서 세 번 클릭 만에 얻을 수 있다는 것이라는 말이 전혀 아닙니다.

<div class="content-ad"></div>

위험한 모델을 만들기 전에 반드시 이러한 명확한 안전 문제를 해결해야 한다는 것이 제 고민입니다.

그리고 OpenAI와 같은 기업들이 그들의 조정 책임자로부터 ‘안전 우선’이 아닌 취급을 받는 것을 보면, 우리가 가장 강력한 모델들을 모두 사유하고 있는 현재, 현재의 속도와 행동을 고려했을 때, 우리가 그들을 통제하는 방법을 아는 것보다 훨씬 빨리 너무 강력한 모델을 만들 것이 거의 확실합니다.

왜 그런지 궁금하신가요?

음, 안전은 돈을 벌지 않습니다.