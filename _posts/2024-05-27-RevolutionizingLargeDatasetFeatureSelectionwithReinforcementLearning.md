---
title: "대용량 데이터셋 특성 선택을 혁신하는 강화 학습"
description: ""
coverImage: "/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_0.png"
date: 2024-05-27 15:05
ogImage:
  url: /assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_0.png
tag: Tech
originalTitle: "Revolutionizing Large Dataset Feature Selection with Reinforcement Learning"
link: "https://medium.com/towards-data-science/reinforcement-learning-for-feature-selection-be1e7eeb0acc"
---

## 아주 큰 데이터셋을 다룰 때 특성 선택에 강화 학습의 힘을 활용해 보세요

기계 학습 모델에 대한 특성 선택을 변화시키는 강화 학습의 힘을 경험해보세요. 실용적인 예시와 전용 Python 라이브러리를 활용하여 이 혁신적인 접근 방식의 과정, 구현, 그리고 혜택에 대해 알아보세요.

[이미지](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_0.png)

특성 선택은 기계 학습 모델을 구축하는 과정에서 결정적인 단계입니다. 모델에 좋은 특성을 선택하면 원하는 작업에 대한 결과를 향상시킬 수 있습니다. 실제로 특성은 노이즈를 추가하여 모델을 방해할 수 있습니다.

<div class="content-ad"></div>

기능을 선택하는 것은 특히 고차원 데이터 세트를 다룰 때 더 중요합니다. 이는 모델이 더 빨리 그리고 더 잘 배울 수 있게 합니다. 그 아이디어는 최적의 기능 수와 가장 의미 있는 것들을 찾는 데 있습니다.

이 글에서는 이 문제를 다루고 새롭게 구현된 기능 선택 방법을 소개할 것입니다. 다양한 기능 선택 프로세스가 있지만 이미 다뤄진 글들이 많기 때문에 여기서 소개하지 않겠습니다. 저는 강화 학습 전략을 사용한 기능 선택에 초점을 맞출 것입니다.

먼저, 강화 학습과 특히 Markov Decision Process에 대해 다룰 것입니다. 이는 데이터 과학 분야에서 매우 새로운 접근법이며 특히 기능 선택 목적으로 사용됩니다. 그 다음으로, 이를 구현하고 파이썬 라이브러리(FSRLearning)를 설치하고 사용하는 방법을 소개할 것입니다. 마지막으로, 이 구현의 효율성을 검증할 것입니다. 래퍼나 필터링과 같은 가능한 기능 선택 접근법 중에서, 강화 학습이 가장 강력하고 효율적입니다.

이 글의 목표는 구체적이고 실제 문제를 위한 구현에 중점을 두는 것입니다. 이 문제의 이론적 측면은 예제를 통해 간단히 설명되지만 참고 자료가 끝에 제공될 것입니다.

<div class="content-ad"></div>

## 강화 학습: 특성 선택을 위한 마르코프 의사 결정 문제

강화 학습 (RL) 기법이 게임 해결과 같은 문제에 매우 효율적일 수 있다는 것이 입증되었습니다. RL의 개념은 마르코프 의사 결정 과정 (MDP)에 기반을 두고 있습니다. 여기서 중요한 것은 MDP를 깊이 이해하는 것이 아니라 그 작동 방식과 어떻게 우리 문제에 유용할지에 대한 일반적인 개념을 이해하는 것입니다.

RL 뒤에 숨겨진 단순한 아이디어는 에이전트가 알 수 없는 환경에서 시작한다는 것입니다. 이 에이전트는 작업을 완료하기 위해 조치를 취해야 합니다. 에이전트의 현재 상태 및 이전에 선택한 조치에 따라, 에이전트는 일부 조치를 선택하기 쉬울 것입니다. 도달한 각 새로운 상태와 취한 조치에 대해, 에이전트는 보상을 받게 됩니다. 그래서 여기 특성 선택을 위해 정의해야 할 주요 파라미터들이 있습니다:

- 상태란 무엇인가요?
- 조치란 무엇인가요?
- 보상은 무엇인가요?
- 어떻게 조치를 선택하나요?

<div class="content-ad"></div>

첫째, 상태는 데이터 세트에 존재하는 기능들 중의 하위 집합에 불과합니다. 예를 들어, 데이터 세트에 세 가지 기능 (나이, 성별, 키)와 하나의 레이블이 있다면, 가능한 상태는 다음과 같습니다:

```js
[]                                              --> 빈 집합
[나이], [성별], [키]                             --> 1개 기능 집합
[나이, 성별], [성별, 키], [나이, 키]             --> 2개 기능 집합
[나이, 성별, 키]                                --> 모든 기능 집합
```

상태에서는 기능의 순서가 중요하지 않으며, 이를 뒤쪽에서 조금 더 설명하겠습니다. 우리는 이를 세트로 간주하고 특징들의 목록으로 간주해서는 안 됩니다.

행동에 대해서는, 하위 집합으로부터 현재 상태보다 한 가지 미탐색 기능이 더 추가된 다른 하위 집합으로 이동할 수 있습니다. 기능 선택 문제에서, 한 행동은 현재 상태에서 이미 탐색되지 않은 기능을 선택하고 다음 상태에 추가하는 것입니다. 여기에 가능한 행동의 예시가 있습니다:

<div class="content-ad"></div>


[나이] -> [나이, 성별]
[성별, 키] -> [나이, 성별, 키]


이제 불가능한 행동의 예를 살펴봅시다:


[나이] -> [나이, 성별, 키]
[나이, 성별] -> [나이]
[성별] -> [성별, 성별]


우리는 상태와 행동을 정의했지만 보상은 정의하지 않았습니다. 보상은 상태의 품질을 평가하는 데 사용되는 실수입니다. 예를 들어 로봇이 미로의 출구에 도달하려고 노력하고 다음 행동으로 출구로 가기로 결정한다면, 이 행동에 대한 보상은 "좋음"이 될 것입니다. 만일 함정으로 가기로 다음 행동을 선택하면 보상은 "나쁨"이 될 것입니다. 보상은 이전 조치에 대한 정보를 제공하는 값입니다.



<div class="content-ad"></div>

기능 선택 문제에서 흥미로운 보상은 새로운 기능을 추가함으로써 모델에 추가되는 정확도 값일 수 있습니다. 다음은 보상이 계산되는 방법의 예시입니다:

```js
[나이] --> 정확도 = 0.65
[나이, 성별] --> 정확도 = 0.76
보상(성별) = 0.76 - 0.65 = 0.11
```

첫 방문한 각 상태마다 분류기가 해당 기능 집합으로 훈련됩니다. 이 값은 상태에 저장되며 분류기의 훈련은 상태가 나중에 다시 방문되더라도 한 번만 발생합니다. 분류기는 기능의 순서를 고려하지 않습니다. 이것이 우리가 이 문제를 트리가 아닌 그래프로 볼 수 있는 이유입니다. 이 예시에서 모델에 새로운 기능으로 성별 선택 작업의 보상은 현재 상태와 다음 상태의 정확도 차이입니다.

<img src="/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_1.png" />

<div class="content-ad"></div>

위의 그래픽에서 각 기능이 숫자에 매핑되었습니다 (예: "나이"는 1, "성별"은 2이고 "키"는 3입니다). 최적의 세트를 찾기 위해 다른 메트릭을 사용하는 것이 완전히 가능합니다. 많은 비즈니스 응용 프로그램에서 정확도보다 재현율이 더 중요하게 여겨집니다.

다음 중요한 질문은 현재 상태에서 다음 상태를 선택하는 방법이거나 환경을 어떻게 탐사하는지입니다. 매우 복잡한 문제가 될 수 있기 때문에 가장 최적의 방법을 찾아야 합니다. 실제로, 문제에서 10개의 기능이 있는 경우 모든 가능한 기능 세트를 단순히 탐색하면 상태의 수가

```js
10! + 2 = 3,628,802개의 가능한 상태
```

입니다.

+2는 빈 상태와 모든 가능한 기능을 포함하는 상태를 고려했기 때문입니다. 이 문제에서는 정확도를 극대화하는 기능 세트를 얻기 위해 모든 상태에서 동일한 모델을 교육해야 할 것입니다. 강화 학습 접근 방식에서는 모든 상태를 방문할 필요가 없고 이미 방문한 상태로 이동할 때마다 모델을 교육할 필요가 없을 것입니다.

<div class="content-ad"></div>

이 문제에 대한 몇 가지 중지 조건을 결정해야 했고, 이를 나중에 자세히 설명할 것입니다. 현재 epsilon-탐욕 상태 선택이 선택되었습니다. 이것은 현재 상태에서 epsilon(0과 1 사이, 주로 0.2 정도)의 확률로 다음 동작을 무작위로 선택하고, 그렇지 않은 경우에는 함수를 최대화하는 동작을 선택하는 아이디어입니다. 특징 선택에 대한 함수는 각 특징이 모델의 정확도에 기여한 보상의 평균입니다.

Epsilon-탐욕 알고리즘에는 두 단계가 포함됩니다:

- 무작위 단계: epsilon의 확률로, 현재 상태의 가능한 이웃 중에서 다음 상태를 무작위로 선택합니다 (균일하게 또는 소프트맥스 선택이라고 상상할 수 있음)
- 탐욕 단계: 현재 상태에 추가된 기능이 모델의 정확도에 대한 최대 기여를 갖도록 다음 상태를 선택합니다. 시간 복잡성을 줄이기 위해, 각 특징에 대한 이러한 값을 포함하는 목록을 초기화했습니다. 이 목록은 특징이 선택될 때마다 업데이트됩니다. 업데이트는 다음 공식 덕분에 매우 최적화되었습니다:


<img src="/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_2.png" />


<div class="content-ad"></div>

- AORf: 기능 "f"가 가져온 보상의 평균
- k: "f"가 선택된 횟수
- V(F): 특징 집합 F의 상태 값 (이 기사에서는 상세히 설명되지 않음)

전체 아이디어는 어떤 기능이 모델에 가장 높은 정확도를 제공했는지 찾는 것입니다. 이것이 바로 우리가 여러 다른 환경에서 여러 상태를 검토하여 모델에 대한 가장 전반적인 정확도 값을 평가해야 하는 이유입니다.

마지막으로 두 가지 중지 조건을 설명하겠습니다. 알고리즘이 방문하는 상태의 수를 최소화하는 것이 목표이기 때문에 이를 주의 깊게 살펴봐야 합니다. 방문한 상태가 적을수록 서로 다른 기능 집합으로 학습해야 할 모델의 양이 줄어듭니다. 정확도를 얻기 위해 모델을 학습하는 것은 시간과 계산 능력면에서 가장 비용이 많이 드는 단계입니다.

- 알고리즘은 모든 기능을 포함하는 집합인 최종 상태에서 반드시 중지됩니다. 이 상태에 도달하는 것을 피하기 위해 모델을 학습하는 데 가장 비용이 많이든다.
- 또한, 연속적으로 값이 감소하는 방문된 상태들의 시퀀스가 발생하면 그래프를 조회하는 것을 중지합니다. 데이터 세트의 총 기능 수의 제곱근 이후에는 계속 탐색을 중단하는 임계값이 설정되어 있습니다.

<div class="content-ad"></div>

문제의 모델링이 설명되었으므로, 이제 파이썬에서의 구현에 대해 자세히 설명하겠습니다.

## 강화 학습을 사용한 특성 선택을 위한 파이썬 라이브러리

이 문제를 해결하는 파이썬 라이브러리가 있습니다. 이 부분에서 그 동작 방식을 설명하고 그것이 효율적인 전략임을 입증하겠습니다. 또한, 이 문서는 설명서로 작용하여 해당 부분이 끝나면 이 라이브러리를 여러분의 프로젝트에 사용할 수 있을 것입니다.

## 1. 데이터 전처리

<div class="content-ad"></div>

방문한 상태의 정확도를 평가해야 하기 때문에, 해당 기능 및 이 기능 선택 작업에 사용된 데이터를 모델에 제공해야 합니다. 데이터는 정규화되어야 하며 범주형 변수는 인코딩되어야 하며 가능한 적은 행을 가지고 있어야 합니다 (행이 적을수록 알고리즘은 더 빠릅니다). 또한, 기능과 일부 정수 사이의 매핑을 생성하는 것이 매우 중요합니다. 이 단계는 필수는 아니지만 매우 권장됩니다. 이 단계의 최종 결과는 예측할 수 있는 라벨과 함께 모든 기능이 포함된 DataFrame을 얻는 것입니다. 아래는 기준으로 사용된 데이터셋 예시입니다 (UCI Irvine Machine Learning Repository에서 확인할 수 있습니다).

## 2. FSRLearning 라이브러리 설치 및 가져오기

두 번째 단계는 라이브러리를 pip을 사용하여 설치하는 것입니다. 여기에 설치 명령어가 있습니다:

```js
pip install FSRLearning
```

<div class="content-ad"></div>

라이브러리를 가져오려면 아래 코드를 사용합니다:

단순히 Feature_Selector_RL 객체를 만들어서 기능 선택기를 만들 수 있을 겁니다. 몇 가지 매개변수를 채워야 합니다.

- feature_number (정수): DataFrame X에 있는 피처 수
- feature_structure (사전): 그래프 구현을 위한 사전
- eps (부동 소수 [0; 1]): 무작위 다음 상태를 선택할 확률, 0은 오직 탐욕 알고리즘이고 1은 오직 무작위입니다.
- alpha (부동 소수 [0; 1]): 업데이트 속도를 제어합니다, 0은 거의 업데이트하지 않는 상태이고 1은 매우 업데이트됩니다.
- gamma (부동 소수 (0, 1]): 다음 상태의 관찰을 조절하는 요소, 0은 근시적인 상태이고 1은 장기적인 행동을 나타냅니다.
- nb_iter (정수): 그래프를 통해 이동할 수열의 수
- starting_state ("empty" 또는 "random"): "empty"인 경우 알고리즘은 빈 상태에서 시작하고, "random"인 경우 그래프의 무작위 상태에서 시작합니다.

모든 매개변수가 튜닝될 수 있지만 대부분의 문제에 대해 일부 이터레이션만으로 충분합니다 (대략 100회) 그리고 epsilon 값이 0.2 정도면 충분합니다. 시작 상태는 그래프를 효율적으로 더 탐색하는 데 유용하지만 데이터셋에 매우 의존적일 수 있고 두 값 모두 테스트할 수 있습니다.

<div class="content-ad"></div>

마침내 다음 코드로 선택기를 매우 간단하게 초기화할 수 있습니다.

알고리즘을 훈련하는 것은 대부분의 머신 러닝 라이브러리와 동일한 기초에 따라 매우 쉽습니다.

다음은 출력 예시입니다:


![Output Example](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_3.png)


<div class="content-ad"></div>

다음은 5-튜플 출력입니다:

- 데이터프레임 X 내 특성의 인덱스(마치 맵핑처럼)
- 특성이 관측된 횟수
- 반복이 끝난 후 특성에 의해 가져온 보상의 평균
- 특성의 중요도를 가장 적은 것부터 가장 중요한 것까지 순위로 나타낸 것(여기서 2는 가장 적은 중요도이고 7은 가장 중요한 특성입니다)
- 전역적으로 방문된 상태의 수

이 선택기의 또 다른 중요한 방법은 Scikit-Learn의 RFE 선택기와 비교하는 것입니다. X, y 및 선택기의 결과를 입력으로 사용합니다.

선택 과정마다 RFE와 FSRLearning의 전역 측정 지표를 출력합니다. 또한 모델의 정확도에 대한 시각적 비교를 출력합니다. x축에는 선택된 특성 수가 있고 y축에는 정확도가 있습니다. 두 개의 수평 선은 각 방법의 정확도 중앙값입니다. 다음은 예시입니다:

<div class="content-ad"></div>

```js
평균 기준 정확도: 0.854251012145749, 강화학습 정확도: 0.8674089068825909
중간 기준 정확도: 0.8552631578947368, 강화학습 정확도: 0.868421052631579
RFE보다 더 좋은 메트릭을 가진 변수 세트를 얻을 확률: 1.0
두 곡선 사이의 면적: 0.17105263157894512
```

이 예시에서는 강화학습 방법이 항상 RFE보다 모델을 위한 더 좋은 특성 집합을 제공했습니다. 따라서 우리는 정렬된 특성 집합 중에서 확실히 선택할 수 있으며, 그것은 모델에 더 높은 정확도를 제공할 것입니다. 여러 번 모델과 비교자를 실행하여 매우 정확한 평가를 얻을 수 있지만 강화학습 방법이 항상 더 좋습니다.

또 다른 흥미로운 방법은 get_plot_ratio_exploration입니다. 이는 지정된 이터레이션에 대해 이미 방문한 노드 수와 순서대로 방문한 노드 수를 비교하는 그래프를 그립니다.



<div class="content-ad"></div>



![그림1](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_5.png)

또한, 두 번째 중지 조건 덕분에 알고리즘의 시간 복잡도가 지수적으로 감소합니다. 따라서 기능의 수가 많더라도 수렴이 빨리 이루어질 것입니다. 아래 그림은 특정 크기의 집합이 방문된 횟수를 나타냅니다.

![그림2](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_6.png)

모든 반복에서 알고리즘이 6개 이하의 변수를 포함하는 상태를 방문했습니다. 6개 이상의 변수를 넘어가면 도달한 상태의 수가 줄어드는 것을 볼 수 있습니다. 이는 작은 기능 집합으로 모델을 훈련하는 것이 큰 기능 집합보다 빠르기 때문에 좋은 동작입니다.



<div class="content-ad"></div>

# 결론 및 참고 자료

전반적으로 RL 방법은 모델의 지표를 극대화하는 데 매우 효율적임을 알 수 있습니다. 항상 흥미로운 특성 하위 집합으로 빠르게 수렴합니다. 또한 FSRLearning 라이브러리를 사용하면 ML 프로젝트에서 이 방법을 매우 쉽고 빠르게 구현할 수 있습니다.

프로젝트의 Github 저장소와 완벽한 문서는 여기에서 확인할 수 있습니다.

문의 사항이 있으시면 직접 링크드인에서 연락하실 수 있습니다.

<div class="content-ad"></div>

이 라이브러리는 다음 두 논문을 참고하여 구현되었습니다:

- Sali Rasoul, Sodiq Adewole, 및 Alphonse Akakpo, FEATURE SELECTION USING REINFORCEMENT LEARNING (2021), ArXiv
- Seyed Mehdin Hazrati Fard, Ali Hamzeh, 및 Sattar Hashemi, Using reinforcement  learning to find an optimal set of features (2013), ScienceDirect

