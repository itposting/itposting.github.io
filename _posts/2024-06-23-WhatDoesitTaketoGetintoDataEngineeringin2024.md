---
title: "2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까"
description: ""
coverImage: "/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png"
date: 2024-06-23 16:28
ogImage: 
  url: /assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png
tag: Tech
originalTitle: "What Does it Take to Get into Data Engineering in 2024?"
link: "https://medium.com/towards-data-science/what-does-it-take-to-get-into-data-engineering-in-2024-5efb9c41865b"
---


<img src="/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png" />

만약 이 글을 읽고 있다면, 요즘 직업을 바꾸려고 고민 중이신 것 같군요. 아마도 소프트웨어 엔지니어링과 데이터베이스 디자인에 관심이 있으신 거겠죠. 당신의 배경이 무엇이든 중요하지 않아요 — 마케팅, 분석, 혹은 금융 분야에서 오셨든, 여러분도 할 수 있어요! 이 이야기는 데이터 공간에 빠르게 진입하는 방법을 찾아주기 위한 것이에요. 예전에 저도 똑같이 해서 그 뒤로 후회한 적이 없어요. 기술 분야, 특히 데이터는 매력과 혜택이 넘쳐나요. 선뜻 언급하지 않은 원격 근무와 선도 기업들로부터의 막대한 혜택 패키지도 말이죠. 파일과 숫자로 마법을 부릴 수 있다는 사실 그 자체가 멋진 거 아니겠어요? 이 글에서는 2~3개월 안에 완료할 수 있는 기술과 프로젝트들을 요약해서 소개할 거에요. 상상해보세요, 몇 달간의 노력만으로 첫 직장 면접 준비를 마치고 있을 수 있답니다.

## 왜 데이터 엔지니어링이고 데이터 과학이 아닌가요?

사실 왜 데이터 분석이나 데이터 과학이 아니고 데이터 엔지니어링을 선택하는 걸까요? 저는 이 역할의 본질에 답이 있다고 생각해요. 데이터 엔지니어가 되기 위해서는 소프트웨어 엔지니어링과 데이터베이스 디자인, 기계 학습(ML) 모델, 그리고 데이터 모델링과 비즈니스 인텔리전스(BI) 개발을 배워야 해요.

<div class="content-ad"></div>

데이터 엔지니어링은 DICE에 따르면 가장 빠르게 성장하는 직업입니다. 그들은 갭을 보여주기 위해 연구를 수행했습니다. 빨리 움직이세요.

![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_1.png)

데이터 과학자는 장기간 시장에서 "가장 매혹적인" 직업으로 여겨져 왔지만, 최근에 데이터 엔지니어의 부족이 있다는 것으로 보입니다. 이 분야에서 엄청난 수요가 있음을 확인할 수 있습니다. 이는 경험이 풍부하고 높은 자격을 갖춘 엔지니어들 뿐만 아니라 입문자도 포함됩니다. 지난 5년 동안 영국에서 데이터 엔지니어링은 가장 빠르게 성장하는 직업 중 하나였으며, 2023년 LinkedIn의 인기 있는 직업 목록에서 13위를 차지했습니다. 평균적으로 매주 약 4차례의 취업 면접 요청을 받습니다. 입문 데이터 엔지니어들은 훨씬 더 자주 초대될 것입니다.

데이터 엔지니어링이 매우 복잡하기 때문에 급여와 혜택 package는 다른 기술 분야보다 훨씬 더 좋아보입니다. 실제로, 데이터가 잡다하고 지루한 데이터 조작 작업인 것처럼 보여 소프트웨어 엔지니어 중에는 데이터를 피하는 것을 선호하는 사람들도 많이 있습니다. 이로 인해 데이터 엔지니어링은 데이터 플랫폼 및 데이터 파이프라인 디자인 패턴을 배우려는 사람들에게 수익성 있는 목표가 되고 있습니다. 데이터 엔지니어링은 데이터 조작 및 프로세스 조정에 관한 것입니다. 데이터는 정제되고 테스트되고 승인되어 사용자에게 적시에 전달되어야 합니다. 이는 ML 및 BI가 그것을 많이 의존하는 이유입니다.

<div class="content-ad"></div>

이 이야기에서는, 2~3개월 안에 달성할 수 있는 기술 세트와 가능한 프로젝트를 요약해 보려고 합니다. 상상해보세요, 몇 달 동안 적극적인 학습을 한 후에는 첫 직장 면접 준비가 완료되어 있게 됩니다.

## 데이터 엔지니어링은 압도적으로 느껴질 수 있어요

STEM(과학, 기술, 공학, 수학) 배경 없이 데이터 엔지니어링에 뛰어드는 것은 매우 어려울 수 있어요. 코딩 자체가 쉬운 일이 아니에요. 데이터베이스 및 데이터 파이프라인 오케스트레이션은 처음부터 이해하기가 더욱 어려웠어요. 몇 년 전에 저는 양적 금융을 전공한 석사 학위를 받고 분석 매니저로 일했었어요. 코딩을 배우기로 결정한 날을 기억해요. 대학에서 얻은 수준처럼은 아니지만 실제 세계의 문제를 해결하기 위해 실무에서 나의 기술을 적용할 수 있도록 배우고자 한 것이죠.

일상적으로 업무를 수행하면서 여가 시간에만 소프트웨어 엔지니어링을 배워야 했던 어려움이 있었던 것도 기억이 나네요.

<div class="content-ad"></div>

나는 Fiverr과 PeoplePerHour에서 프로젝트를 찾아보며 기업들이 데이터에 대해 어떤 것을 필요로 하는지 살펴보았던 기억이 납니다. 지금 생각해보면 이것이 많은 고객들의 진정한 고통 포인트를 이해하는 데 많은 도움이 되었고 아마도 가장 효율적인 학습 방법이었을 것입니다.

그래서 모든 데이터 실무자를 향한 첫 번째 조언은 믿음입니다.

데이터 엔지니어링 분야에 진입하는 것은 압도적으로 느껴질 수 있지만 가치가 있습니다. 부끄러워하지 마시고 글을 쓰는 사람들에게 물어보세요. Medium은 그런 점에서 정말 좋은 곳입니다. 왜 취향에 맞는 주제를 확인하고 누구를 팔로우할지 확인해보지 않으세요?

## 계획

<div class="content-ad"></div>

쉬는 시간을 가져가서 진정으로 그것이 필요한지 생각해보세요. 만약 답이 '예' 라면 필요한 건 계획 뿐입니다. 이 곳에서 목표는 속도가 아닙니다. 가능하다면 아무런 고통 없이 "데이터 엔지니어링에 진입하는 방법"을 실현 가능한 해결책으로 기록하는 것이 이제의 목표입니다.

지금은 단지 다음 몇 달에 집중하고자 하는 것과 복습해야 할 것들을 생각해봅시다.

## 데이터 엔지니어링의 습관

첫 두 주 동안 실제로 배우면서 이 습관을 습득하고자 할 것입니다. 조금씩 하되 꾸준히 합니다. 학습의 습관을 형성해야 합니다. 예를 들어, 저는 Google Professional Data Engineer 시험 준비를 하면서 이렇게 했습니다. 매일 아침에 체육관에서 사이클을 타면서 책을 읽었죠. 아침이 가장 생산적인 시간이기 때문에 그 때 진행했습니다. 이 2020년의 글은 여전히 유효합니다. 많은 것들이 실제로 변한 게 많지 않고, 학습은 주로 데이터 엔지니어링의 기본 원칙에 대해 집중했습니다. 물론, 제품 특정 질문이 많았지만, 이 글은 빠르게 학습하는 방법에 대한 지침서입니다.

<div class="content-ad"></div>

데이터 엔지니어링은 다음과 같은 기술 영역들에 관련이 있어요:

- ETL 및 데이터 추출
- 데이터 조작과 데이터 모델링 (대개 SQL을 사용함)
- 파이프라인 테스트
- 데이터 테스트
- 보고 및 비즈니스 인텔리전스
- MLOps 및 기계 학습 파이프라인
- 이 모든 것을 조율하는 것

## 첫 1-2주: SQL

일단 SQL에 집중해 볼게요. 목록에서 첫 번째 항목은 아니지만 저는 이것이 가장 보편적이라고 생각해요. SQL 방언은 데이터 모델링에서 널리 사용되어 왔기 때문에 이제 데이터 조작의 표준으로 간주될 수 있어요. 처음 두 주 동안 해야 할 것은 다양한 SQL 쿼리를 실행해보고 어떤 데이터 파이프라인에서 사용될 수 있는지 상상하는 거에요. 여기서 다시 정리하고 싶은 것들은 아마도 다음과 같을 거에요:

<div class="content-ad"></div>

- SQL을 사용하여 테이블을 만드는 방법
- 공통 테이블 표현식을 사용하는 방법
- SQL을 사용하여 데이터를 모킹하는 방법
- 증분 전략을 사용하여 테이블을 업데이트하는 방법
- 데이터 품질을 테스트하고 데이터를 정리하는 방법

이러한 질문들은 압도될 수 있을지도 모르지만, 많은 훌륭하고 간단한 예제들이 있습니다. 이러한 예제들은 몇 가지 무료 데이터 웨어하우스 솔루션과 결합하여 비교적 간단하고 생산적인 샌드박스를 만드는 데 도움이 될 수 있습니다. 이에 대해 이전 이야기 중 하나에서 이야기했었습니다. SQL 관련해서는 일상적인 데이터 엔지니어링에 실제로 필요한 모든 것입니다.

가장 어려운 주제인 MERGE와 같은 주제도 SQL이 CTE 내에 목된 데이터를 포함할 때 쉽게 설명될 수 있습니다:

```js
create temp table last_online as (
    select 1 as user_id
    , timestamp('2000-10-01 00:00:01') as last_online
)
;
create temp table connection_data  (
  user_id int64
  ,timestamp timestamp
)
PARTITION BY DATE(_PARTITIONTIME)
;
insert connection_data (user_id, timestamp)
    select 2 as user_id
    , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp
union all
    select 1 as user_id
        , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp
union all
    select 1 as user_id
        , timestamp_sub(current_timestamp(),interval 20 hour) as timestamp
union all
    select 1 as user_id
    , timestamp_sub(current_timestamp(),interval 1 hour) as timestamp
;

merge last_online t
using (
  select
      user_id
    , last_online
  from
    (
        select
            user_id
        ,   max(timestamp) as last_online

        from 
            connection_data
        where
            date(_partitiontime) >= date_sub(current_date(), interval 1 day)
        group by
            user_id

    ) y

) s
on t.user_id = s.user_id
when matched then
  update set last_online = s.last_online, user_id = s.user_id
when not matched then
  insert (last_online, user_id) values (last_online, user_id)
;
select * from last_online
;
```

<div class="content-ad"></div>

## 주 3–4: 현대 데이터 스택

현대 데이터 스택 및 데이터 플랫폼 아키텍처 유형에 대한 몇 가지 이야기를 읽는 것을 추천합니다. 데이터 엔지니어링에서 사용할 수 있는 다양한 도구 및 프레임워크에 대한 전략적 개요를 제공하여, 채용 인터뷰 중에 기술에 능통하다는 것을 채용 담당자에게 알려줍니다. 모든 도구를 알 필요는 없지만, "당신은 이 분야에 계십니까"라는 질문은 잠재 고용주와의 첫 만남 중에 상상할 수 있는 가장 매혹적인 질문입니다. 여기서는 기술의 최근 이벤트(IPO, 합병 및 인수), 개발 및 새로운 도구에 대한 인식을 보여주고 싶습니다. DuckDB 또는 Polars와 같은 것에 대해 들었다고 언급하는 것만으로도 여러분이 호기심이 많고 열정적이라는 것을 사람들에게 알려줍니다.

시장에 있는 다양한 데이터 도구로 쉽게 길을 잃을 수 있습니다. 눈송이(Snowflake)에 대해 이야기하고, 그 IPO가 얼마나 성공적이었는지를 언급했던 것을 기억합니다. 그것이 많은 도움이 되었거나 적어도 우리가 인터뷰어와 동일한 의견을 가졌다는 것 같아요. 우리는 현대 데이터 스택 및 그것을 현대적이고 견고하며 비용 효율적으로 만드는 요소들에 대해 토론했습니다. 간단히 말해서, 데이터를 다루기 위해 사용되는 도구 모음입니다. 데이터를 어떻게 처리할 것인가에 따라, 이러한 도구는 다음을 포함할 수 있습니다:

- 관리되는 ETL/ELT 데이터 파이프라인 서비스
- 클라우드 기반 관리되는 데이터 웨어하우스/데이터 레이크(데이터의 대상지)
- 데이터 변환 도구
- 비즈니스 인텔리전스 또는 데이터 시각화 플랫폼
- 기계 학습 및 데이터 과학 기능

<div class="content-ad"></div>

앞선 두 주 동안 SQL을 사용하여 데이터를 변환하고 조작하는 방법을 이미 배웠어요. 이제 이 전략적인 지식을 활용하는 방법을 알게 됐으니, 그에 맞게 활용해봐요.

![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_2.png)

## 5-6주차: 파이썬 기초

지금은 파이썬에 대한 지식을 되짚거나 살짝 배우는 시기예요. 파이썬은 정말 배우기 쉬운 방법이죠. 스크립트 형식이라 코드를 읽기 쉽고 유용한 라이브러리가 많습니다. 이 모든 특징들로 인해 데이터 엔지니어링에서 프로그래밍 언어로 많이 선택되었어요. 반복문, 함수, 조건문, 오류 처리, 그리고 데이터 구조와 같은 기본적인 프로그래밍 개념에 초점을 맞출 거예요. 데이터 엔지니어링에서는 이런 것들을 자주 사용할 거라고 생각해요.

<div class="content-ad"></div>

제안드리는 바는 데이터 API 및 요청을 통해 시작하는 것입니다. 이러한 지식을 클라우드 서비스와 결합하면 미래에 필요한 모든 ETL 프로세스를 위한 매우 좋은 기반이 마련됩니다.

전형적인 데이터 파이프라인 [5]은 Python 함수(또는 오퍼레이터)의 연쇄이며 다음과 같이 보일 것입니다:


![pipeline](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_3.png)


우리는 Python 함수를 사용하여 데이터를 처리하며, 결과적으로 다음과 같은 파이프라인을 얻을 수 있습니다.

<div class="content-ad"></div>


![image](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_4.png)

More examples can be found here:

## API requests

이해API 요청 방법은 중요합니다. 이것은 ETL 서비스가 다른 서비스와 상호 작용하는 주요 방법이기 때문입니다. 즉, 데이터를 추출합니다. 데이터 엔지니어는 API 서비스에 요청을 보내어 데이터를 요청할 때 많이 사용합니다. 그런 다음 실제로 변환 (ETL)하는 데이터를 페이징하거나 스트림으로 응답합니다. 아래 예제를 고려해보십시오. 이 예제는 NASA 소행성 API에서 데이터를 추출하는 방법을 설명합니다. 이는 매우 간단한 예제이며 매우 배우기 쉬운 이유입니다.


<div class="content-ad"></div>

```python
# nasa.py
import requests
session = requests.Session()

url="https://api.nasa.gov/neo/rest/v1/feed"
apiKey="your_api_key"
requestParams = {
    'api_key': apiKey,
    'start_date': '2023-04-20',
    'end_date': '2023-04-21'
}
response = session.get(url, params=requestParams, stream=True)
print(response.status_code)
```

더 고급이고 실행 가능한 예제는 [여기 이야기](6)에서 찾을 수 있습니다.

## 7–8 주차: 추출 — 적재

파이썬과 SQL을 조금 배우면 실제 데이터를 추출하고 클라우드 어딘가에 저장할 수 있습니다. AWS, GCP 및 Azure와 같은 클라우드 서비스 제공업체들이 시장을 선도하고 있으며 그 중 적어도 하나에 익숙해지는 것이 필수적입니다. 그래서 이제 우리는 실제로 첫 번째 데이터 파이프라인을 만들고 싶어할 것입니다. 이것은 간단한 함수일 수 있습니다. NASA 소행성 데이터를 추출하여 AWS S3에 저장하는 것이다. 그게 다입니다! 아주 간단하지만 이것이 우리의 첫 번째 데이터 파이프라인이며 매일, 매 시간 등으로 실행되도록 예약할 수 있습니다. 서버리스 마이크로서비스로 배포하고 무료로 실행되어 클라우드 저장 공간에서 데이터를 추출 및 보존합니다. AWS 웹 UI를 사용하여 쉽게 배포할 수 있습니다. 그러나 서비스를 배포하는 더 선호되는 방법은 인프라스트럭처 코드입니다. 해당 주제는 본질적으로 이해하기 어려우며 초보자이신 경우 깊게 파고들지 않는 것이 좋습니다.


<div class="content-ad"></div>

다음 몇 주 동안은 명령줄 도구에 주로 초점을 맞추고 클라우드 기능을 배포하고 클라우드에서 리소스를 프로비저닝하는 몇 가지 트릭을 익히는 것을 추천합니다.

간단한 AWS 람다처럼 ETL 서비스를 생성할 수 있습니다:

```js
# AWS CLI를 사용하여 패키지된 람다 배포:
aws \
lambda create-function \
--function-name etl-service-lambda \
--zip-file fileb://stack.zip \
--handler <당신의 람다 핸들러 경로>/app.lambda_handler \
--runtime python3.12 \
--role arn:aws:iam::<당신의 AWS 계정 ID>:role/my-lambda-role

# # 이미 배포되었다면 업데이트를 위해 다음을 사용합니다:
# aws --profile mds lambda update-function-code \
# --function-name mysql-lambda \
# --zip-file fileb://stack.zip;
```

예를 들어, AWS CLI를 사용하여 ETL 서비스를 호출할 수 있습니다:

<div class="content-ad"></div>

```js
aws lambda invoke \
    --function-name etl-service-lambda \
    --payload '{ "data": "value" }' \
    response.json
```

일반적인 코드 예시와 람다 패키지를 사용한 예시는 여기 [7]에서 확인할 수 있습니다.

이제 데이터가 클라우드에 저장되었으므로 데이터 웨어하우스 도구로 불러올 수 있습니다. 저는 1TB 데이터 스캔당 $5가 청구되는 BigQuery를 권장합니다. 테스트 데이터로 작업하게 되므로 비용이 전혀 발생하지 않습니다. 다음과 같이 외부 테이블을 만들 수 있습니다 [8]:

```js
# 데이터가 Google Cloud Storage에 저장되어 있다고 가정
LOAD DATA INTO source.nasa_asteroids
FROM FILES(
  format='JSON',
  uris = ['gs://nasa-asteroids-data/*']
)
```

<div class="content-ad"></div>

또한 AWS CLI의 기본을 배우고 싶다면 이 기사를 시도해보실 수도 있습니다 [9]

## 9-10주차

이번에는 Python에서 단위 테스트의 기본을 배우고 ETL 서비스를 조정하는 방법에 대해 알아보고 싶어요. 이미 Python 함수를 만들어 API 호출을 수행하고 데이터를 추출하는 방법을 배웠죠. 아마도 우리는 Python for Data Engineers의 예제를 사용하여 Python에서 데이터를 변환하는 방법도 배웠을지도 모르겠어요. 이제는 적용한 데이터 변환 로직을 테스트하고 싶어요. 단위 테스트는 소프트웨어 엔지니어링에서 필수적인 기술이며 장기적으로 많은 시간을 절약해줍니다. 코드를 테스트하고 유지하는데 도움이 되죠. 요약하자면 Pytest 모듈의 기본을 배우고 싶습니다. 예를 들어, 아래와 같은 ETL 함수의 논리를 테스트할 수 있어야 합니다:

```js
# etl.py
def etl(item):
    # 여기서 데이터 변환을 수행
    return item.lower()
```

<div class="content-ad"></div>

Python 스크립트를 실행하기만 하면 간단히 이 작업을 수행할 수 있습니다 [10]:

```js
# etl_test.py
from etl import etl

def test_etl_returns_lowercase():
    assert etl('SOME_UPPERCASE') == 'some_uppercase'
```

unittest 라이브러리를 사용한 또 다른 예시를 살펴보겠습니다:

```js
# ./prime.py
import math

def is_prime(num):
    '''num이 소수인지 확인합니다.
    '''
    for i in range(2,int(math.sqrt(num))+1):
        if num%i==0:
            return False
    return True
```

<div class="content-ad"></div>

unittest을 사용하면 간단해요. 다음과 같이 테스트할 거예요:

```js
# ./test.py
import unittest
from prime import is_prime

class TestPrime(unittest.TestCase):

    def test_thirteen(self):
        self.assertTrue(is_prime(13))
```

이제 테스팅의 기본을 알았으니, 함수들이 올바른 데이터를 반환하는지 확신할 수 있어요. 그러므로 다음 단계는 ETL 프로세스를 조정하는 추가적인 마이크로서비스를 배포하는 것이죠. 요약하면, 이는 간단한 AWS Lambda 함수거나 스케줄에 따라 우리의 ETL 서비스를 호출할 수 있는 다른 서버리스 애플리케이션이 될 수 있어요. 이것은 매우 간단하며 여기서 복잡하게 하고 싶지 않아요. 다른 Python Lambda 함수를 배포하고 매일 또는 매시간 실행되도록 스케줄을 지정할 거예요. 그것을 위해 AWS EventBridge 이벤트를 사용하고 cron 스케줄을 설정할 수 있어요. 우리의 Orchestator Lambda 코드는 아래와 같이 보일 거에요.

```js
import json
import boto3
 
# 다른 Lambda를 호출하기 위한 AWS Lambda 클라이언트
client = boto3.client('lambda')
 
def lambda_handler(event, context):
 
    # 다른 Lambda로 전달할 데이터
    data = {
        "ProductName": "iPhone SE"
    }
 
    response = client.invoke(
        FunctionName='arn:aws:lambda:eu-west-1:12345678:function:etl-service-lambda',
        InvocationType='RequestResponse',
        Payload=json.dumps(data)
    )
 
    response = json.load(response['Payload'])
 
    print('\n')
    print(response)
```

<div class="content-ad"></div>

만약 더 알고 싶다면, AWS Step Functions 및 Infrastructure as Code와 관련된 고급 튜토리얼이 있습니다.

## 11–12 주차

ML 기본 지식을 배우세요. 데이터를 추출하고 ETL을 수행하는 방법 및 데이터 웨어하우스로 데이터를 로드하는 방법을 이미 알고 있습니다.

아래 튜토리얼을 살펴보세요. 이 튜토리얼은 사용자 이탈을 다루고 행동 데이터를 사용하여 사용자 이탈을 예측하는 방법을 설명합니다. 몇 시간 만에 완료할 수 있지만 이탈에 대한 구체적인 내용을 깊게 파고들고 싶다면 더 많은 시간이 걸릴 수 있습니다. 모든 머신 러닝 모델을 알 필요는 없습니다. 우리는 머신 러닝 및 데이터 과학 분야에서 아마존 및 구글과 경쟁할 수는 없지만, 사용하는 방법을 알아야 합니다. 클라우드 서비스 제공 업체들이 제공하는 다양한 관리형 ML 서비스가 있으며, 그것들에 친숙해져야 합니다. 데이터 엔지니어들은 이러한 서비스를 위해 데이터 세트를 준비하며, 이에 대해 몇 개의 튜토리얼을 진행하는 것이 유용할 것입니다.

<div class="content-ad"></div>

## 결론

데이터 엔지니어링을 빠르게 학습하고 이 분야의 전문가가 되어야 한다고 자신을 누르지 마세요. 많은 사람들에게는 특정 분야를 숙달하는 데 몇 년이 걸리기 때문에 주말에 학습하면서 몇 달 안에 이룰 수 있는 목표에 집중하는 것이 좋습니다. 데이터 엔지니어는 ETL/ELT 기술, 데이터 모델링에 대한 충분한 지식이 필요하며 적어도 Python에서 코딩할 수 있어야 합니다. 이 글에서는 데이터 엔지니어링을 가장 효율적으로 배울 수 있는 12주 계획을 개요로 설명했습니다. 즐기시기 바랍니다.

## 추천 도서

[1] https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/

<div class="content-ad"></div>

[2] [Google Professional Data Engineer 시험을 2020년에 통과한 방법](https://towardsdatascience.com/how-i-passed-google-professional-data-engineer-exam-in-2020-2830e10658b6)

[3] [초보자를 위한 고급 SQL 기술](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488)

[4] [데이터 플랫폼 아키텍처 유형](https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7)

[5] [데이터 파이프라인 디자인 패턴](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)

<div class="content-ad"></div>


[6] [Python for Data Engineers](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)

[7] [Building a Batch Data Pipeline with Athena and MySQL](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)

[8] [When Your Stack is a Lake House](https://medium.com/towards-artificial-intelligence/when-your-stack-is-a-lake-house-6bcb17f9bff6)

[9] [Mastering AWS CLI](https://medium.com/geekculture/mastering-aws-cli-5454ad5e685c)

<div class="content-ad"></div>

[10] [Python을 사용한 데이터 파이프라인 테스트 안내](https://towardsdatascience.com/a-guide-to-data-pipeline-testing-with-python-a85e3d37d361)

[11] [데이터 파이프라인 Orchestration](https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a)