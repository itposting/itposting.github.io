---
title: "이미지-텍스트 다중모달 기반 모델은 어떻게 작동하나요"
description: ""
coverImage: "/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_0.png"
date: 2024-06-19 06:47
ogImage: 
  url: /assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_0.png
tag: Tech
originalTitle: "How Does an Image-Text Multimodal Foundation Model Work"
link: "https://medium.com/towards-data-science/how-does-an-image-text-foundation-model-work-05bc7598e3f2"
---



![How Does an Image-Text Multimodal Foundation Model work](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_0.png)

요즘에는 다중 모드 기반 모델이 급증하고 있습니다. 이 모델들은 텍스트, 이미지, 비디오, 오디오 등 다양한 종류의 데이터를 이해하며, 둘 이상의 모드로부터의 데이터 지식이 필요한 작업을 수행할 수 있습니다. 이러한 모델들이 어떻게 작동하는지 궁금한 적이 있나요?

다중 모드 모델을 이해하는 핵심은 서로 다른 데이터 모드 간의 정렬 방식을 이해하는 것입니다.

본 문서에서는 CoCa라는 간단한 이미지-텍스트 이중 모드 모델을 사용하여 다중 모드 모델의 내부 작업 방식을 설명합니다. 저는 CoCa를 좋아합니다. CoCa는 직관적인 디자인을 가지고 있으며, 다른 다중 모드 모델에서 아이디어를 차용했습니다.


<div class="content-ad"></div>

# 왜 이미지-텍스트 모델인가?

왜 우리는 이미지-텍스트 이중 모달 모델을 원하는 걸까요? 이 질문에 대한 답변을 하기 전에, 먼저 단일 모달 모델이 무엇을 할 수 있고 무엇을 할 수 없는지 생각해 봅시다.

이미지만을 다루는 모델

예를 들어 ResNet과 같은 단일 이미지 모달 모델은 오직 이미지에서 정보를 배우고, 이미지 분류와 같은 기본적인 이미지 이해 작업을 수행할 수 있습니다. 이는 이미지를 미리 정의된 클래스 집합 중 하나로 분류하는 작업을 말합니다. 예를 들어, 고양이 클래스 또는 개 클래스로 이미지를 분류합니다.

<div class="content-ad"></div>

단일 텍스트 모달리티 모델은 트랜스포머와 같이 텍스트만을 통해 정보를 학습하며, 이전 단어들을 기반으로 다음 단어를 예측하는 작업과 같은 작업을 수행할 수 있어요.

## 단일 모달리티 모델이 작동하지 않는 사용 사례

위 두 가지 단일 모달리티 모델은 다음과 같은 작업을 수행할 수 없어요. 두 마리 개가 달리는 그림을 달리는 예시로 사용해볼게요.

<div class="content-ad"></div>

![이미지](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_1.png)

- 텍스트 기반 이미지 검색입니다. 예를 들어, "개가 달리는 이미지"를 찾습니다. 위의 이미지 전용 분류 모델은 위의 이미지를 개 클래스로 분류할 수 있습니다. 그러나 검색 쿼리를 입력할 수 없으며 이미지만 입력으로 허용하고 미리 정의된 클래스 중 하나를 예측합니다.
- 깊은 이미지 유사성 검색입니다. 예를 들어, 위의 이미지가 주어지면 유사한 이미지를 찾습니다. 위의 이미지 전용 분류 모델이 할 수 있는 최선은 동일한 개 클래스의 이미지를 제공하는 것입니다. 달리 달리는 개 이미지를 특별히 찾지는 않습니다.
- 이미지 캡션 생성입니다. 예를 들어, 위의 이미지가 주어지면 "모래 위를 나란히 달리는 두 마리 개"와 같은 설명 문장 또는 캡션을 생성합니다. 이미지 전용 또는 텍스트 전용 모델 모두 이 작업을 수행할 수 없습니다: 이미지 전용 모델은 "개" 클래스를 생성하고 더 이상의 세부 정보를 생성할 수 없습니다. 텍스트 전용 다음 단어 예측 모델은 이미지를 입력으로 수용하지 않습니다.
- 이미지 질의 응답입니다. 이미지와 질문이 주어졌을 때 그 질문에 대한 답변을 생성합니다. 예를 들어, 위의 이미지와 "왼쪽 개 옆을 누가 뛰고 있는가"라는 질문이 주어진 경우, 모델이 "다른 개"를 생성하도록 원합니다. 이미지 전용 또는 텍스트 전용 모델은 이 경우에 작동할 수 없음이 분명하며 둘 다 (이미지, 질문) 쌍을 입력으로 수용하지 않습니다.

이러한 사용 사례를 지원하기 위해 모델은 이미지와 텍스트에 대해 알고 있어야 하며 이 두 모드를 조화롭게 이해해야 합니다. 다시 말해서 모델은 이미지와 해당 설명이 "모래 위를 나란히 뛰는 두 마리 개"와 같은 기본 개념임을 알아야 합니다. 그리고 모델은 두 수준에서 이 조합을 이해해야 합니다:

- 전역 수준에서, 개념이 "모래 위를 나란히 뛰는 두 마리 개"임을 의미합니다.
- 지역 수준에서, 모델은 이미지 일부가 한 마리 개에 대한 것이고 다른 부분은 다른 개에 대한 것이며 그들이 달리고 있다는 사실을 이해해야 합니다. 텍스트도 마찬가지로 다수의 개에 관해 이야기하며 달리는 것에 대해 이야기합니다.

<div class="content-ad"></div>

# 전역 이미지 및 텍스트 정보 정렬 방법

다음 그림은 이미지와 텍스트에 대한 전역 및 지역 수준 정보를 표현하는 방법과 대조적인 손실을 사용하여 전역 수준에서 이미지와 텍스트 간의 정렬을 설정하는 방법을 보여줍니다. 곧 무엇이 정렬을 의미하는지 명확해질 것입니다.

![이미지와 텍스트의 정렬 방법](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_2.png)

위 그림은 훈련 중에 모델이 입력 이미지-텍스트 쌍 (이미지ᵢ, 텍스트ᵢ)를 받았을 때의 상황을 보여줍니다. "i"아래 첨자가 있는 이유는 (이미지ᵢ, 텍스트ᵢ)가 미니 배치 내의 쌍 세트 중 하나임을 나타냅니다.

<div class="content-ad"></div>

단계 1. 이미지 입력 이미지ᵢ는 해상도가 288×288이고 RGB 채널이 세 개인 고정 크기의 컬러 이미지입니다. ViT 인코더와 attentional pooling 구성 요소를 거쳐, 이 288×288×3 이미지는 형태가 257×512인 텐서로 변환됩니다. 이 변환에 대한 자세한 설명은 나중에 설명할 예정이니 일단은 이미지가 256개의 작은 패치로 나뉘고, 리스트 끝에 새로운 클래스 패치를 추가하여 이러한 패치 간 정보를 섞는다는 것만 이해하면 됩니다. 이 모델은 257×512 출력 텐서를 두 부분으로 분할합니다.

- 256개의 행으로 구성된 첫 번째 부분은 img_tokensᵢ라고 하며, 256×512 모양을 가지며 입력 이미지로부터 인코딩된 256개 패치를 나타냅니다. 이는 지역 수준 이미지 정보를 나타냅니다.
- 1개의 행으로 이루어진 마지막 부분은 img_clsᵢ이라고 하며, 1×512 모양을 가지며, 추가된 클래스 패치에서 나온 것으로 전역 수준 이미지 정보를 나타냅니다. 몇 개의 패치가 있든, 이 고정된 크기의 img_clsᵢ 벡터의 중요성을 깨달아 주세요. — 이미지 전체를 요약하는데 있어 이 벡터가 중요한 역할을 합니다. 물론 저희 경우에는 패치의 개수가 항상 256개입니다. 하지만 입력 이미지가 다른 수의 패치로 분할되었을 때에도, 이 고정된 크기의 img_clsᵢ 텐서는 이미지 전체를 요약합니다.

단계 2. 텍스트 입력 텍스트ᵢ는 최대 397단어로 패딩된 문장입니다. 여기서의 397은 저희 모델이 지원하는 텍스트의 최대 길이를 나타내는 임의의 숫자입니다. 397보다 짧은 텍스트의 경우에는 원래의 텍스트 뒤에 PAD로 표시된 패딩 단어가 추가됩니다. 문장에 START, END, CLS라는 3개의 특수 단어가 추가됩니다. 이 특수 단어들은 어휘에 있는 다른 단어들과 마찬가지로 처리됩니다.

예를 들어, 특수 단어가 추가된 후 "w₁ w₂ w₃"라는 문장은 "START w₁ w₂ w₃ PAD PAD … END CLS"가 됩니다. 이로써 400단어로 된 텍스트 입력이 생성됩니다:

<div class="content-ad"></div>

- START 단어는 텍스트의 시작을 나타냅니다.
- END 단어는 텍스트의 끝을 나타냅니다.
- CLS 단어는 글로벌 텍스트 정보에 대한 텍스트 클래스를 나타냅니다.

언어 모델 인코더는 이 400단어로 이루어진 텍스트 입력을 400×512 형태의 행렬로 변환합니다. 이 행렬을 attended_textᵢ라고 부를 것입니다. 이 언어 모델 인코더는 입력된 각 단어에 대한 임베딩을 출력합니다. 따라서 이 attended_textᵢ의 각 행은 해당하는 순서의 단어의 임베딩을 나타냅니다.

400×512 attended_textᵢ 행렬은 두 부분으로 나뉩니다:

- 첫 399행은 text_tokenᵢ로, 399×512 형태로 구성되며 실제 단어들에 대한 로컬 레벨 텍스트 정보를 나타냅니다.
- 마지막 행은 text_clsᵢ로, 1×512 형태로 구성되며 글로벌 레벨 텍스트 정보를 나타냅니다. 다시 한 번 강조하면 이 고정 크기의 text_clsᵢ 텐서가 얼마나 강력한지 이해하는 것이 중요합니다: 원래 입력 문장이 얼마나 길든 상관없이 256개의 길이를 가진 텐서입니다! text_clsᵢ 벡터의 크기가 글로벌 이미지 정보 벡터 img_clsᵢ의 크기와 동일하기 때문에, 이미지와 문장 사이의 유사성을 계산하는 방법을 이미 알 수 있습니다. 그 길이가 같은 벡터들 사이의 점곱입니다. 또한, OpanAI 텍스트 임베딩 API가 비슷한 작업을 수행한다고 상상할 수 있습니다.

<div class="content-ad"></div>

모델은 미리 훈련된 언어 모델 변환기를 사용하지 않으며, 이 변환기의 매개변수를 최적화합니다.

단계 3. 대조 손실은 전체 이미지 정보 텐서 img_clsᵢ와 전체 텍스트 정보 텐서 text_clsᵢ 간의 정렬을 설정합니다. 두 텐서 모두 형태가 1×512인 벡터입니다. 이들 간의 정렬은 이 두 벡터 간의 내적으로 정의됩니다. 내적은 벡터 유사성을 측정합니다. 두 벡터는 내적 측면에서 서로 더 유사하면 더 정렬되어 있습니다.

내적은 입력 (이미지ᵢ, 텍스트ᵢ)에 대한 img_clsᵢ와 text_clsᵢ의 유사성을 측정합니다. 어떤 텐서들이 유사해야 하는지 설정하는 대조 손실은 또한 어떤 텐서들이 유사하지 않아야 하는지도 설정해야 합니다:

- 이미지ᵢ에 대한 전체 수준 이미지 정보 벡터 img_clsᵢ는 다른 모든 (이미지, 텍스트) 쌍의 텍스트에서 가져온 전역 텍스트 정보 텐서와 유사해서는 안 됩니다.
- 텍스트ᵢ에 대한 전체 수준 텍스트 정보 text_clsᵢ 벡터는 다른 모든 (이미지, 텍스트) 쌍의 이미지에서 가져온 전역 이미지 정보 텐서와 유사해서는 안 됩니다.

<div class="content-ad"></div>

위의 비유사점을 전체 훈련 데이터 세트에서 모든 (이미지, 텍스트) 쌍에 대해 계산하는 것은 분명히 매우 비싸다. 대안으로, 훈련 중에 대조 손실은 크기 N의 미니 배치 내에서 (이미지, 텍스트) 쌍에 대해서만 계산됩니다. 이 미니 배치에는 (이미지₁, 텍스트₁), (이미지₂, 텍스트₂), ..., (이미지_N, 텍스트_N)의 N개의 쌍이 포함됩니다.

전체 대조 손실은 다음과 같습니다:

![equation](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_3.png)

위의 수식에서:

<div class="content-ad"></div>

- img_clsᵢ은 (imageᵢ, textᵢ) 쌍의 이미지에서 가져온 전역 수준 이미지 정보 텐서입니다.
- text_clsᵢ는 (imageᵢ, textᵢ) 쌍의 텍스트에서 가져온 전역 수준 텍스트 정보 텐서입니다.
- "·"은 벡터 내적 연산을 의미합니다.
- σ는 하이퍼파라미터로, 보통 온도라고 불립니다. 이는 유사성의 차이가 얼마나 민감한지를 조절합니다. 만약 σ가 무한대에 가깝다면, then log 함수 내부의 분수는 img_clsᵢ, text_clsᵢ 및 text_clsⱼ 벡터의 값이 무엇이든지 관계없이 항상 1/N으로 평가됩니다. 따라서 이 경우 손실은 해당 벡터 유사성에 전혀 민감하지 않습니다. σ가 작은 경우에는 손실이 이 유사성에 더 민감해집니다.
- 대조 손실에는 이미지 대 텍스트 항과 텍스트 대 이미지 항 두 가지 항이 있습니다.
- 이미지 대 텍스트 항은 이미지와 텍스트의 전역 정보 텐서 간 유사성이 (imageᵢ, textᵢ) 쌍의 img_clsᵢ와 모든 다른 쌍에서 가져온 전역 텍스트 정보 텐서인 text_clsⱼ의 모든 유사성의 합에 비해 더 큰지를 요구합니다. 여기서 j는 미니 배치에서 쌍을 선택합니다.
- 이미지 대 텍스트 항을 최대화하려고 합니다. 왜냐하면 동일한 쌍 벡터 간의 유사성을 크게, 다른 쌍 벡터 간의 유사성을 작게 하고 싶기 때문입니다. 동등하게, 이 항의 부정값을 최소화할 수 있습니다. 이게 바로 위의 손실을 정의하는 방식입니다.
- 텍스트 대 이미지 항은 이미지 대 텍스트 항과 동일한 구조를 가지고 있지만, 텍스트에서 이미지로 방향이 다릅니다.
- 로그 함수 내부에서 이미지 대 텍스트 항과 텍스트 대 이미지 항은 동일한 분자를 가지지만 다른 분모를 가지고 있습니다. 이것이 하나의 항 대신 두 개의 항이 필요한 이유입니다.

# 이러한 전역 수준 정렬 모델은 어떤 작업을 수행할 수 있나요?

위의 대조 손실로 훈련된 이미지-텍스트 모델은 다음 작업을 수행할 수 있습니다.

## 텍스트 기반 이미지 검색

<div class="content-ad"></div>

텍스트 검색 쿼리인 "two dogs running side by side"와 같이 사용되는 언어 모델 변환기를 사용하여 글로벌 수준의 텍스트 정보 텐서를 생성한 후, 이 텐서를 검색 키로 사용하여 글로벌 수준의 이미지 정보 텐서를 저장하는 데이터베이스에서 해당 키로 이미지를 검색하여 유사한 이미지를 검색합니다. 순위 결정 기준은 내적 유사도입니다.

## 딥 이미지 유사성 검색

위와 같은 두 마리 강아지가 있는 이미지와 같은 이미지 쿼리가 주어지면 ViT+attentional pooling 컴포넌트를 사용하여 글로벌 수준의 이미지 정보 텐서를 생성한 후, 이 텐서를 동일한 이미지 데이터베이스에서 검색 키로 사용하여 유사한 이미지를 검색합니다.

## 이미지 분류

<div class="content-ad"></div>

이미지 분류 작업에는 "개" 또는 "고양이" 또는 "테니스 라켓"과 같은 각 클래스가 단문인 사전 정의된 클래스 집합이 함께 제공됩니다.

입력 이미지를 분류하려면 먼저 ViT+어텐션 풀링 구성 요소를 사용하여 입력 이미지에 대한 전역 수준 이미지 정보 텐서를 생성합니다.

그런 다음 템플릿을 사용하여 각 클래스에 대한 문장을 생성합니다. 예를 들어 템플릿 "this is a picture of `class`"을 사용하면 다음과 같이 생성됩니다:

- 강아지 클래스에 대해 "이것은 개의 사진입니다",
- 고양이 클래스에 대해 "이것은 고양이의 사진입니다".

<div class="content-ad"></div>

마지막으로, 각 문장에 대해 언어 모델 변환기를 사용하여 전역 수준의 텍스트 정보 텐서를 생성하십시오. 입력 이미지를 처리하여, 점곱 방식에서 이미지 정보 텐서와 가장 유사한 전역 수준 텍스트 정보 텐서를 갖는 클래스로 이미지를 분류합니다.

# 이미지 캡션 지원 방법

이미지 캡션 작업은 이미지만으로 텍스트 설명을 생성합니다. 현재의 전체적인 이미지-텍스트 맞춤형 모델은 이를 수행할 수 없습니다. 이 기능을 활성화하기 위해서는 지역 수준의 이미지 정보와 텍스트 정보를 맞춰야 합니다.

캡션은 문장입니다. 입력 이미지에 대한 문장을 모델이 어떻게 예측할 수 있을까요? "START two running dogs END"와 같은 특수 단어를 문장에 추가하여, 모델은 다음을 훈련할 수 있습니다:

<div class="content-ad"></div>

- 이미지와 첫 번째 단어 "START"가 주어지면, 다음 단어 "two"를 예측합니다. 특별 단어를 추가하는 좋은 점은 항상 특별 "START" 단어를 사용할 수 있다는 것입니다. 우리는 이를 첫 번째 예측된 단어로 취급합니다.
- 그런 다음 이미지와 지금까지 예측된 단어인 "START two"가 주어지면, 모델은 다음 단어 "running"을 예측할 수 있습니다.
- 그런 다음 이미지와 "START two running" 단어가 주어지면, 모델은 다음 단어로 "dogs"를 예측할 수 있습니다.
- 그런 다음 이미지와 "START two running dogs" 단어가 주어지면, 모델은 마지막 단어 "END"를 예측할 수 있습니다.

## 어휘 내 단어 확률 예측

머신러닝에서 다음 단어를 예측하는 것은 일반적으로 어휘 내 모든 단어의 확률 배열을 예측하고, 원하는 단어의 해당 배열 항목의 확률을 모든 다른 단어보다 높게 설정하는 방식으로 구현됩니다.

우리의 (이미지ᵢ, 텍스트ᵢ) 쌍에 대한 이미지 캡션 작업에서, 이전에 예측된 모든 단어 및 로컬 수준 이미지 정보 img_tokensᵢ를 고려하여 다음 단어의 확률을 예측할 수 있는 모델이 필요합니다.

<div class="content-ad"></div>

Notationally, we define this word probability predictor as:

![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_4.png)

where:

- textᵢ is the original text with special words START and END added, such as "START w₁ w₂ w₃ END".
- T is the length, or the number of words, of textᵢ.
- img_tokensᵢ is the local level image information tensor, which is created by the ViT + attentional pooling component.

<div class="content-ad"></div>

## 위 단어 확률 예측기를 우리의 신경망이 어떻게 구현하는지?

다음 그림은 이전 그림의 연장선인데, 위의 단어 확률 예측기가 어떻게 구현되었는지를 보여줍니다.

![이미지](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_5.png)

이 그림은 이전 그림에서 대조 손실을 계산하는 단계 3을 공간을 절약하기 위해 생략했습니다. 그러나 단계 3이 여전히 대조 손실을 생성하기 위해 존재함을 상기해주시기 바랍니다.

<div class="content-ad"></div>

4단계. 동일한 언어 모델 변환기는 399×512 형태의 로컬 수준 텍스트 정보 텐서인 text_tokensᵢ를 새로운 동일한 형태의 텐서 cross_attended_textᵢ로 변환합니다. 이 변환 과정에서 로컬 수준 이미지 정보 텐서인 img_tokensᵢ가 교차 주의를 통해 결과 텐서에 섞입니다. 이를 통해 로컬 수준 텍스트 정보와 이미지 정보 간의 조정이 이루어집니다. 교차 주의에서는 메인 계산이 점곱이기 때문에 로컬 이미지와 텍스트 해석 간의 이 조정이 더 타당해집니다.

5단계. 399×512 크기의 cross_attended_textᵢ 텐서를 399×20000 word_probability_matrixᵢ로 선형 변환합니다. 20000은 어휘 크기를 나타내는 임의의 숫자입니다. 이 선형 변환층 내에는 학습 가능한 매개변수가 들어 있습니다.

word_probability_matrixᵢ의 각 행의 값, 예를 들어 w₁_prob, w₂_prob은 [0, 1] 범위로 정규화되어 있고, 이들의 합은 1이 되도록 설계되어 있습니다. 각 행을 어휘의 20000 단어에 대한 예측 확률로 해석하기 위한 의도가 담겨 있습니다.

중요한 한 가지 관찰 결과는 word_probability_matrixᵢ에서 첫 번째 행은 시작 단어 START를 주면서 w₁에 대한 단어 확률로 해석됩니다. 두 번째 행은 START와 w₁을 주면서 w₂에 대한 확률을 나타내며, 세 번째 행은 START, w₁ 및 w₂를 주면서 w₃에 대한 확률을 보여줍니다. 입력 텐서와 출력 텐서 간의 이 일회성 조정을 통해 모든 이전 단어를 고려한 다음 단어 예측 메커니즘이 구현됩니다.

<div class="content-ad"></div>

"단어 확률로 해석된다"는 무슨 뜻인가요? 이것은 단어 예측 정확도를 측정하는 새로운 손실 함수인 캡션 손실에 관한 맥락에서 의미가 있습니다. 이 캡션 손실은 각 예측된 단어 확률 텐서와 실제 실제 단어 간의 교차 엔트로피를 사용하여 예측 정확도를 측정하며, 이 정확도를 최대화하고자 합니다. 즉, 적도로, 이것을 최소화하고자 합니다.

따라서 w₁_prob에 대한 맥락에서는, 이는 [0, 1] 사이의 값이고 총합이 1인 20000개의 항목으로 구성된 벡터인데, 교차 엔트로피는 이 벡터의 "two"라는 단어에 대한 항목이 클 값이어야 한다는 것을 요구한다. 지역 수준 이미지 정보 텐서와 첫 번째 단어 "START"가 주어졌을 때, 예측해야 할 다음 단어는 "two"여야 합니다.

## 단일 예측된 단어에 대한 교차 엔트로피

다음 그림은 단어 확률 벡터 w₁_prob이 "two"라는 단어의 원핫(one-hot) 인코딩과 비교되어 "two"에 대한 w₁_prob의 항목이 커야 한다는 것을 요청하는 방식을 보여줍니다. 이 말은 모델이 이 위치에서 "two"라는 단어를 예측해야 한다는 것과 동등합니다.

<div class="content-ad"></div>

"단어 'two'에 대한 원핫 인코딩은 'two' 단어의 위치에 '1'이 들어가 있고, 다른 모든 위치에 '0'이 들어갑니다. 예측 확률 텐서 w₁_prob에서는 숫자가 랜덤하게 사용되어 해당 값들이 언어 모델 변환기에 의해 생성된 것을 설명합니다.

![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_6.png)

이 단어 w₁에 대한 텍스트ᵢ의 교차 엔트로피는 다음과 같습니다:

![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_7.png)"

<div class="content-ad"></div>

위 손실 함수는 "i" 위첨자가 있으므로 (이미지ᵢ, 텍스트ᵢ) 쌍에서 한 단어에 대한 것입니다.

단어 당 캡션 손실이 정의되었으므로, 텍스트ᵢ의 전체 문장에 대한 교차 엔트로피는 399개 단어의 교차 엔트로피의 합입니다:

![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_8.png)

<div class="content-ad"></div>

미니배치 전체의 손실은 이미지ₖ와 텍스트ₖ 쌍 (여기서 k는 1부터 N까지)의 모든 손실입니다.

![Image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_9.png)

## 모든 단어들이 동시에 예측됩니다

이전 그림의 단계 5로 돌아가 봅시다. 이제 우리는 설명에 있는 모든 단어들이 동시에 예측된다는 것을 볼 수 있습니다.

<div class="content-ad"></div>

- 첫 번째 행이 P(w₁|START, img_tokensᵢ)를 나타내고,
- 두 번째 행이 P(w₂|START, w₁, img_tokensᵢ)를 나타내며,
- 세 번째 행이 P(w₃|START, w₁, w₂, img_tokensᵢ)를 나타내고,
- 이어서 계속됩니다.

## 전체 손실

우리 모델의 전체 손실 함수는 대조 손실과 캡션 손실의 선형 조합입니다.

<img src="/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_10.png" />

<div class="content-ad"></div>

두 개의 λ 계수는 이 두 가지 손실 구성 요소 사이의 가중치로 사용되는 하이퍼파라미터입니다.

## 이미지 캡션 생성

이제 우리는 전체 손실로 훈련된 이 모델이 이미지에 대한 캡션을 생성하는 방법을 볼 수 있습니다:

- 입력 이미지로부터 로컬 레벨 이미지 정보 텐서를 생성하기 위해 ViT+어텐션 풀링 구성 요소를 사용합니다.
- 시작 단어만을 포함하는 캡션 접두어를 구성합니다.
- 언어 모델 트랜스포머를 두 번 사용하여(첫 번째로 어텐션된 텍스트를 생성하고, 두 번째로 교차 어텐션된 텍스트를 생성하기 위해), 다음 단어를 예측하기 위해 선형 투영을 거쳐 단어 확률을 생성합니다. 캡션 접두어로부터 다음 단어를 예측합니다. 이 두 단어를 각각 predicted_w₁, predicted_w₂로 부르겠습니다. 이 두 단어는 단계 3의 두 번의 실행에 의해 생성됩니다.
- 새롭게 예측된 단어를 캡션 접두어에 추가합니다.
- END 단어가 예측될 때까지 단계 3에서 4를 반복합니다.

<div class="content-ad"></div>

패딩

언어 모델 변환기는 400단어의 고정 길이 입력을 받습니다. 따라서 캡션 생성의 시작 부분에서 변환기로 보내는 입력에는 많은 패딩 단어가 포함됩니다.

예를 들어, 첫 번째 입력 문장은 “START PAD … PAD”이고, 두 번째는 “START predicted_w₁ PAD … PAD”이며, 세 번째는 “START predicted_w₁ predicted_w₂ PAD … PAD” 등입니다. 다시 말해, 문장은 항상 399단어로 패딩되어야 한 뒤 언어 모델 변환기로 전송됩니다.

언어 모델 변환기의 출력(두 번의 적용 후)은 399×20000 단어 확률 행렬입니다. 매번 이 행렬의 한 행만 사용하여 다음 단어의 가장 높은 확률을 찾고, 그 다음 실제 단어를 찾습니다. 이 399×20000 행렬의 다른 행은 사용되지 않습니다.

<div class="content-ad"></div>

이 프로세스를 최적화하는 방법이 있지만, 이 글에서는 개념을 이해하는 데 집중하고 최적화에 대해 걱정할 필요는 없습니다.

# 이미지 질문 응답을 지원하는 방법은?

이미지 캡션 작업 방식을 이해한 후에 이미지 질문 응답 작업 방식을 동일하게 이해하는 것이 더 쉽습니다:

- 이미지 캡션 작업은 입력 이미지와 텍스트 접두사 "START"로 시작됩니다.
- 이미지 질문 응답 작업은 입력 이미지와 질문을 텍스트 접두사로 시작합니다.

<div class="content-ad"></div>

그거에요.

문제의 단어를 무시해야 하는 예측 단어 확률 행렬의 행들에 관해서 섬세함이 있습니다. 하지만 이에 대해 시간을 투자해 이해하려고 결정한다면 이해할 수 있다고 믿습니다.

하지만 이미지에 대한 질문 답변 모델을 구축하려면 이미지 캡션 손실을 다르게 처리해야 할 필요가 있습니다. 이미지 캡션에 대한 손실은 다음 단어 예측 문제를 다루지만 이미지에 대한 질문 응답에 대한 손실은 다른 문장을 제공하면서 문장 예측 문제를 다룹니다.

# Foundation models

<div class="content-ad"></div>

지금 우리는 모델이 한 번 훈련된 후에 새로운 작업을 수행하거나 새로운 작업을 수행하도록 다시 훈련할 수 있다는 것을 알 수 있습니다. 이것이 바로 왜 이 모델을 기반 모델이라고 부르는 이유입니다. 기반 모델은 다양한 하류 작업을 위한 시작점으로 기반 또는 부분적으로 완성된 모델을 제공합니다.

# ViT 인코더와 어텐션 풀링

대부분의 모델이 설명되었으니, 이제 마지막 부분으로 넘어가 보겠습니다 — ViT 인코더와 어텐션 풀링을 사용하여 모델이 어떻게 전역 및 지역 수준의 이미지 정보 텐서를 생성하는지 알아봅시다.

## ViT 인코더

<div class="content-ad"></div>

ViT 인코더는 고정 크기 이미지를 패치 목록으로 분할하여 각 패치를 인코딩합니다. 다음 그림은 ViT 인코더가 인코딩된 이미지 패치를 생성하는 방법을 보여줍니다. 여기에서 i 아래 첨자를 무시했습니다. img_tokensᵢ를 소개할 때 사용한 것과 같은 이유입니다. 이전에는 미니 배치에서 텐서에 대해 이야기해야 했는데, 그 텐서는 손실 함수에 들어가며 손실 함수는 미니 배치로 정의됩니다. 그러나 여기에서는 더 이상 미니 배치에 대해 이야기할 필요가 없습니다.

![이미지](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_11.png)

ViT 인코더는 고정 크기 이미지를 처리하는 표준 트랜스포머 기반 이미지 인코더로, 논문 '이미지는 16x16 단어만큼 가치가 있다'에서 소개되었습니다. 이 ViT 인코더는 오직 고정 크기 이미지와 함께 동작합니다. 위 그림은 이 ViT 인코더가 작동하는 방식을 보여줍니다:

- 우리의 경우 288×288×3 크기의 컬러 RGB 이미지(따라서 컬러 채널은 3)로 표현되는 고정 크기 이미지를 받았을 때, ViT 인코더는 먼저 이미지를 18×18 크기의 256개 패치로 분할합니다.
- 그런 다음 ViT 인코더는 각 패치를 1024 길이의 벡터로 변환하여 256×1024 행렬인 'encoded_img_patches'라고 불리는 것을 얻습니다.

<div class="content-ad"></div>

ViT 인코더가 정지되어 있습니다

이미지 패치 인코딩 중 ViT 인코더의 매개변수는 정지되어 있음을 유의하십시오. 따라서 ViT 인코더에는 학습 가능한 매개변수가 없습니다.

이전에 소개된 언어 모델 트랜스포머는 왜 정지되지 않고 학습 가능한 매개변수가 포함되어 있는 반면, 여기서 ViT 인코더는 왜 정지되어 있는 걸까요? 저는 저자가 언어 모델 트랜스포머를 동결해 보았지만, 결과가 학습 가능한 모델보다 좋지 않았던 것 같습니다.

정지 또는 비정지 선택에 대해 조금 더 심도 깊게 생각해 보고 싶습니다. ViT 인코더에서 온 한 세트와 언어 모델 트랜스포머에서 온 다른 세트의 벡터 두 개를 맞추기 위해서는, 한 벡터 세트를 만드는 모델의 일부를 고정시키고, 옵티마이저가 다른 벡터 세트를 만드는 모델의 매개변수를 조정하게 하면 충분합니다. 두 모델 구성 요소를 모두 움직이게 유지할 필요는 없습니다. 모델 일부를 동결시키면, 옵티마이저가 학습할 매개변수가 적기 때문에 작업이 더 쉬워집니다.

<div class="content-ad"></div>

## 주의 집중 풀링은 인코딩된 이미지 패치에 학습 가능한 매개변수를 혼합합니다

다음 단계인 주의 집중 풀링은 인코딩된 이미지 패치를 모델 매개변수와 혼합합니다. 신경망 아키텍처 디자인에서 구현된 인코더를 재사용하는 전형적인 방법입니다. 구현된 인코더를 동결된 상태로 사용하고, 자신의 네트워크에 trainable 레이어를 도입하여 동결된 인코더의 출력을 자신의 네트워크에 혼합합니다. 다음 그림은 주의 집중 풀링이 작동하는 방식을 보여줍니다.

![Image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_12.png)

단계 1. ViT는 변환된 256×1024 인코딩된 이미지 패치를 선형으로 다른 동일한 모양의 텐서로 투영합니다. 이 선형 투영에는 trainable 매개변수가 있습니다.

<div class="content-ad"></div>

스텝 2. 선형으로 프로젝트된 encoded_img_patches 텐서는 특성 차원(1024 차원)에서 동일한 모양의 256×512로 나누어지며, 이 중 하나는 v 텐서로, 다른 하나는 k 텐서로 불립니다. 이미 어텐션 메커니즘에서 사용되는 key 행렬 중요성 때문에 k, v 텐서 명칭에 익숙할 것입니다.

스텝 3. 모양이 257×512인 img_queries 텐서가 소개됩니다. PyTorch의 Embedding 클래스에서 제공되며 학습 가능한 매개변수를 포함합니다. img_queries 텐서는 동일한 모양의 q 텐서로 선형으로 프로젝트됩니다. 이 선형 프로젝션에도 학습 가능한 매개변수가 있습니다. img_queries는 257개의 행을 가지고 있습니다. 그래서 인코딩된 이미지 패치의 개수인 256개보다 한 벡터가 더 있습니다. 이 추가 벡터는 전역 수준 이미지 정보를 표현하는 데 사용됩니다.

스텝 4. 유명한 교차 어텐션 매트릭스 곱인 q@kᵀ입니다. 이 곱셈은 모양이 257×256인 sim 텐서를 만듭니다. sim 매트릭스는 내적 매트릭스로, 각 항목은 img_queries 텐서의 열과 k 텐서의 행 사이의 내적 유사성입니다. 이는 입력 이미지의 단일 패치를 인코딩합니다.

스텝 5. 또 다른 교차 어텐션 매트릭스 곱셈인 sim@v가 이어집니다. 이는 모양이 257×512인 텐서를 만듭니다. 스텝 4와 5는 어텐션 메커니즘에서 일반적인 q, k, v 곱셈입니다.

<div class="content-ad"></div>

6단계에서는 sim @ v 곱셈으로 얻은 257×512 텐서를 두 개의 텐서로 분할합니다:

- 첫 번째는 형상이 256×512인 img_tokens 텐서입니다. 이 텐서는 이미지의 패치별 로컬 수준 이미지 정보를 나타낸다는 것을 의미합니다. 즉, img_tokens의 각 1×512 행은 256개의 이미지 패치에서 정보를 나타냅니다.
- 두 번째는 형상이 1×512인 img_cls 텐서입니다. 이 텐서는 전체 이미지를 요약하는 글로벌 수준 이미지 정보를 나타내는 것으로 해석됩니다.

이 두 텐서는 이전에 설명한 대로 이미지-텍스트 정렬을 수행하는 데 사용됩니다.

## 결론

<div class="content-ad"></div>

이 글은 이미지-텍스트 이중 모달 기반 모델의 설계를 설명합니다. 설계의 핵심 부분은 대조 손실 함수를 통해 전역 수준에서 이미지와 텍스트 간의 정렬을 설정하고 교차 엔트로피 손실 함수를 통해 로컬 수준에서 정렬을 수행하는 것입니다.

전역 수준의 이미지-텍스트 정렬은 이미지 분류, 텍스트 기반 이미지 검색 및 깊은 이미지 유사성 검색과 같은 작업을 지원합니다. 로컬 수준의 이미지-텍스트 정렬은 이미지 캡셔닝 및 이미지 질문에 대한 답변과 같은 작업을 지원합니다.