---
title: "Code Llamaë¡œ ë‚˜ë§Œì˜ LLM ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ë§Œë“œëŠ” ë°©ë²• "
description: ""
coverImage: "/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png"
date: 2024-06-22 21:36
ogImage: 
  url: /assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png
tag: Tech
originalTitle: "How to Build Your Own LLM Coding Assistant With Code Llama ğŸ¤–"
link: "https://medium.com/towards-artificial-intelligence/how-to-build-your-own-llm-coding-assistant-with-code-llama-04d8340900a3"
---


ì´ ì‹¤ìŠµì—ì„œëŠ” ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê³  ë¡œì»¬ GPUì—ì„œ ì‹¤í–‰ë˜ëŠ” AI ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤.

ì±—ë´‡ì— ì§ˆë¬¸ì„ í•˜ë©´ ìì—°ì–´ë¡œ ë‹µë³€í•˜ë©° ì—¬ëŸ¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ì½”ë“œë„ ì œê³µí•©ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” Hugging Face transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì„ êµ¬í˜„í•˜ê³  Chatbot í”„ë¡ íŠ¸ ì—”ë“œì—ëŠ” Streamlitì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

# LLMì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?

<div class="content-ad"></div>

ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì¸ GPT ê³„ì—´ì€ ì£¼ì–´ì§„ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ í…ìŠ¤íŠ¸ ìƒì„±ì— ì•„ì£¼ ëŠ¥ìˆ™í•©ë‹ˆë‹¤.

![ì´ë¯¸ì§€](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png)

ì¶©ë¶„í•œ í›ˆë ¨ ë°ì´í„°ê°€ ì œê³µëœë‹¤ë©´, ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒë„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. IDEì—ì„œ ì½”ë“œë¥¼ ì±„ìš°ëŠ” ë°©ì‹ì´ë‚˜ ì±—ë´‡ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

GitHub Copilotì€ ìƒìš© ì˜ˆì‹œë¡œì„œ AI í˜ì–´ í”„ë¡œê·¸ë˜ë¨¸ì˜ í•œ ì˜ˆì…ë‹ˆë‹¤. Meta AIì˜ Code Llama ëª¨ë¸ì€ ìœ ì‚¬í•œ ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆì§€ë§Œ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

# ì½”ë“œ ëŒë§ˆë€ ë¬´ì—‡ì¸ê°€ìš”?

ì½”ë“œ ëŒë§ˆëŠ” Meta AIê°€ ë§Œë“¤ê³  2023ë…„ 8ì›”ì— ì²˜ìŒìœ¼ë¡œ ì¶œì‹œí•œ ì½”ë“œ ì „ìš© LLM ê³„ì—´ì˜ íŠ¹ë³„í•œ ì œí’ˆì…ë‹ˆë‹¤.

![ì´ë¯¸ì§€](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_1.png)

Meta AIëŠ” ê¸°ë³¸ ëª¨ë¸ Llama 2(ë””ì½”ë” ì „ìš© Transformer ëª¨ë¸ë¡œ GPT-4ì™€ ìœ ì‚¬í•¨)ì„ ì‹œì‘ìœ¼ë¡œ, ëŒ€ë¶€ë¶„ ì½”ë“œë¡œ ì´ë£¨ì–´ì§„ 500B í† í°ì˜ êµìœ¡ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¶”ê°€ êµìœ¡ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

ê·¸ ì´í›„ë¡œ Code Llamaì— ëŒ€í•œ ì„¸ ê°€ì§€ ë²„ì „ì´ ë„¤ ê°€ì§€ ë‹¤ë¥¸ í¬ê¸°ë¡œ ì œê³µë©ë‹ˆë‹¤.

Code Llama ëª¨ë¸ì€ ì—°êµ¬ ë° ìƒì—…ì  ì‚¬ìš©ì„ ìœ„í•´ ë¬´ë£Œì…ë‹ˆë‹¤.

![ì´ë¯¸ì§€](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_2.png)

## Code Llama

<div class="content-ad"></div>

ì½”ë“œ LlamaëŠ” ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤. ì½”ë“œ Llama ëª¨ë¸ì€ infill ëª©ì ìœ¼ë¡œ í›ˆë ¨ë˜ì–´ IDE ë‚´ì—ì„œ ì½”ë“œ ì™„ì„±ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

## ì½”ë“œ Llama â€” Instruct

Instruct ë²„ì „ì€ ì¸ê°„ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì§€ì‹œ ë°ì´í„°ì…‹ì— ë§ì¶° ì„¸ë°€í•˜ê²Œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ChatGPTì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.

## ì½”ë“œ Llama â€” Python

<div class="content-ad"></div>

íŒŒì´ì¬ ë²„ì „ì€ ì¶”ê°€ ë°ì´í„°ì…‹ì¸ 100B í† í°ì˜ íŒŒì´ì¬ ì½”ë“œë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ë“¤ì€ ì½”ë“œ ìƒì„±ì„ ìœ„í•´ ì˜ë„ë˜ì—ˆìŠµë‹ˆë‹¤.

# LLM ì±—ë´‡ ì½”ë”©

ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Instruct ë²„ì „ ì¤‘ ê°€ì¥ ì‘ì€ ëª¨ë¸ì¸ CodeLlama-7b-Instruct â€” hfë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ìì—°ì–´ ì§ˆë¬¸ì— ë‹µë³€í•˜ë„ë¡ ì„¸ë°€í•˜ê²Œ íŠœë‹ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì±—ë´‡ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê°€ì¥ ì‘ì€ ëª¨ë¸ì¡°ì°¨ë„ ì—¬ì „íˆ 7B ë§¤ê°œë³€ìˆ˜ë¡œ ìƒë‹¹íˆ í½ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ì˜ 16ë¹„íŠ¸ ë°˜ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ë©´, ëª¨ë¸ì€ ì•½ 14 GBì˜ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´, ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ì•½ 3.5 GB ì •ë„ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

## ëª¨ë¸ êµ¬í˜„í•˜ê¸°

ìš°ë¦¬ëŠ” ë¨¼ì € Hugging Faceì—ì„œ Code Llama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ê¸°ë°˜í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ChatModel í´ë˜ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.

ìš°ë¦¬ëŠ” 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ìœ„í•´ BitsAndBytesConfigë¥¼ ì‚¬ìš©í•˜ë©°, ëª¨ë¸ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ AutoModelForCausalLMì„ ì‚¬ìš©í•˜ê³  ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í† í° ì„ë² ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•´ AutoTokenizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```js
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

class ChatModel:
    def __init__(self, model="codellama/CodeLlama-7b-Instruct-hf"):
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True, # 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model,
            quantization_config=quantization_config,
            device_map="cuda",
            cache_dir="./models", # ëª¨ë¸ì„ models í´ë”ì— ë‹¤ìš´ë¡œë“œ
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            model, use_fast=True, padding_side="left"
        )
```

<div class="content-ad"></div>

ë˜í•œ, ì‚¬ìš©ìì˜ ì´ì „ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì™€ AIê°€ ìƒì„±í•œ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ê³ ì • ê¸¸ì´ì˜ íˆìŠ¤í† ë¦¬ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” ëŒ€í™”ì˜ ê¸°ì–µì„ ì œê³µí•˜ì—¬ LLMì—ê²Œ ëŒ€í™”ì˜ ê¸°ì–µì„ ë¶€ì—¬í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

```js
self.history = []
self.history_length = 1
```

Code Llamaì€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì•ì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ, codellama-13b-chat ì˜ˆì œì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

```js
self.DEFAULT_SYSTEM_PROMPT = """\
ë‹¹ì‹ ì€ ì½”ë“œì™€ ì†Œí”„íŠ¸ì›¨ì–´ ë””ìì¸ì— ëŒ€í•œ ê¹Šì€ ì§€ì‹ì„ ê°€ì§„, ë„ì›€ì´ ë˜ëŠ”, ì˜ˆì˜ ë°”ë¥´ê³  ì •ì§í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•­ìƒ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ í•´ì•¼ í•˜ë©°, ì•ˆì „í•˜ê³  ì‹ ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤. ë‹µë³€ì— í•´ë¡œìš´, ë¶€ì •í•œ, ì¸ì¢… ì°¨ë³„ì , ì„± ì°¨ë³„ì , ìœ í•´í•œ, ìœ„í—˜í•œ, ë˜ëŠ” ë¶ˆë²•ì ì¸ ë‚´ìš©ì„ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ë‹µë³€ì´ ì‚¬íšŒì ìœ¼ë¡œ í¸í–¥ë˜ê±°ë‚˜ ë¶€ì •ì ì´ì—¬ì„  ì•ˆë©ë‹ˆë‹¤.\n\në§Œì•½ ì§ˆë¬¸ì´ ì´í•´í•  ìˆ˜ ì—†ê±°ë‚˜ ì‚¬ì‹¤ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ì—†ë‹¤ë©´, ì˜¬ë°”ë¥¸ ëŒ€ë‹µ ëŒ€ì‹  ì™œ ì˜ëª»ëœ ê²ƒì¸ì§€ ì„¤ëª…í•˜ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ëª¨ë¥´ë©´, ê°€ì§œ ì •ë³´ë¥¼ ê³µìœ í•˜ì§€ ë§ê³  ëŒ€ì‹  ë§í•´ì£¼ì„¸ìš”.\
        """
```

ì´ì œ self.historyì— í˜„ì¬ ëŒ€í™”ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.

LLM(ì–´ë¼ìš´ë“œ  ëª¨ë¸)ì€ í•œì •ëœ ë¬¸ë§¥ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ì— ì •ë³´ë¥¼ í•œì •ì ìœ¼ë¡œ ë³´ê´€í•  ìˆ˜ë°–ì— ì—†ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” self.history_length = 1 ê°œì˜ ì§ˆë¬¸ê³¼ ëŒ€ë‹µë§Œ ìµœëŒ€í•œ ë³´ê´€í•©ë‹ˆë‹¤.

```js
    def append_to_history(self, user_prompt, response):
        self.history.append((user_prompt, response))
        if len(self.history) > self.history_length:
            self.history.pop(0)
```

<div class="content-ad"></div>

ë§ˆì¹¨ë‚´ ìš°ë¦¬ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ê¸°ë°˜í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” generate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

ê° LLMì—ëŠ” í›ˆë ¨ì— ì‚¬ìš©ëœ íŠ¹ì • í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ìˆìŠµë‹ˆë‹¤. Code Llamaì˜ ê²½ìš° codellama-13b-chatì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì°¸ì¡°ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

```js
    def generate(
        self, user_prompt, system_prompt, top_p=0.9, temperature=0.1, max_new_tokens=512
    ):

        texts = [f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n"]
        do_strip = False
        for old_prompt, old_response in self.history:
            old_prompt = old_prompt.strip() if do_strip else old_prompt
            do_strip = True
            texts.append(f"{old_prompt} [/INST] {old_response.strip()} </s><s>[INST] ")
        user_prompt = user_prompt.strip() if do_strip else user_prompt
        texts.append(f"{user_prompt} [/INST]")
        prompt = "".join(texts)

        inputs = self.tokenizer(
            prompt, return_tensors="pt", add_special_tokens=False
        ).to("cuda")

        output = self.model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            pad_token_id=self.tokenizer.eos_token_id,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=top_p,
            top_k=50,
            temperature=temperature,
        )
        output = output[0].to("cpu")
        response = self.tokenizer.decode(output[inputs["input_ids"].shape[1] : -1])
        self.append_to_history(user_prompt, response)
        return response
```

ì‘ë‹µì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ë‹µë³€ì˜ ì°½ì˜ì„±ì€ top_p ë° temperatureì™€ ê°™ì€ ë§¤ê°œë³€ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

<div class="content-ad"></div>

top_pë¥¼ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ í† í°ì˜ í™•ë¥  ê°’ì„ ì œí•œí•˜ì—¬ ë„ˆë¬´ ë“œë¬¼ê²Œ ë°œìƒí•˜ëŠ” í† í°ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ í”¼í•  ìˆ˜ ìˆì–´ìš”:

temperatureë¥¼ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ í† í°ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ í•˜ê±°ë‚˜ ë‚ ì¹´ë¡­ê²Œ í•  ìˆ˜ ìˆì–´ìš”:

í”„ë¡ íŠ¸ì—”ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì§„í–‰í•˜ê¸° ì „ì— ChatModelì„ í…ŒìŠ¤íŠ¸í•´ë³´ì£ .

```js
from ChatModel import *

model = ChatModel()
response = model.generate(
    user_prompt="C++ì—ì„œ hello world í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•´ë´", 
    system_prompt=model.DEFAULT_SYSTEM_PROMPT
)
print(response)
```

<div class="content-ad"></div>

```js
ë‹¹ì‹ ì´ ìš”ì²­í•œ ì‘ì—…ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ í…Œì´ë¸” íƒœê·¸ê°€ Markdown í˜•ì‹ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.
```

<div class="content-ad"></div>

```js
import streamlit as st
from ChatModel import *

st.title("Code Llama Assistant")


@st.cache_resource
def load_model():
    model = ChatModel()
    return model


model = load_model()  # load our ChatModel once and then cache it
```

ë‹¤ìŒìœ¼ë¡œ generate í•¨ìˆ˜ë¥¼ ìœ„í•œ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì…ë ¥ ì œì–´í•˜ëŠ” ì‚¬ì´ë“œë°”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```js
with st.sidebar:
    temperature = st.slider("ì˜¨ë„", 0.0, 2.0, 0.1)
    top_p = st.slider("top_p", 0.0, 1.0, 0.9)
    max_new_tokens = st.number_input("max_new_tokens", 128, 4096, 256)
    system_prompt = st.text_area(
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", value=model.DEFAULT_SYSTEM_PROMPT, height=500
    )
```

ê·¸ë¦¬ê³  ì±—ë´‡ ë©”ì‹œì§€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

<div class="content-ad"></div>

```js
# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì•± ì¬ì‹¤í–‰ì‹œ ê¸°ë¡ëœ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆì— í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì±—ë´‡ ì‘ë‹µì„ ì±„íŒ… ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆì— í‘œì‹œ
    with st.chat_message("assistant"):
        user_prompt = st.session_state.messages[-1]["content"]
        answer = model.generate(
            user_prompt,
            top_p=top_p,
            temperature=temperature,
            max_new_tokens=max_new_tokens,
            system_prompt=system_prompt,
        )
        response = st.write(answer)
    st.session_state.messages.append({"role": "assistant", "content": answer})
```

ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì„ streamlit run app.pyë¡œ ì‹¤í–‰í•˜ì—¬ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦½ë‹ˆë‹¤.

ì´ì œ ì±—ë´‡ì— ì½”ë”© ê´€ë ¨ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ê²°ë¡ 

<div class="content-ad"></div>

ì €í¬ëŠ” Meta AIì˜ Code Llama LLMì„ í™œìš©í•˜ì—¬ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í–ˆì–´ìš”. ê·¸ë¦¬ê³  Hugging Faceì˜ transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ Streamlitì„ ì‚¬ìš©í•´ì„œ í”„ë¡ íŠ¸ì—”ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì—ˆì–´ìš”.

6GBì˜ GPU ë©”ëª¨ë¦¬ë¥¼ ê°–ì¶˜ ë…¸íŠ¸ë¶ìœ¼ë¡œëŠ” 4ë¹„íŠ¸ ì–‘ìí™”ëœ Code Llama ëª¨ë¸ì„ 7B ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ë°–ì— ì—†ì—ˆì–´ìš”. ë” í° GPUë¥¼ ì‚¬ìš©í•˜ë©´ 16ë¹„íŠ¸ ë²„ì „ì´ë‚˜ ë” í° ëª¨ë¸ì´ ë” ì˜ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.

P.S. Code Llamaë¡œë¶€í„° ì œê°€ ë°›ì€ ë†ë‹´ë³´ë‹¤ ë” ì¬ë¯¸ìˆëŠ” ë†ë‹´ë“¤ì„ ê¸°ëŒ€í•´ë´…ë‹ˆë‹¤ ğŸ¤¡.

ë” ë§ì€ LLMì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œë‹¤ë©´, ìµœê·¼ì— ê³µê°œëœ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì— ëŒ€í•œ ê°œìš”ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”:

<div class="content-ad"></div>

# ì°¸ê³  ìë£Œ

[1] B. RoziÃ¨re ì™¸: Code Llama: ì½”ë“œë¥¼ ìœ„í•œ ì˜¤í”ˆ ê¸°ë°˜ ëª¨ë¸ (2023), arXiv:2308.12950

# ìì›

- Streamlit ì±„íŒ… ì•± ì˜ˆì œ: ê¸°ë³¸ LLM ì±„íŒ… ì•± êµ¬ì¶•
- Hugging Face Code Llama gradio êµ¬í˜„: codellama-13b-chat
- ì´ ë¬¸ì„œì˜ ì „ì²´ ì‘ì—… ì½”ë“œ: [https://github.com/leoneversberg/codellama-chatbot](https://github.com/leoneversberg/codellama-chatbot)