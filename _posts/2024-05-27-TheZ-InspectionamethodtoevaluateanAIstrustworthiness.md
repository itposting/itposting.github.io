---
title: "Z-검사 AI 신뢰성 평가 방법"
description: ""
coverImage: "/assets/img/2024-05-27-TheZ-InspectionamethodtoevaluateanAIstrustworthiness_0.png"
date: 2024-05-27 15:39
ogImage: 
  url: /assets/img/2024-05-27-TheZ-InspectionamethodtoevaluateanAIstrustworthiness_0.png
tag: Tech
originalTitle: "The Z-Inspection: a method to evaluate an AI’s trustworthiness"
link: "https://medium.com/user-experience-design-1/the-z-inspection-a-method-to-evaluate-an-ais-trustworthiness-f6e638f4ddd7"
---


<img src="/assets/img/2024-05-27-TheZ-InspectionamethodtoevaluateanAIstrustworthiness_0.png" />

간략 요약: AI의 전체 잠재력을 실현하기 위해서는 신뢰성이 필요하며, 이를 위해 투명성, 공정성 및 보안에 주의를 기울여야 합니다. 편향과 개인 정보 보호에 대한 과제를 해결하기 위해서는 AI 수명 주기 전반에 걸쳐 종합적인 접근 방식이 필요합니다. Z-Inspection 방법은 윤리적, 기술적, 법적 고려사항을 연결하여 AI의 신뢰성을 평가하고 강화하는 실용적인 프레임워크로 나타나고 있습니다. 이 종합적인 접근 방식을 통해 우리는 신뢰를 불러일으키며 책임 있는 혁신을 이끌어내는 AI 시스템의 길을 열어갑니다.

신뢰할 만한 AI란 무엇을 의미할까요? 정책 제정자와 AI 개발자들은 전 세계적으로 이 질문을 다루기 위해 수백만을 쏟아부었습니다. 이 노력 뒤에 있는 원동력은 사회가 AI의 전체 잠재력을 실현할 수 있는 믿음이 AI의 개발, 배포 및 사용에 신뢰가 내재되어야만 가능하다는 것입니다. 예를 들어, 의사와 환자가 AI 기반 시스템의 진단이나 치료 권고를 신뢰하지 않으면, 이러한 권고가 환자의 복지를 개선할 수 있다 하더라도 따를 가능성이 낮습니다. 마찬가지로 대중이 자율주행 차량을 신뢰하지 않는다면, 이들은 전통적인 수동 운전 차량을 대체하지 않을 것입니다.

그러나 현재 많은 AI 시스템은 인지할 수 없는 공격에 취약하며, 소수 그룹에 편향이 있고, 사용자 프라이버시 보호가 부족합니다. 이러한 위반 사항은 채용 및 대출 결정에서 자동 시스템의 편향적 대우부터 인간 목숨의 손실에 이르기까지 다양합니다. 이러한 문제는 모든 AI 시스템에 대한 사용자 경험을 감소시키고 신뢰를 손상시킵니다.

<div class="content-ad"></div>

전통적으로 인공지능 실무자들인 연구자, 개발자, 의사결정자들은 시스템 성능(즉, 정확도)을 주요 지표로 삼아왔습니다. 그러나 이 지표만으로는 인공지능 시스템의 신뢰성을 반영하는 데 충분하지 않습니다. 신뢰성을 향상시키려면 로버스트성, 알고리즘 공정성, 설명 가능성, 투명성 등 성능 이외의 여러 요인을 고려해야 합니다.

인공지능 신뢰성에 대한 학계 연구는 주로 모델의 알고리즘적 속성에 중점을 두었습니다. 그러나 알고리즘적 연구만 진전시키는 것으로는 신뢰성 있는 인공지능 제품을 만들기에 부족합니다. 산업적 관점에서 보면, 인공지능 제품의 수명주기는 다양한 단계를 거치며 각 단계에서 신뢰성을 향상시키기 위한 특별한 노력이 필요합니다. 이러한 단계에는 데이터 준비, 알고리즘 설계, 개발, 배포, 운영, 모니터링, 그리고 거버넌스 등이 포함됩니다. 예를 들어, 로버스트성을 향상시키려면 데이터 살균, 강력한 알고리즘 개발, 이상 감지 모니터링, 위험 감사 등이 필요합니다. 반대로, 어느 한 단계에서 신뢰 손실이 발생하면 전체 시스템의 신뢰성이 훼손될 수 있습니다.

따라서 인공지능 신뢰성은 시스템의 전체 수명주기 동안 체계적으로 세워지고 평가되어야 합니다.

# 신뢰성 있는 컴퓨팅의 여정

<div class="content-ad"></div>

컴퓨팅에서 신뢰성 개념은 중요한 사건에서 비롯되었습니다. 마이크로소프트의 공동 창업자인 빌 게이츠가 2002년 회사의 모든 직원에게 보낸 이메일에서 시작되었습니다. 이 이메일은 컴퓨팅 역사상 중요한 순간으로, 디지털 시스템에서의 신뢰성 개념을 위한 기초를 마련했습니다.

이 신뢰성 컴퓨팅 방식은 컴퓨터 과학과 시스템 공학 분야 일부에서 계속해서 채택되고 있습니다 (예: 전기전자공학회 (IEEE) 및 국제전기기술위원회 (IEC)).

국제표준화기구 (ISO) 및 IEEE 표준의 신뢰성 정의는 게이츠의 시스템 신뢰성 속성을 중심으로 구축되어 있습니다:

- 컴퓨터 시스템의 신뢰성은 제공하는 서비스에 정당한 신뢰를 둘 수 있는 정도의 신뢰성
- 특정 항목의 성능에 대한 믿음성 및 요구 시 수행할 수 있는 능력.

<div class="content-ad"></div>

2024년 3월 13일, 유럽 의회가 인공지능 법안 (AI Act)을 채택했습니다. 이 법안은 세계 최초의 포괄적 수평적 인공지능 법적 프레임워크로 간주되며, 데이터 품질, 투명성, 인간 감시 및 책임성에 대한 EU 전체적인 규칙을 제정합니다. 이는 AI 시스템의 신뢰성 원칙을 더욱 발전시키는데 일조하고 있습니다.

# AI 신뢰성에 대한 실용적 접근방식

그러나 언급된 요구 사항 목록은 실제 응용에는 너무 추상적으로 보입니다. 이러한 지침은 AI 신뢰성 이해를 위한 기초를 제공하지만, 실세계에서의 구현을 위해 더 실용적인 접근 방식이 필요합니다.

AI 신뢰성을 평가해야 할 주 배우자들뿐만 아니라 설계 및 개발 과정에 참여하지 않은 사용자들이 신뢰성을 평가하는 것 또한 좋은 실천 방법입니다. 이는 어떠한 우려 사항도 해소하고 시스템에 대한 신뢰를 높이는데 도움이 됩니다.

<div class="content-ad"></div>

이 문맥에서 Z-Inspection과 같은 더 실용적인 평가 프로세스가 중요해집니다. Z-Inspection은 현실 세계 시스템 개발의 문제와 요구 사항에 맞춰 설계되었으며 AI 시스템의 협업, 자가평가 또는 외부 감사에 활용할 수 있습니다.

이는 AI 시스템의 신뢰성을 평가하고 향상시키는 데 일반 목적 방법론으로 적합하게 만듭니다.

- 설계 단계에서 Z-Inspection은 신뢰할 수 있는 AI 시스템을 설계하는 방법에 대한 통찰력을 제공할 수 있습니다.
- 개발 중에는 윤리적 시스템 개발을 확인하고 테스트하고 관련 사용자 그룹의 수용을 평가하는 데 사용할 수 있습니다.
- 배포 후에도 이 프로세스는 변화하는 모델, 데이터 또는 환경의 영향을 검사하는 지속적인 모니터링 노력으로 사용할 수 있습니다.

Z-Inspection은 두 가지 완성된 접근 방식을 신중하게 결합한 방법론입니다:

<div class="content-ad"></div>

- 전체 시스템을 이해하려는 종합적인 접근방식,
- 문제 영역의 각 구성 요소를 신중하게 조사하는 분석적인 접근방식.

이 통합은 “Responsible AI — Two frameworks for ethical design practice” 논문의 저자들의 관점과 일치합니다:

# Z-Inspection의 단계별 가이드

이 프로세스는 세 가지 주요 단계로 구성됩니다:

<div class="content-ad"></div>

- 사전 조건이 명확하게 파악되고 평가팀이 식별되며 경계에 동의하는 설정 단계
- AI 시스템이 분석되는 것이 평가 단계
- 식별된 문제와 긴장을 해소하고 권고사항을 제공하는 결과 단계.

# 설정 단계

설정 단계는 사전 조건을 명확히하고 조사팀을 선정하며 평가의 경계를 정의하고 프로토콜을 작성하는 데 도움을 줍니다.

Z-검사 과정은 AI의 영향을 다루는 전문가들의 다학제적 팀을 구성하여 시작됩니다. 이 팀에는 도메인 전문가, 기술 전문가, 사용자, 법률 자문자 및 윤리학자가 포함되어야 합니다. 도메인 전문가는 AI의 기본 가정을 확인하고, 기술 전문가는 적절한 구현을 보증합니다. 사용자는 시스템과 관련된 우려를 표명하고, 법률 및 윤리 전문가는 가능한 도전에 대비하여 팀을 안내합니다.

<div class="content-ad"></div>

스테이크홀더 간의 기대치를 조율하는 것이 중요합니다. 검사 요청자가 누구인지, 결과가 어떻게 활용될지(예: 공중 홍보, 시스템 개선, 또는 최종 사용자와의 커뮤니케이션 등) 명확히 해야 합니다.

이 단계는 또한 평가 범위를 정의하는 과정을 포함하며, AI 시스템이 더 넓은 사회 기술적 맥락 내에서 운영됨을 인식해야 합니다. 이 환경을 철저히 평가하는 것은 포괄적인 평가에 필요합니다.

# 평가 단계

평가 단계는 네 가지 작업으로 구성되어 있습니다:

<div class="content-ad"></div>

- AI 시스템 사용 분석;
- 잠재적 윤리적 문제 및 기술적, 법적 문제 식별;
- 해당 문제를 신뢰할 수 있는 AI 윤리적 가치 및 요구 사항에 매핑;
- 해당 요구 사항의 검증.

평가 단계의 첫 번째 단계는 AI 시스템을 사용하는 사람들의 가능한 일상적인 경험을 설명하는 사회 기술적 시나리오를 정의하는 것입니다. 이러한 시나리오는 발생할 수 있는 문제를 예측하고 강조하기 위해 사용됩니다. 이들은 공급 업체가 제공한 정보, 참가자의 개인 경험 및 이전에 정의된 평가의 한계를 기반으로 합니다.

다음으로, 이러한 시나리오는 다른 팀 구성원들에 의해 분석됩니다. 각 팀 구성원은 다양한 배경을 통해 보완적인 시각을 제공할 수 있으므로 자신의 시각을 기여하도록 권장됩니다. 예를 들어, AI 엔지니어는 방사선과의 타겟마커와 간호사와는 달리 시스템의 다른 측면에 중점을 둘 것이며, 그들의 초점은 간호사의 것과 다를 것입니다.

각 참가자 또는 동일한 배경을 가진 참가자 그룹은 시스템에서 보이는 문제점을 각자의 시각에서 설명하고 이러한 문제가 실제 문제인 이유에 대한 증거를 제시합니다. 이 증거는 수립된 모범 사례, 과학 보고서 또는 관찰된 행동과 공급 업체 시스템 설명 사이의 충돌과 같은 다양한 형태로 나타날 수 있습니다.

<div class="content-ad"></div>

다음 단계는 신뢰성 높은 AI 요구 사항과 충돌하는 문제를 식별하고 시스템 행동과 신뢰성 사이의 긴장을 강조하는 것입니다. 이 프로세스는 이러한 문제가 AI 시스템의 신뢰성에 미치는 영향을 강조하는 데 도움이 됩니다.

그 후에 식별된 문제를 통합합니다. 각 이해관계자 또는 그룹은 발견한 결과를 제시하여 관련 문제를 결합하여 간결한 목록을 작성할 수 있습니다.

# 해결 단계

해결 단계는 이전에 식별된 윤리적, 기술적, 법적 문제에 대해 대응하며 가능한 경우 윤리적 긴장을 해소합니다. 참가자들은 이러한 문제를 논의하고 우선순위를 정하며 실행 가능한 해결책을 찾기 위해 협력합니다. 이 과정에서 시스템 및 구체적인 문제에 따라 원칙 간의 잠재적인 갈등이 발생할 수 있습니다. 이러한 경우 시간이 흐른 후 윤리적 AI 유지 보수 계획이 권장될 수 있습니다.

<div class="content-ad"></div>

다음 중 세 가지 케이스 중 하나가 발생할 수 있습니다.

- 긴장 상태가 해결되지 않을 수 있습니다.
- 추가 리소스 할당을 통해 긴장 상태를 극복할 수 있거나
- 두 가지 상충하는 원칙 사이에서 선택할 수 있는 해결책이 있을 수 있습니다.

경우에 따라 충돌의 명백한 해결책이 있을 수도 있습니다. 그러나 다른 케이스에서는 트레이드오프가 필요하거나 현실적으로 해결책이 없을 수도 있습니다. 이 단계에서 참가자들은 윤리적 긴장을 직접 다루고 이해 관계자들과 트레이드오프를 논의하여 잠재적인 영향을 완화하기 위한 권고안을 제시합니다.

# 최종 생각

<div class="content-ad"></div>

원래는 AI 시스템의 신뢰성을 평가하기 위해 들어가기로 한 적은 없었어요. 처음에 제 초점은 디자이너가 사용자들이 회의를 극복하고 새로운 기술을 받아들이는 데 어떻게 도와줄 수 있는지였죠. 하지만 더 깊게 파고들수록, 시스템이 정말로 신뢰할 만한지 확인하는 것이 단지 그렇게 인식되는 것보다 훨씬 중요하다는 것을 깨달았어요.

AI 시스템의 윤리적, 사회적 영향은 상당한 우려를 불러일으킵니다. 그리고 안타깝게도, 디자이너와 일반 사용자 모두에게 AI 신뢰성을 평가하는 방법에 대한 접근 가능한 정보가 부족합니다.

AI 시스템의 신뢰성을 평가하는 경험이 있으시다면, 사용한 방법에 대해 알려주시면 정말 좋겠어요. 여러분의 통찰력을 공유해주시면 우리 모두가 투명하고 공정하며 안전한 AI 기술의 미래를 향해 함께 나아갈 수 있습니다.